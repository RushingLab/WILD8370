[{"path":"http://rushinglab.github.io/WILD8370/articles/Lab01b_Mathematical_Notation.html","id":"analysis-flowchart","dir":"Articles","previous_headings":"","what":"Analysis Flowchart","title":"Lab1: Mathematical Notation","text":"One hardest part Bayesian analysis converting idea ecological system English math math English. Unlike lot maximum likelihood packages, get decide exact variable model defined, distributions think parameters come , assumptions making underlying system trying understand. process made confusing fact aren’t directly math ! can think process fun little flow chart:  Steps 1 -3 specific Bayesian analysis – need thing maximum likelihood analysis well. Also notice steps 4 5 simply Bayesian analysis class. need MCMC software R perform Bayesian analysis, just happens easiest way . key step help way practicing reading math equations translating English translating code.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab01b_Mathematical_Notation.html","id":"some-useful-notation","dir":"Articles","previous_headings":"","what":"Some Useful Notation","title":"Lab1: Mathematical Notation","text":"E(x) expected value something. instance, x number frogs pond, E(x) expected number frogs pond. \\phi Phi, usually used indicate survival probability (can also used willy-nilly) Norm( \\mu, \\sigma^2) normal distribution mean \\mu standard deviation \\sigma. Pois(\\lambda) poisson distribution mean \\lambda. poisson accepts produces zero positive values Binom(N, p) binomial distribution size N probability success p. \\sum^w_{j=1} K_j sum value K_j values j 1 w. instance, K_j represents number personalized mugs Dr. Rushing’s office year j , sum year j = 3 year 10 \\sum^{10}_{j=3} K_j \\beta_0 + \\epsilon \\sim Norm(0, \\sigma^2) shorthand way saying residuals equation normally distributed. assumption linear models can quicker write way rather line E(x) line residuals. \\beta_0 + \\beta_1[q] value \\beta_1 depends value integer q. q can take 3 values (1, 2, 3), know three possible values \\beta_1. commonly used categorical variables. logit(x) = \\beta_0 tells us x probability scale (0 1) \\beta_0 logit scale (real numbers). R, knew \\beta_0 wanted get x, type: plogis( \\beta_0 )","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab01b_Mathematical_Notation.html","id":"math-to-english","dir":"Articles","previous_headings":"","what":"Math to English","title":"Lab1: Mathematical Notation","text":"’ll often see people’s equations scientific papers want understand analysis.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab01b_Mathematical_Notation.html","id":"example-bears","dir":"Articles","previous_headings":"Math to English","what":"Example: Bears","title":"Lab1: Mathematical Notation","text":"want use data weight abundance black bears collected past decade determine bear health indices responds different environmental biological factors. ’re just going focus Process Model - , think system works, even can’t observe directly. following information available: Let’s say come following equation. mean English? E(weight_{}) = \\beta_0 + \\beta_1*percentforest_{} + \\beta_2 * sex_i y_{} \\sim Normal(E(weight_{}), \\sigma^2) Answer: expected weight individual bear time t function percent forest bear’s county sex bear. observed bear weight normal distribution centered around expected weight standard deviation \\sigma. equation following? E(weight_{}) = \\beta_0[landcover_{}] + \\beta_1*percentforest_{} + \\beta_2 * sex_i y_{} \\sim Normal(E(weight_{}), \\sigma^2) Answer: expected weight individual bear time t function categorical landcover, percent forest bear’s county sex bear. observed bear weight normal distribution centered around expected weight standard deviation \\sigma.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab01b_Mathematical_Notation.html","id":"example-turtles","dir":"Articles","previous_headings":"Math to English","what":"Example: Turtles","title":"Lab1: Mathematical Notation","text":"’m running headstarting experiment spotted turtles Northeastern United States. turtles sexed release. released wild 3 different age groupings (juvenile, subadult, adult). come following potential model explain probability turtle survived 6 week study: logit(\\phi_{}) = \\phi_0[releaseage_i] + \\phi_1*birthweight_i y_i \\sim Bernoulli(\\phi_i) mean English? Answer: probability turtle survived study function age group release turtle’s birthweight. Note y_i going binary (0 = didn’t make , 1 = survived).","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab01b_Mathematical_Notation.html","id":"homework-questions","dir":"Articles","previous_headings":"","what":"Homework Questions","title":"Lab1: Mathematical Notation","text":"Write mathematical representation matches following information: expected weight individual bear time t function categorical landcover, sex bear, time since fire. relationship time since fire different males females. observed bear weight normal distribution centered around expected weight standard deviation \\sigma. Note: multiple ways write , just choose one makes sense Looking following bear equation, interpret (english) \\beta_1 negative? \\beta_2 negative? E(weight_{}) = \\beta_0[landcover_{}] + \\beta_1*percentforest_{} + \\beta_2 * sex_i Looking data (Example 2 Lab), notice also recorded turtle’s sex release. Sex recorded either unknown, male female. add information previous turtle model survival probability? (Assume ’s possible turtles sex category age.) 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab10_CJS.html","id":"the-data-set","dir":"Articles","previous_headings":"","what":"The Data Set","title":"Lab10: Discrete Survival Models","text":"Today’s lab work data saw class - frog toad capture recapture information published study chytrid fungus. ’s citation: Russell, R.E., Halstead, B.J., Fisher, R.N., Muths, E.L., Adams, M.J., Hossack, B.R., 2019, Amphibian capture mark-recapture: U.S. Geological Survey data release, https://doi.org/10.5066/P9LNLEDF. lecture looked might model apparent survival using data, completely ignore infection status animals. Now, let’s see can model apparent infection - joint probability animal survives becomes infected chytrid. assume animal gets disease positive rest life.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab10_CJS.html","id":"known-fate-style","dir":"Articles","previous_headings":"","what":"Known-fate Style","title":"Lab10: Discrete Survival Models","text":"often tempting ignore detection probability modeling capture mark recapture data. However, often give biased information parameter interest. fit simple known fate model data, might look something like : case eta (\\eta) survival rather probability getting disease, conditional surviving. estimate survival directly. animal infected, z = 0 stop monitoring animal study. Let’s organize data fit simple model. ’ll use boreal toad (Rana muscosa) California example. lecture, concern yearly level, capture-date level. Thus, animal ever tested positive given year, mark positive entire year. need organize data get capture history determine first last time saw animal negative state. One major problem , course, lot detection error, lot thinning data remove -NA rows -0 rows. also need remove individuals seen least two consecutive time periods (neither first capture second capture NA). , finding first detection individual last useable detection. actual known-fate data well censoring. individuals seen (first time also last time), drop model: Now can make objects nimble send model: Check everything working properly: Run model: Check chains. Remember defined eta similar survival, ’re really interested 1-eta (probability infection).  model suggests annual probability infection (conditional survival) around 3.9% course, fit model inappropriate type data. Let’s see happens fit model account detection probability.","code":"KnownFrogs <- nimbleCode({   eta ~ dbeta(1, 1)      for(i in 1:nind){     for(t in (first_live[i]+1):last_live[i]){       z[i,t] ~ dbern(eta)     }   } }) data(\"WyomingFrogs\") Rmus <- subset(frog_caps, frog_caps$Species == 'Rana muscosa') Rmus$Bd <- ifelse(Rmus$Bd.presence %in% c('Negative', 'negative'), 1,                   ifelse(Rmus$Bd.presence == 'Positive', 0, NA) ) Rmus$id <- as.numeric(as.factor(Rmus$Ind.ID)) Rmus$date <- as.Date(Rmus$Survey.Date, format = '%m/%d/%y') Rmus$year <- as.numeric(format(Rmus$date, '%y')) Rmus$occ <- as.numeric(as.factor(Rmus$year)) head(Rmus) #> # A tibble: 6 × 11 #>   Ind.ID   Survey.Date Project Species  Sex   Bd.presence    Bd    id date       #>   <chr>    <chr>       <chr>   <chr>    <chr> <chr>       <dbl> <dbl> <date>     #> 1 20836382 9/17/10     CA      Rana mu… Male  Negative        1   100 2010-09-17 #> 2 20836382 7/7/09      CA      Rana mu… Male  Negative        1   100 2009-07-07 #> 3 20836382 6/5/12      CA      Rana mu… Male  Negative        1   100 2012-06-05 #> 4 20839859 5/27/09     CA      Rana mu… Male  Positive        0   101 2009-05-27 #> 5 20840567 5/27/09     CA      Rana mu… Fema… Negative        1   102 2009-05-27 #> 6 20841578 8/15/14     CA      Rana mu… Fema… Negative        1   103 2014-08-15 #> # ℹ 2 more variables: year <dbl>, occ <dbl> nind <- length(unique(Rmus$id)) nocc <- length(unique(Rmus$occ)) z <- array(NA, c(nind, nocc)) for(j in 1:nrow(Rmus)){   z[Rmus$id[j],Rmus$occ[j]] <- Rmus$Bd[j] } new.z <- z[rowSums(z, na.rm = T) > 0,] #get rid of ones with no negative caps first <- apply(new.z, 1, function(row) {   which(row > 0)[1]  # Returns the index of the first detection }) last <- array(NA, nrow(new.z)) for(j in 1:nrow(new.z)){   if(first[j] == nocc) next  last[j] <-  which(is.na(new.z[j, (first[j]+1):nocc]))[1]+first[j]-1 } z_useable <- new.z[which(last-first > 0),] first.use <- first[which(last-first > 0)] last.use <- last[which(last-first > 0)] #Make sure censors are correct: for(k in 1:nrow(z_useable)){   if(first.use[k] != 1){     z_useable[k, 1:(first.use[k]-1)] <- NA   }   if(last.use[k] != nocc){     z_useable[k, (last.use[k]+1):nocc] <- NA   } } nind <- nrow(z_useable) nd <- list(z = z_useable) nc <- list(nind = nind,            first_live = first.use,            last_live = last.use) ni <- list(eta = rbeta(1,1,1)) params <- c('eta') prepnim <- nimbleModel(code = KnownFrogs, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong #> [1] -20.42 frogs_kf <- nimbleMCMC(code = KnownFrogs,                      data = nd,                      constants = nc,                      inits = ni,                      monitors = params,                      thin = 1,                      niter = 5000,                      nburnin = 2500,                      nchains = 3,                      samplesAsCodaMCMC = TRUE                       ) MCMCvis::MCMCtrace(frogs_kf, pdf = F, Rhat = T, n.eff = T) 1- MCMCvis::MCMCsummary(frogs_kf, Rhat = F, n.eff = F) #>        mean    sd    2.5%     50%   97.5% #> eta 0.03892 0.983 0.07726 0.03637 0.01294"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab10_CJS.html","id":"cjs-style","dir":"Articles","previous_headings":"","what":"CJS Style","title":"Lab10: Discrete Survival Models","text":"CJS style model, fit similar model, account detection probability estimates. need make sure data doesn’t let frogs recover chytrid (match model) removes frogs positive first capture, otherwise cleanup pretty easy. Remember can’t use individuals seen last interval also need fill z object, just contains 1’s. know -first time saw animal last time saw uninfected still alive/negative. Time add initial values z: Send params objects, make sure nimble model works: Check everything working properly: Run model: Check chains. , ’re really interested 1-eta (probability infection).  Let’s see estimates compare Known-fate style model.  ignore detection probability.","code":"UnknownFrogs <- nimbleCode({ eta ~ dbeta(1, 1) p ~ dbeta(1, 1)  for(i in 1:nind){   z[i,first[i]] <- 1 #initial capture   for(t in (first[i]+1):nocc){     z[i,t] ~ dbern(eta*z[i,t-1])     y[i,t] ~ dbern(p*z[i,t])   } } }) nind.cjs <- length(unique(Rmus$id)) nocc.cjs <- length(unique(Rmus$occ)) y.cjs <- array(NA, c(nind.cjs, nocc.cjs)) for(j in 1:nrow(Rmus)){   y.cjs[Rmus$id[j],Rmus$occ[j]] <- Rmus$Bd[j] } y.new.cjs <- y.cjs[rowSums(y.cjs, na.rm = T) > 0,] #get rid of ones with no negative caps first.cjs <- apply(y.new.cjs, 1, function(row) {   which(row > 0)[1]  # Returns the index of the first detection }) bad <- which(first.cjs == nocc.cjs) y.new.cjs <- y.new.cjs[-c(bad),] #remove bad ones z.new.cjs <- y.new.cjs z.new.cjs[z.new.cjs == 0] <- NA #unknown  first.cjs <- first.cjs[-c(bad)] nind2 <- length(first.cjs)  #Remove anything before the first detection and put 0's after last detection for(k in 1:nrow(y.new.cjs)){   if(first.cjs[k] != 1){      y.new.cjs[k, 1:(first.cjs[k]-1)] <- 0   }     y.new.cjs[k, is.na(y.new.cjs[k,])] <- 0 #didn't see } head(y.new.cjs, n = 5) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    0    0    0    0    0    1    0    0    0     0     0     0     0     0 #> [2,]    0    0    0    0    1    1    0    0    0     0     0     0     0     0 #> [3,]    0    0    0    0    0    1    1    0    0     0     0     0     0     0 #> [4,]    0    0    0    0    1    0    0    0    0     0     0     0     0     0 #> [5,]    0    0    0    0    0    1    0    0    0     0     1     0     0     0 last.cjs <- apply(y.new.cjs, 1, function(row) {   max(which(row > 0))  # Returns the index of the last detection }) for(j in 1:nrow(z.new.cjs)){   z.new.cjs[j, first.cjs[j]:last.cjs[j]] <- 1 } head(z.new.cjs) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]   NA   NA   NA   NA   NA    1   NA   NA   NA    NA    NA    NA    NA    NA #> [2,]   NA   NA   NA   NA    1    1   NA   NA   NA    NA    NA    NA    NA    NA #> [3,]   NA   NA   NA   NA   NA    1    1   NA   NA    NA    NA    NA    NA    NA #> [4,]   NA   NA   NA   NA    1   NA   NA   NA   NA    NA    NA    NA    NA    NA #> [5,]   NA   NA   NA   NA   NA    1    1    1    1     1     1    NA    NA    NA #> [6,]   NA   NA   NA   NA    1    1    1    1    1    NA    NA    NA    NA    NA z.init <- z.new.cjs for(j in 1:nind2){   if(first.cjs[j] > 1){     z.init[j, 1:(first.cjs[j]-1)] <- 1 #start everyone alive before first capture    }   if(last.cjs[j] < nocc.cjs){     z.init[j, (last.cjs[j]+1):nocc.cjs] <- 0 #kill everyone after last detection   } } z.init[!is.na(z.new.cjs)] <- NA #no inits for known data ni <- list(eta = rbeta(1,1,1), p = rbeta(1,1,1), z = z.init) params <- c('eta', 'p') nd <- list(y = y.new.cjs, z = z.new.cjs) nc <- list(first = first.cjs, nocc = nocc.cjs, nind = nind2) prepnim <- nimbleModel(code = UnknownFrogs, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong #> [1] -1423 frogs_cjs <- nimbleMCMC(code = UnknownFrogs,                      data = nd,                      constants = nc,                      inits = ni,                      monitors = params,                      thin = 1,                      niter = 5000,                      nburnin = 2500,                      nchains = 3,                      samplesAsCodaMCMC = TRUE                       ) MCMCvis::MCMCtrace(frogs_cjs, pdf = F, Rhat = T, n.eff = T) kf_out <- 1- MCMCvis::MCMCsummary(frogs_kf, Rhat = F, n.eff = F) cjs_out <- 1- MCMCvis::MCMCsummary(frogs_cjs, Rhat = F, n.eff = F, params = 'eta') frogs_out <- rbind(kf_out, cjs_out) frogs_out #>         mean     sd    2.5%     50%   97.5% #> eta  0.03892 0.9830 0.07726 0.03637 0.01294 #> eta1 0.28274 0.9795 0.32314 0.28271 0.24415 gg_frogs <- data.frame(Median = frogs_out$`50%`,                        LCI = frogs_out$`2.5%`,                        UCI = frogs_out$`97.5%`,                        Model = c('Known Fate', 'CJS')) ggplot(gg_frogs, aes(x = Model, y = Median))+   geom_pointrange(aes(ymin = LCI, ymax = UCI))+   theme_bw()+   ylab('Infection Probability')+   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25))"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab10_CJS.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab10: Discrete Survival Models","text":"Using capture records Rana sierrae, write CJS model apparent survival models capture probability function sex random effect year. Make ggplot showing detection probability across time, different colors different sexes. Note: need remove frogs Ids start number 9. error dataset (example: “9.00043E+14”) reviewer journal trying run code several manuscripts make sure author’s results make sense. Unfortunately, information given, none models run. Use data(‘bad_data’) find information 3 manuscripts (Birds Occupancy, frog CJS, falcons). Diagnose problem(s) dataset. problem initial values, determine initial value needed instead make model run. need run models way, just fix information can get non-NA, non-infinite value calculate(). ’s example workflow get started first manuscript (birds): 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":"data('bad_data') birds <- bad_data$birds model <- birds$model nd <- birds$dat nc <- birds$consts ni <- birds$inits params <- birds$monitors prepnim <- nimbleModel(code = model, constants = nc,                        data = nd, inits = ni, calculate = T) prepnim$initializeInfo()  prepnim$calculate()"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab11_Multistate.html","id":"model-structure","dir":"Articles","previous_headings":"","what":"Model structure","title":"Lab11: Discrete and continuous-time multistate models","text":"example, assume alive individuals can one two states can transition state 1 state 2. Call transition rate \\eta_{1,2}. groups experience state-specific hazard rate h_1 h_2. rates constant. transition rate matrix thus: \\mathbf Q = \\left[\\begin{array} {ccc} -[\\eta_{1,2} + h_1] & \\eta_{1,2} & h_{1}\\\\ 0 & -h_2 & h_2\\\\ 0 & 0 & 0 \\end{array}\\right] Individuals can detected alive states based state-specific detection rates \\lambda_1 \\lambda_2. observation matrix : \\mathbf \\Lambda = \\left[\\begin{array} {ccc} \\lambda_1 & 0 & 0\\\\ 0 & \\lambda_2 & 0\\\\ 0 & 0 & 0 \\end{array}\\right]","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab11_Multistate.html","id":"simulating-the-data","dir":"Articles","previous_headings":"","what":"Simulating the data","title":"Lab11: Discrete and continuous-time multistate models","text":"simulate detection data, approximate continuous-time state transitions simulating individual’s state day, based state previous day. First, define parameters transition rate matrix Q: Next, create matrix s store individual’s state day. use Q simulate states: Next simulate encounter histories based detection intensity \\lambda. length encounter histories differ among individuals, first simulate observations list convert list matrix based longest detection history. also keep track state detection (state_list): fit continuous-time model, also need calculate time detections \\Delta t. individual’s first detection, \\Delta t_1 = t_1 - 0 = t_1. also include time individual’s last detection end study \\Delta t_{U+1} = T - t_{U+1}.","code":"library(expm)  set.seed(584893)  ## Set parameters N <- 100                    # Number of individuals T <- 100                    # Length of study, in days h <- c(0.0025, 0.01)        # Hazard rates eta12 <- 0.025              # Transition rate from state 1 to state 2     lambda <- c(0.25, 0.15, 0)  # Detection rates    ## Define transition rate matrix and detection matrix Q <- matrix(0, 3, 3) Q[1, 1] <- -(eta12 + h[1]) Q[1, 2] <- eta12 Q[1, 3] <- h[1]  Q[2, 2] <- -h[2] Q[2, 3] <- h[2] ## Simulate true states s <- matrix(NA, nrow = N, ncol = T) s[,1] <- 1  for(i in 1:N){   for(t in 2:T){     s[i, t] <- which(rmultinom(1, 1, prob = expm(Q)[s[i, t - 1],]) == 1)   } }  # Change state 3 to 0 (dead) s[s == 3] <- 0 ## Simulate encounter histories as a list because  ##  history length differs among individuals det_list <- state_list <- vector(mode = \"list\", length = N) U1 <- U2 <- vector(length = N)  # How many days was each individual in each state? s1 <- apply(s, 1, function(x) sum(x == 1)) s2 <- apply(s, 1, function(x) sum(x == 2))  for(i in 1:N){   U1[i] <- rpois(1, lambda[1] * s1[i]) # Number of detections   dets1 <- sort(runif(U1[i], 0, s1[i])) # Time of detections    state1 <- rep(1, U1[i])      U2[i] <- rpois(1, lambda[2] * s2[i]) # Number of detections   dets2 <- sort(runif(U2[i], 0, s2[i])) + s1[i] # Time of detections    state2 <- rep(2, U2[i])      det_list[[i]] <- c(dets1, dets2)   state_list[[i]] <- c(state1, state2) } # Total number of detections for each individual U <- U1 + U2  # Covert detections to matrix  det <- state <- matrix(0, nrow = N, ncol = max(U))  for(i in 1:N){   if(U[i] > 0){     det[i, 1:U[i]] <- det_list[[i]]     state[i, 1:U[i]] <- state_list[[i]]   } } # Calculate time difference between detections (including last detection to end of study) delta <- matrix(0, nrow = N, ncol = max(U) + 1) delta[,1] <- det[,1]  for(i in 1:N){   if(U[i] > 1){     for(j in 2:U[i]){       delta[i, j] <- det[i, j] - det[i, j - 1]     }   }   delta[i, U[i] + 1] <- T - max(det[i,]) }"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab11_Multistate.html","id":"discrete-time-model","dir":"Articles","previous_headings":"","what":"Discrete-time model","title":"Lab11: Discrete and continuous-time multistate models","text":"many ecologists familiar discrete-time multi-state models, tempting fit data binning detections discrete “occasions.” One problem , unlike data truly detected pre-defined sampling occasions, length post-hoc intervals arbitrary. shorter intervals, discrete-time model approximate continuous-time model also model loop occasions (many contain detections). Longer intervals computationally efficient less accurate. Setting issues aside, ’ll bin data 10-day intervals. First, create matrix store detections. observed states state 1, state 2, observed, pre-fill matrix 3. just fill observed states individual: Next, need fill observed states. can use cut() function quickly determine 10-day bin detection occurred . also know state detection: two things worth noting . First, example, binning data greatly reduces amount information estimate parameters. individual observed multiple times nearly intervals can “encounter” individuals per occasion discrete-time formulation lose lot data. Also, interval 8, individual observed states 1 2. discrete-time model allows single state per sampling occasion make arbitrary decision state keep one remove (Don’t ask best way , ’m sure) Now ’re ready fit model nimble. Prepare data fit model: Run model: Look traceplots:  Look model output: compare estimates simulated parameter values? Remember \\phi = e^{-ht}. detection transition probabilities?","code":"d <- matrix(3, nrow = N, ncol = 10) # empty detection matrix with 10 \"sampling occasions\" # Cut returns a factor, so convert to numeric bin <- as.numeric(cut(det_list[[6]], breaks = seq(0, T, by = 10))) obs_state <- state_list[[6]]  data.frame(bin, obs_state) #>    bin obs_state #> 1    2         1 #> 2    3         1 #> 3    3         1 #> 4    3         1 #> 5    3         1 #> 6    4         1 #> 7    4         1 #> 8    5         1 #> 9    6         1 #> 10   6         1 #> 11   6         1 #> 12   6         1 #> 13   6         1 #> 14   7         1 #> 15   7         1 #> 16   8         1 #> 17   8         1 #> 18   8         2 #> 19   9         2 #> 20  10         2 #> 21  10         2 #> 22  10         2 for(i in 1:N){   bin <- as.numeric(cut(det_list[[i]], breaks = seq(0, 100, by = 10)))   obs_state <- state_list[[i]]   for(j in unique(bin)){     d[i, j] <- min(obs_state[bin == j])   } }  ## All individuals captured in first occasion d[, 1] <- 1 dt_ms <- nimbleCode({   # Priors    phi1 ~ dbeta(1, 1)   phi2 ~ dbeta(1, 1)   psi12 ~ dbeta(1, 1)   p1 ~ dbeta(1, 1)   p2 ~ dbeta(1, 1)      # Define state-transition and observation matrices   # Define probabilities of state S(t+1) given S(t)   ps[1,1] <- phi1 * (1 - psi12)   ps[1,2] <- phi1 * psi12   ps[1,3] <- 1 - phi1   ps[2,1] <- 0   ps[2,2] <- phi2   ps[2,3] <- 1 - phi2   ps[3,1] <- 0   ps[3,2] <- 0   ps[3,3] <- 1    # Define probabilities of O(t) given S(t)   po[1,1] <- p1   po[1,2] <- 0   po[1,3] <- 1 - p1   po[2,1] <- 0   po[2,2] <- p2   po[2,3] <- 1 - p2   po[3,1] <- 0   po[3,2] <- 0   po[3,3] <- 1    # Likelihood    for (i in 1:N){     # Define latent state at first capture     z[i, f[i]] <- y[i, f[i]]     for (t in (f[i] + 1):nOcc){       # State process: draw S(t) given S(t-1)       z[i, t] ~ dcat(ps[z[i, t - 1], 1:nStates])       # Observation process: draw O(t) given S(t)       y[i, t] ~ dcat(po[z[i, t], 1:nStates])     } #t   } #i }) f <- rep(1, N) z <- z.init <- d  ### NA for occasion 1 (deterministic) or occasions individual was not captured z[,1] <- NA z[z == 3] <- NA  ### Intialize z for occasions where individual was not captured z.init[,1] <- NA z.init[d %in% c(1, 2)] <- NA   for(i in 1:N){   nd <- which(d[i,] == 3)   max1 <- max(which(d[i,] == 1), na.rm = TRUE)   z.init[i, nd[nd < max1]] <- 1   z.init[i, nd[nd > max1]] <- 2 }  nd <- list(y = d, z = z) nc <- list(N = N,            f = f,            nOcc = 10,             nStates = 3) ni <- list(phi1 = runif(1),             phi2 = runif(1),            psi12 = runif(1),            p1 = runif(1),            p2 = runif(1),            z = z.init) params <- c('phi1', 'phi2', 'psi12', 'p1', 'p2')   prepnim <- nimbleModel(code = dt_ms, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong #> [1] -2421 dt_fit <- nimbleMCMC(code = dt_ms,                      data = nd,                      constants = nc,                      inits = ni,                      monitors = params,                      thin = 1,                      niter = 5000,                      nburnin = 2500,                      nchains = 3,                      samplesAsCodaMCMC = TRUE                       ) MCMCvis::MCMCtrace(dt_fit , pdf = F, Rhat = T, n.eff = T) MCMCvis::MCMCsummary(dt_fit, Rhat = T, n.eff = T) #>         mean      sd   2.5%    50%  97.5% #> p1    0.9232 0.01633 0.8918 0.9221 0.9519 #> p2    0.7456 0.02775 0.6881 0.7461 0.7981 #> phi1  0.9549 0.01228 0.9289 0.9568 0.9743 #> phi2  0.8755 0.02325 0.8240 0.8758 0.9148 #> psi12 0.2001 0.02221 0.1600 0.2006 0.2419"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab11_Multistate.html","id":"continuous-time-model","dir":"Articles","previous_headings":"","what":"Continuous-time model","title":"Lab11: Discrete and continuous-time multistate models","text":"Fitting continuous-time version model nimble slightly challenging nimble built-matrix exponential function built-likelihood function. However, can calculate likelihoods observation hand use zeros trick added model likelihood. calculate likelihoods hand, take advantage nimble’s user defined functions creating function computes matrix exponentials using base R functions nimbleFunction() function allows us define functions use inside nimble models using notation similar creating functions R. dCTMS() function takes two matrices (Q Lambda), time detections, observed states associated detection, whether individuals last detection. function returns associated likelihood detection, using matrix algebra can read elsewhere really want know works. use function nimble model, need first compile . can test works correctly testing expm() function: Now ’re ready write model code estimate parameters. make code little easier, distinguish three groups individuals - never recountered, recaptured , captured . groups different indexing calculate likelihoods, makes slightly easier (though probably slower): Next, ’ll create data objects, constants, initiatial values. Note sort matrices number detections, first include individuals never detected, individuals detected , : Pretty close true parameter estimates! case, continuous time model took little longer run discrete model. least part matrix multiplication calculate likelihood slow compute. software (JAGS Stan) built-matrix exponential functions much faster. also likely nimble function altered calculate matrix exponential using faster operations. code also somewhat slower tended lot detections individual relative number occasions discrete-time model. Bigger dimensions loops always slows models. cases, continuous-time formulation may faster.","code":"dCTMS <- nimbleFunction(   run = function(Q = double(2),                  Lambda = double(2),                  delta = double(0),                  s1 = double(0),                  s2 = double(0),                  last = logical(0, default = 0),                  log = logical(0, default = 0)) {     P <- eigen((Q - Lambda) * delta)$vectors     D <- eigen((Q - Lambda) * delta)$values     G <- P %*% diag(exp(D)) %*% inverse(P)     if(last){       return(sum(G[s1, 1:3]))     }else{       return((G %*% Lambda)[s1, s2])     }     returnType(double(0))   }) C_dCTMS <- compileNimble(dCTMS) C_dCTMS(Q = Q, Lambda = diag(lambda), delta = 0.75, s1 = 1, s2 = 2) #> [1] 0.002388 (expm::expm((Q - diag(lambda)) * 0.75) %*% diag(lambda))[1, 2] #> [1] 0.002388  C_dCTMS(Q = Q, Lambda = diag(lambda), delta = 0.75, s1 = 1, s2 = 2, last = T) #> [1] 0.8298 sum(expm::expm((Q - diag(lambda)) * 0.75)[1,]) #> [1] 0.8298 ct_ms <- nimbleCode({   # Priors   h[1] ~ dunif(0, 0.5)   h[2] ~ dunif(0, 0.5)   eta12 ~ dunif(0, 0.5)   p[1] ~ dunif(0, 1)   p[2] ~ dunif(0, 1)   p[3] <- 0      Q[1, 1] <- -(h[1] + eta12)   Q[1, 2] <- eta12   Q[1, 3] <- h[1]   Q[2, 1] <- 0   Q[2, 2] <- -h[2]   Q[2, 3] <- h[2]   Q[3, 1] <- 0   Q[3, 2] <- 0   Q[3, 3] <- 0      # Likelihood   # 1. Individuals that were not reencountered   for (i in 1:N0){     L[i, 1] <- dCTMS(Q = Q[1:3, 1:3], Lambda = diag(p[1:3]),                       delta = delta[i, 1], s1 = 1, s2 = 1, last = 1)     LL[i] <- -log(L[i, 1])   }      # 2. Individuals that were reencountered once   for(i in (N0 + 1):(N0 + N1)){     # First encounter     L[i, 1] <- dCTMS(Q = Q[1:3, 1:3], Lambda = diag(p[1:3]),                       delta = delta[i, 1], s1 = 1, s2 = state[i, 1])          # First encounter to end of study     L[i, U[i] + 1] <- dCTMS(Q = Q[1:3, 1:3], Lambda = diag(p[1:3]),                              delta = delta[i, 2], s1 = state[i, 1],                              s2 = state[i, 1], last = 1)          LL[i] <- -sum(log(L[i, 1:2]))   }      # 2. Individuals that were reencountered more than once   for (i in (N0 + N1 + 1):N){     # First encounter     L[i, 1] <- dCTMS(Q = Q[1:3, 1:3], Lambda = diag(p[1:3]),                       delta = delta[i, 1], s1 = 1, s2 = state[i, 1])          # Between subsequent encounters     for (j in 2:U[i]){       L[i, j] <- dCTMS(Q = Q[1:3, 1:3],                         Lambda = diag(p[1:3]),                          delta = delta[i, j],                          s1 = state[i, j - 1],                          s2 = state[i, j])     } # j      # Between last detection to end of study     L[i, U[i] + 1] <- dCTMS(Q = Q[1:3, 1:3],                              diag(p[1:3]),                              delta = delta[i, U[i] + 1], s1 = state[i, U[i]],                              s2 = state[i, U[i]], last = 1)        LL[i] <- -sum(log(L[i, 1:(U[i] + 1)]))   }      # B. Zeros trick to implement the likelihood   for (i in 1:N){     phi[i] <- LL[i] + 10000     zeros[i] ~ dpois(phi[i])   } # i }) ord <- order(U)     ct_data <- list(zeros=rep(0, N)) ct_const <- list(N = N, N0 = sum(U == 0),                 N1 = sum(U==1),                U = U[ord],                 delta = delta[ord,],                 state = state[ord,]) ct_inits <- function(){list(h=runif(2, 0, 0.005),                            eta12 = runif(1, 0, 0.05), p = c(runif(2, 0, 0.5), NA))}  # Parameters monitored param <- c(\"h\", \"p\", \"eta12\")  ct_fit <- nimbleMCMC(code = ct_ms,                      data = ct_data,                      constants = ct_const,                      inits = ct_inits,                      monitors = param,                      thin = 1,                      niter = 1500,                      nburnin = 500,                      nchains = 2,                      samplesAsCodaMCMC = TRUE ) MCMCvis::MCMCsummary(ct_fit, Rhat = T, n.eff = T) #>           mean       sd      2.5%      50%    97.5% #> eta12 0.022675 0.002510 0.0177222 0.022444 0.027888 #> h[1]  0.002325 0.001038 0.0004644 0.002132 0.004493 #> h[2]  0.013566 0.002145 0.0099099 0.013234 0.018045 #> p[1]  0.254431 0.008318 0.2374105 0.254614 0.271169 #> p[2]  0.148262 0.006582 0.1367348 0.147997 0.161442 #> p[3]  0.000000 0.000000 0.0000000 0.000000 0.000000"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab11_Multistate.html","id":"extensions","dir":"Articles","previous_headings":"","what":"Extensions","title":"Lab11: Discrete and continuous-time multistate models","text":"seen several continuous time models, used constant rates. Often, rates vary time seen CMR models. Continuous-time models can accommodate temporal variation rates coding complex time cover. interested time-varying models, see paper, particularly appendices.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab11_Multistate.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab11: Discrete and continuous-time multistate models","text":"Vary length time bins discrete model. Fit one model longer bins (e.g., 25 days) one shorter bins (e.g., 5 days). parameter estimates change? Modify simulation code continuous-time model allow transitions state 2 back state 1. Set rate something relatively low, e.g. 0.01. 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"joint-live-dead-recovery-models","dir":"Articles","previous_headings":"","what":"Joint live-dead recovery models","title":"Lab12: Joint Live-Dead Recovery Models","text":"world game-species, often reward hunters report tags bands find harvested prey. every animal tagged reported, either hunters poach animal animal dies method besides human-induced mortality. non-game world, models often used situations dead animals sometimes found incidentally surveying live animals (finding marked tortoise shell surveying gopher tortoises). following graphic phidot.org MARK handbook great job explaining general idea.  diagram, can easily establish likelihood capture/encounter history (, stealing MARK handbook). likelihood 2-occasion study.","code":"knitr::include_graphics(\"Fates.png\") knitr::include_graphics(\"Likelihoods.png\")"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"example---snow-goose-data","dir":"Articles","previous_headings":"","what":"Example - Snow Goose Data","title":"Lab12: Joint Live-Dead Recovery Models","text":"Today’s data comes 18-year data set lesser snow goose (Anser caerulescens caerulescens). 18,493 adult female geese captured 1997 2014, flightless phase Nunavut, Canada. citation paper containing data can found : Weegman MD, Wilson S, Alisauskas RT, Kellett DK. Assessing bias demographic estimates joint live dead encounter models. PeerJ. 2020 Jun 23;8:e9382. doi: 10.7717/peerj.9382. PMID: 32612891; PMCID: PMC7319022. geese observed 1 3 states: 1 = alive study area 2 = recovered dead 3 = seen sake lab, using subset data first 10 years.","code":"library(WILD8370) data(snowgeese) head(snowgeese) #>    1 2 3 4 5 6 7 8 9 10 #> 57 1 3 3 3 3 3 3 3 3  3 #> 12 1 3 3 3 3 3 3 3 3  3 #> 70 1 3 3 3 3 3 3 3 3  3 #> 16 1 3 3 3 3 3 3 3 3  3 #> 17 1 3 3 3 3 3 3 3 3  3 #> 36 1 3 3 3 3 3 3 3 3  3"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"model","dir":"Articles","previous_headings":"","what":"Model","title":"Lab12: Joint Live-Dead Recovery Models","text":"like code model geese can 5 possible true states: 1 = Pre-alive 2 = alive study area 3 = alive permanently emigrated 4 = recently dead recovered (notice state inherently includes detection death) 5 = recently dead, recovered, dead 1 time period (absorbing) treat \\phi, p, gam time-varying. \\phi p,modeled logit-link intercept random effect year.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"transition-matrix","dir":"Articles","previous_headings":"Model","what":"Transition Matrix","title":"Lab12: Joint Live-Dead Recovery Models","text":"build model sections, starting transition matrix.","code":"for(t in 1:nocc){   gamma[1,1,t] <- 1-gam[t]            # Pr(pre alive t -> pre alive t+1)   gamma[1,2,t] <- gam[t]              # Pr(pre alive t -> alive and here t+1)   gamma[1,3,t] <- 0                   # Pr(pre alive t -> alive but gone t+1)   gamma[1,4,t] <- 0                   # Pr(pre alive t -> recent dead and recovered t+1)   gamma[1,5,t] <- 0                   # Pr(pre alive t -> absorbing dead t+1)      gamma[2,1,t] <- 0                   # Pr(alive t -> pre-alive t+1)   gamma[2,2,t] <- phi[t]*f            # Pr(alive t -> alive and here t+1)   gamma[2,3,t] <- phi[t]*(1-f)        # Pr(alive t -> alive but gone t+1)   gamma[2,4,t] <- (1-phi[t])*r        # Pr(alive t -> recent dead and recovered t+1)   gamma[2,5,t] <- (1-phi[t])*(1-r)    # Pr(alive t -> absorbing dead t+1)      gamma[3,1,t] <- 0                   # Pr(alive but gone t -> pre-alive t+1)   gamma[3,2,t] <- 0                   # Pr(alive but gone t -> alive and here t+1)   gamma[3,3,t] <- phi[t]              # Pr(alive but gone t -> alive but gone t+1)   gamma[3,4,t] <- (1-phi[t])*r        # Pr(alive but gone t -> recent dead and recovered t+1)   gamma[3,5,t] <- (1-phi[t])*(1-r)    # Pr(alive but gone t -> absorbing dead t+1)      gamma[4,1,t] <- 0                   # Pr(Recently dead t -> pre-alive t+1)   gamma[4,2,t] <- 0                   # Pr(Recently dead t -> alive and here t+1)   gamma[4,3,t] <- 0                   # Pr(Recently dead t -> alive but gone t+1)   gamma[4,4,t] <- 0                   # Pr(Recently dead t -> recent dead and recovered t+1)   gamma[4,5,t] <- 1                   # Pr(Recently dead t -> absorbing dead t+1)      gamma[5,1,t] <- 0                   # Pr(absorbing dead t -> pre-alive t+1)   gamma[5,2,t] <- 0                   # Pr(absorbing dead t -> alive and here t+1)   gamma[5,3,t] <- 0                   # Pr(absorbing dead t -> alive but gone t+1)   gamma[5,4,t] <- 0                   # Pr(absorbing dead t -> recent dead and recovered t+1)   gamma[5,5,t] <- 1                   # Pr(absorbing dead t -> absorbing dead t+1)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"observation-matrix","dir":"Articles","previous_headings":"Model","what":"Observation Matrix","title":"Lab12: Joint Live-Dead Recovery Models","text":"Next let’s make observation matrix. 5 X 3 matrix since can observe geese 3 possible states (alive, dead, observed).","code":"omega[1,1,t] <- 0           # Pr(pre alive t and detected as alive)   omega[2,1,t] <- p[t]        # Pr(alive and here t and detected as alive)   omega[3,1,t] <- 0           # Pr(alive and gone t and detected as alive)   omega[4,1,t] <- 0           # Pr(recent dead and recovered t and detected as alive)   omega[5,1,t] <- 0           # Pr(absorbing dead t and detected as alive)      omega[1,2,t] <- 0           # Pr(pre alive t and recovered dead)   omega[2,2,t] <- 0           # Pr(alive and here t and recovered dead)   omega[3,2,t] <- 0           # Pr(alive and gone t and recovered dead)   omega[4,2,t] <- 1           # Pr(recent dead and recovered t and recovered dead)   omega[5,2,t] <- 0           # Pr(absorbing dead t and recovered dead)      omega[1,3,t] <- 1           # Pr(pre alive t and not observed)   omega[2,3,t] <- 1-p[t]      # Pr(alive and here t and not observed)   omega[3,3,t] <- 1           # Pr(alive and gone t and not observed)   omega[4,3,t] <- 0           # Pr(recent dead and recovered t and not observed)   omega[5,3,t] <- 1           # Pr(absorbing dead t and not observed)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"initial-matrix-and-priors","dir":"Articles","previous_headings":"Model","what":"Initial Matrix and Priors","title":"Lab12: Joint Live-Dead Recovery Models","text":"Let’s get initial probability state add priors. Rather writing dbeta prior delta, ’m just going use dirichlet prior, ensure resulting probabilities sum 1. dirichlet can thought multinomial probabilities.","code":"delta[1:5] ~ ddirch(alphas[1:5])  for(t in 1:nocc){   logit(p[t]) <- mu.p + eps.p[t] #detection   logit(phi[t]) <- mu.phi + eps.phi[t] #survival   eps.p[t] ~ dnorm(0, sd = sd.p)   eps.phi[t] ~ dnorm(0, sd = sd.phi)   gam[t] ~ dbeta(1,1) #removal from augmented population }  r ~ dbeta(1, 1) #recovery probability f ~ dbeta(1, 1) #fidelity probability sd.phi ~ dexp(1) sd.p ~ dexp(1)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"full-model","dir":"Articles","previous_headings":"Model","what":"Full Model","title":"Lab12: Joint Live-Dead Recovery Models","text":"","code":"livedeadHMM <- nimbleCode({ for(t in 1:nocc){   gamma[1,1,t] <- 1-gam[t]            # Pr(pre alive t -> pre alive t+1)   gamma[1,2,t] <- gam[t]              # Pr(pre alive t -> alive and here t+1)   gamma[1,3,t] <- 0                   # Pr(pre alive t -> alive but gone t+1)   gamma[1,4,t] <- 0                   # Pr(pre alive t -> recent dead and recovered t+1)   gamma[1,5,t] <- 0                   # Pr(pre alive t -> absorbing dead t+1)      gamma[2,1,t] <- 0                   # Pr(alive t -> pre-alive t+1)   gamma[2,2,t] <- phi[t]*f            # Pr(alive t -> alive and here t+1)   gamma[2,3,t] <- phi[t]*(1-f)        # Pr(alive t -> alive but gone t+1)   gamma[2,4,t] <- (1-phi[t])*r        # Pr(alive t -> recent dead and recovered t+1)   gamma[2,5,t] <- (1-phi[t])*(1-r)    # Pr(alive t -> absorbing dead t+1)      gamma[3,1,t] <- 0                   # Pr(alive but gone t -> pre-alive t+1)   gamma[3,2,t] <- 0                   # Pr(alive but gone t -> alive and here t+1)   gamma[3,3,t] <- phi[t]              # Pr(alive but gone t -> alive but gone t+1)   gamma[3,4,t] <- (1-phi[t])*r        # Pr(alive but gone t -> recent dead and recovered t+1)   gamma[3,5,t] <- (1-phi[t])*(1-r)    # Pr(alive but gone t -> absorbing dead t+1)      gamma[4,1,t] <- 0                   # Pr(Recently dead t -> pre-alive t+1)   gamma[4,2,t] <- 0                   # Pr(Recently dead t -> alive and here t+1)   gamma[4,3,t] <- 0                   # Pr(Recently dead t -> alive but gone t+1)   gamma[4,4,t] <- 0                   # Pr(Recently dead t -> recent dead and recovered t+1)   gamma[4,5,t] <- 1                   # Pr(Recently dead t -> absorbing dead t+1)      gamma[5,1,t] <- 0                   # Pr(absorbing dead t -> pre-alive t+1)   gamma[5,2,t] <- 0                   # Pr(absorbing dead t -> alive and here t+1)   gamma[5,3,t] <- 0                   # Pr(absorbing dead t -> alive but gone t+1)   gamma[5,4,t] <- 0                   # Pr(absorbing dead t -> recent dead and recovered t+1)   gamma[5,5,t] <- 1                   # Pr(absorbing dead t -> absorbing dead t+1)      omega[1,1,t] <- 0                   # Pr(pre alive t and detected as alive)   omega[2,1,t] <- p[t]                # Pr(alive and here t and detected as alive)   omega[3,1,t] <- 0                   # Pr(alive and gone t and detected as alive)   omega[4,1,t] <- 0                   # Pr(recent dead and recovered t and detected as alive)   omega[5,1,t] <- 0                   # Pr(absorbing dead t and detected as alive)      omega[1,2,t] <- 0                   # Pr(pre alive t and recovered dead)   omega[2,2,t] <- 0                   # Pr(alive and here t and recovered dead)   omega[3,2,t] <- 0                   # Pr(alive and gone t and recovered dead)   omega[4,2,t] <- 1                   # Pr(recent dead and recovered t and recovered dead)   omega[5,2,t] <- 0                   # Pr(absorbing dead t and recovered dead)      omega[1,3,t] <- 1                   # Pr(pre alive t and not observed)   omega[2,3,t] <- 1-p[t]              # Pr(alive and here t and not observed)   omega[3,3,t] <- 1                   # Pr(alive and gone t and not observed)   omega[4,3,t] <- 0                   # Pr(recent dead and recovered t and not observed)   omega[5,3,t] <- 1                   # Pr(absorbing dead t and not observed)    logit(p[t]) <- mu.p + eps.p[t] #detection   logit(phi[t]) <- mu.phi + eps.phi[t] #survival   eps.p[t] ~ dnorm(0, sd = sd.p)   eps.phi[t] ~ dnorm(0, sd = sd.phi)   gam[t] ~ dbeta(1,1) #removal from augmented population } #end t  delta[1:5] ~ ddirch(alphas[1:5]) r ~ dbeta(1, 1) #recovery probability f ~ dbeta(1, 1) #fidelity probability sd.phi ~ dexp(1) sd.p ~ dexp(1) mu.p ~ dnorm(0, 1) mu.phi ~ dnorm(0, 1)  for(i in 1:M){     y[i,1:nocc] ~ dDHMMo(init =  delta[1:5],                          probObs = omega[1:5, 1:3, 1:nocc],                          probTrans = gamma[1:5, 1:5, 1:(nocc-1)],                          len = nocc,                          checkRowSums = 1)   } #end M })"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"analysis","dir":"Articles","previous_headings":"","what":"Analysis","title":"Lab12: Joint Live-Dead Recovery Models","text":"Time make nimble objects. Check model correctly setup. step may take several minutes (’s big model!)","code":"M <- nrow(snowgeese)*2 nocc <- ncol(snowgeese) y <- as.matrix(snowgeese) nind <- nrow(snowgeese) y <- rbind(y, array(3, c(M-nind, nocc))) nd <- list(y = y) nc <- list(M = M, nocc = nocc, alphas = rep(1, 5)) ni <- list(eps.p = rep(0, nocc),            eps.phi = rep(0, nocc),            sd.phi = rexp(1),            sd.p = rexp(1),            mu.p = rnorm(1),            mu.phi = rnorm(1),            r = rbeta(1,1,1),            f = rbeta(1,1,1),            gam = rbeta(nocc, 1,1),            delta = rdirch(1, rep(1, 5)) )    params <- c('mu.p', 'eps.p', 'sd.p', 'mu.phi', 'eps.phi', 'sd.phi', 'gam',  'delta', 'r', 'f', 'phi', 'p') library(nimbleEcology) prepnim <- nimbleModel(code = livedeadHMM, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() prepnim$calculate() #> [1] -4286 library(parallel) library(coda) cl <- makeCluster(3) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"ni\",  \"nc\", 'nd', \"livedeadHMM\", 'params')) geese <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) library(nimbleEcology) prepnim <- nimbleModel(code = livedeadHMM, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = params, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 5000, nburnin = 1000, thin = 1) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res) }) #this will take awhile and not produce any progress bar geese <- as.mcmc.list(geese) stopCluster(cl)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"output","dir":"Articles","previous_headings":"","what":"Output","title":"Lab12: Joint Live-Dead Recovery Models","text":"Time inspect chains (inspect real analysis)  Let’s check population estimate time. Remember : \\Large   N_t = \\frac{N_{observed}}{p_t}  can also evaluate p \\phi changed time:  Finally, can learn fidelity recovery probabilities. ’ll notice many estimates wide CIs - models data hungry!","code":"MCMCvis::MCMCtrace(geese, params = 'delta',  pdf = F, Rhat = T, n.eff = T) ps <- MCMCsummary(geese, params = 'p') seenalive <- colSums(snowgeese == 1) LCI <- seenalive/ps$`2.5%` UCI <- seenalive/ps$`97.5%` Median <- seenalive/ps$`50%`  gg_ab <- data.frame(Median = Median,                     LCI = LCI,                     UCI = UCI,                     Time = 1:nocc)  ggplot(gg_ab, aes(x = Time, y= Median))+   geom_pointrange(aes(ymin = LCI,ymax = UCI))+   geom_line()+   theme_bw()+   theme(axis.text= element_text(size = 20), axis.title = element_text(size = 20))+   xlab(\"Year\")+   ylab(\"Abundance\") phis <- MCMCsummary(geese, params = 'phi') gg_ps <- data.frame(Median = c(ps$`50%`, phis$`50%`),                     LCI = c(ps$`2.5%`, phis$`2.5%`),                     UCI = c(ps$`97.5%`, phis$`97.5%`),                     Time = rep(1:nocc, 2),                     Estimate = rep(c('Detection probability', 'Survival'), each = nocc))  ggplot(gg_ps, aes(x = Time, y= Median, col = Estimate, group = Estimate))+   geom_pointrange(aes(ymin = LCI,ymax = UCI), position = position_dodge(width = .25))+   geom_line()+   theme_bw()+   theme(axis.text= element_text(size = 20), axis.title = element_text(size = 20))+   xlab(\"Year\")+   ylab(\"Probability\") MCMCvis::MCMCsummary(geese, params = c('f', 'r')) #>      mean      sd   2.5%     50%  97.5% Rhat n.eff #> f 0.17550 0.04312 0.1066 0.16989 0.2755 1.00   518 #> r 0.05657 0.02750 0.0260 0.04936 0.1301 1.02   290"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab12_JointLiveDead.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab12: Joint Live-Dead Recovery Models","text":"Using data joint live-dead recovery model ran , run hidden markov model allow dead recovery. data, animals recovered dead, treat unobserved time period. Graphically compare estimates \\phi N estimated lab . including dead recoveries impact results? Using dipper data used class, run hidden markov model accounts location capture state process. Instead 3 states class (pre-alive, alive, dead), now 5 states (pre-alive, alive site 1, alive site 2, alive site 3, dead). 4 observation states (seen site 1, seen site 2, seen site 3, seen). Keep phi constant sites, allow p vary site. Make graph showing estimated population site time. 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"what-is-data-simulation","dir":"Articles","previous_headings":"","what":"What is data simulation","title":"Simulating linear regression data","text":"Data simulation technique generating random data stochastic processes known parameters. Although framed “data simulation”, already done several times semester. example, simple data simulation generate random samples normal distribution known mean (\\mu=3) variance (\\sigma^2=0.75^2 = 0.5625). exercise, ’ll learn simulating data slightly complex models similar ones might use analyze data. example, imagine simple single-season occupancy model probability occupancy \\psi=0.75 detection probability p=0.4. words1: z_i \\sim Bernoulli(\\psi) y_i \\sim Bernoulli(z_i \\times p) can simulate data set model using lines R code: seven lines code, now fake data set fed occupancy model estimate \\psi p.","code":"x <- rnorm(100, 3, 0.75) nSites <- 100  # Number of sites nVisits <- 3 psi <- 0.75    # Occupancy probability p <- 0.4       # Detection probability  z <- rbinom(n = nSites, size = 1, prob = psi)     ## Generate true state of each site;                                                   ## nb Bernoulli = binomial with size = 1 y <- rbinom(n = nSites*nVisits, size = 1, prob = z * p)   ## Generate observations y <- matrix(y, nrow = nSites, ncol = nVisits)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"why-simulate-data","dir":"Articles","previous_headings":"","what":"Why simulate data?","title":"Simulating linear regression data","text":"first, may seem strange generate fake data set just can run modeling exercise get answers already know. data simulation powerful technique toolbox ecological modeler. number reasons data simulation useful2: Truth known: Usually apply model data, don’t know true parameter values generated data. case, may able fit model ’ll never know got right answer. simulated data, can check whether model returns known parameter values. useful way make sure code think ’s . Sampling error: already learned, sampling error inherent part ecological analysis. noise results sampling error makes harder detect true signals process model. real data, single data set, makes hard understand effect sampling error inference. simulated data, can generate hundreds even thousands data sets process/observation models, allowing observe effects sampling error directly. Check characteristics estimators: Related point 1, complex model just single data set ’s difficult determine whether estimators using well-behaved; , return estimates unbiased precise. simulated data, can directly quantify properties. Power analysis: varying effect sizes sample sizes simulated data, can easily perform power analyses. Using simulated data way can useful designing field studies helping interpret inferences data collected analyzed. Check identifiability/estimability parameters: Bayesian models, can always obtain posterior distributions every parameter model. However, posteriors always useful. cases, data may provide little--information value parameter therefore posterior distribution parameter simply determined prior. lack identifiability may caused intrinsic properties model (example, two parameters completely confounded different combinations parameter values likelihood) data provide enough information estimate parameters model (example, regression model dozens predictors observations). Although rigorous methods testing intrinsic identifiability, task can extremely difficult complex hierarchical models. Simulated data allow check whether parameters model can estimated generating replicate data sets properties (sample size, etc) data. Check robustness violations model assumptions: models assumptions data generated. assumptions stem way formulate process observation models. course, assumptions violated degree real data sets. simulated data sets, can generate data know violate assumptions model one ways (e.g., generating heterogeneous survival probabilities model assumes constant survival). comparing parameters estimates “mis-specified” data sets, can gauge degree inferences sensitive violations. Better understand model: One good way test whether really understand model see can write code simulate data model. many cases, exercise uncover misunderstandings lack understanding model actually . Simulating data good way make sure understand parameter model actually represents. can simulate data part model, chances can also figure model may working way think . short, simulating data great way develop deeper understanding model.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"simulating-linear-regression-data","dir":"Articles","previous_headings":"","what":"Simulating linear regression data","title":"Simulating linear regression data","text":"exercise, simulate visualize data generated general process model: linear regression. ’m probably guilty animal focused, assume response variable model counts number seeds produced flowers rare, endangered orchid. ecologists studying orchid, want know whether seed production related number visits orchid’s specialist pollinator. Perhaps , can increase growth rate orchid population boosting abundance pollinator (assuming number visits function pollinator abundance). hypothesize seed number increase linearly pollinator visitation. hypothesis can translated process model: y_i = \\alpha + \\beta * x_i + \\epsilon_i \\epsilon_i \\sim Normal(0, \\sigma^2) y_i number seeds counted flower x_i number pollinator visits. simplicity, assume record y_i x_i without error. model, \\alpha \\beta regression coefficients govern relationship seed counts visitation \\epsilon_i normally distribution error term. may recognize basic linear regression model single covariate x. Choosing probability distribution describe seed counts Earlier semester, discussed Poisson distribution default distribution data positive integers (e.g., count data). However, case, choice linear regression implies normally distributed data. choice justified? First , remember linear model composed two parts: response = deterministic\\; part+stochastic\\; part distributional assumptions linear model refer residuals (\\epsilon_i’s). , assume stochastic error response variable (y_i) error equally likely produce values larger smaller value predicted deterministic portion model (\\alpha + \\beta * x_i). small counts, Poisson distribution asymmetrical, meaning likely generate values larger mean smaller mean. can see clearly histogram , generated Poisson(\\lambda = 2):  case, assuming error terms normally distribution likely inappropriate. However, counts get larger, Poisson distribution starts appear “normal”:  case, \\mu = \\lambda = 500. counts get bigger, little difference results linear regression Poisson GLM. ***","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"step-0-set-up-your-workspace-and-directories","dir":"Articles","previous_headings":"","what":"Step 0: Set up your workspace and directories","title":"Simulating linear regression data","text":"Hopefully already created main directory course started homework assignment last week. already done , create sub-directory folder called data. exercise, create new script add whatever sub-directory use store analysis scripts (alternatively, add script data data-raw folders since ’ll use create data. Just use whatever system makes sense ). next section, copy code markdown file script.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"step-1-set-the-model-parameters","dir":"Articles","previous_headings":"","what":"Step 1: Set the model parameters","title":"Simulating linear regression data","text":"first step simulating data set fixed values needed generate stochastic data. usually includes sample size, covariate parameter values fixed value relevant analysis. case, ’ll first set number flowers counted seeds : Next, need generate covariate values, case pollination visits. ’ll store values data frame assume number visits ranges 0 25: common task data preparation adding new variables derived variables raw data. example, might want add new variable scaled values covariate. analyses, good practice scale covariate values mean 0 extend far 0 (large values (positive negative) can create numerical issues fitting models). next center scale visit covariate (’ll manually also done using built-function scale() simply simulating data normal distribution first place). tidyverse, workhorse adding new variables dplyr::mutate(): can check covariate now mean = 0 sd = 1. NOTE can sometimes confusing know functions come packages (know function R default using). reason, ’s good practice get habit using package::function() syntax, mutate() . Using syntax makes explicit package/function intend use makes code easier understand reduces potential errors. benefit, can stop using library(package) beginning script. Finally, need set parameter values regression models. understanding parameter represents helpful. example, \\alpha expected number seeds covariate value 0 (centered visits, interpret \\alpha expected number seeds mean number visits): Now set \\beta coefficient. already said \\beta positive (seed count increases visits). ’s left decide specific value. Remember interpret \\beta additional number seeds 1 sd increase number visits.","code":"N <- 175 # Number of flowers sim_df <- data.frame(visits = runif(N, 0, 25)) # Number of pollination visits sim_df <- dplyr::mutate(sim_df, visits.c = (visits - mean(visits))/sd(visits)) alpha <- 250 # Expected number of seeds at mean number of visits beta <- 50   # Effect of visits on seed count"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"step-2-generate-expected-counts","dir":"Articles","previous_headings":"","what":"Step 2: Generate expected counts","title":"Simulating linear regression data","text":"generate simulated seed counts flower, first calculate \\mu_i, expected seeds flower. get values simply plugging observed visitation values site linear model: \\begin{bmatrix}     \\mu_1 \\\\     \\mu_2 \\\\     \\mu_3 \\\\     .\\\\     .\\\\     .\\\\     \\mu_N \\end{bmatrix} = \\begin{bmatrix}     1  & visits_1\\\\     1  & visits_2\\\\     1  & visits_3\\\\     . & .\\\\     . & .\\\\     . & .\\\\     1  & visits_N \\end{bmatrix} \\times \\begin{bmatrix}     \\alpha\\\\     \\beta \\end{bmatrix} remember matrix algebra, multiplying covariate matrix coefficient matrix : 1 \\times \\alpha + visits_i \\times \\beta matrix predicted responses called linear predictor. Now refreshed memory basic linear model structure, let’s add predicted seed counts data frame:","code":"sim_df <- dplyr::mutate(sim_df, mu = alpha + beta*visits.c)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"step-2b-plot-relationships","dir":"Articles","previous_headings":"Step 2: Generate expected counts","what":"Step 2b: Plot relationships","title":"Simulating linear regression data","text":"Whenever simulate data, ’s useful plot data early often. often difficult know ahead time exactly response values complex model produce. Plots great way quickly assess whether simulation producing values consistent domain expertise.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"brief-intro-to-ggplot2","dir":"Articles","previous_headings":"Step 2: Generate expected counts > Step 2b: Plot relationships","what":"Brief intro to ggplot2","title":"Simulating linear regression data","text":"consistent use tidyverse, create plots using ggplot2(). power flexibility ggplot2 come ’s consistent structure. Although bit overwhelming first, get hang structure actually makes quite easy create highly customized publication-quality graphics. plots created using ggplot2 use underlying structure: \\underbrace{ggplot}_{initiate\\; plot}(\\underbrace{data = df}_{data\\;frame},\\; \\underbrace{aes(x =\\; , y = \\;)}_{plot\\; attributes}) + \\underbrace{geom\\_line()}_{geometry} ggplot() function initiates new plot. function, tell ggplot2 data frame using plot tell map attributes data visual properties figures. Attributes mapped inside aes() argument. Attributes usually include location (x-axis y-axis placement), color, size, shape, line type, many others. general, attribute mapped one column data frame. ggplot() function simply initiates graph run just portion code get blank graph. can see creating new plot showing relationship elevation (x-axis plot) predicted abundance (y-axis):  can see ggplot created figure correct axes labels. data. ’s didn’t tell ggplot type geometry use represent data. Geometry refers actual type geometric object(s) want use display data. Common geometries include points (e.g., scatterplot), lines (e.g., time series), bars (e.g., histograms). many others. add geometry, can see data:  can see model predicts seed counts ranging 171.66 individuals 337.96. reasonable? knows, made species. wasn’t, good time go back play different parameter values generate abundances consistent domain expertise. example, model predicts \\approx 171.66 seeds flower 0 pollination visits. Maybe makes sense (perhaps orchids can self-pollinate necessary) maybe doesn’t. doesn’t, need re-think model structure.","code":"ggplot(data = sim_df, aes(x = visits, y = mu)) ggplot(data = sim_df, aes(x = visits, y = mu)) + geom_point()"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"step-3-generate-the-actual-seed-counts","dir":"Articles","previous_headings":"","what":"Step 3: Generate the actual seed counts","title":"Simulating linear regression data","text":"far, simulated seed counts contain stochastic variation (visitation covariate stochastic given value, predicted counts completely deterministic). create realistic data set, need add process variance (\\sigma^2_p). example, requires setting another parameter controls amount process variation. Now simply generate random seed counts using linear predictor process variation  expected, seed count increases \\mu, though can see process variation added model stage.","code":"sigma <- 7.5 ### Generate actual abundance for each site sim_df <- dplyr::mutate(sim_df, y = rnorm(n = N, mu, sigma))  ### Plot lambda vs. N ggplot(data = sim_df, aes(x = mu, y = y)) + geom_point() +   scale_x_continuous(expression(mu[i])) +   scale_y_continuous(expression(y[i])) +   geom_abline(slope = 1, intercept = 0)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"step-4-save-the-simulated-data","dir":"Articles","previous_headings":"","what":"Step 4: Save the simulated data","title":"Simulating linear regression data","text":"Now simulated data set, let’s save ’s available future use. many ways save objects R one well-behaved saveRDS(). want use object future, run3:","code":"saveRDS(object = sim_df, file = \"data/sim_seed_counts.rds\") sim_df <- readRDS(\"data/sim_seed_counts.rds\")"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"homework-questions-part-1-easier","dir":"Articles","previous_headings":"","what":"Homework Questions Part 1 (Easier)","title":"Simulating linear regression data","text":"performing study deer CWD want simulate 20 years abundance data declining population. general sense population declining average 5% year (e.g. E(N_{t+1}) = N_t\\lambda_t), realized population stochasticity (N_{t+1} \\sim Pois(E(N_{t+1}))). population year 1 475 deer. Write simulation data use ggplot visualize realized population values across 20 years. Use set.seed() command make reproducible. decide add complexity simulation \\lambda_t vary time. real data collected 1994 2016, ’ll use environmental data time period. think lambda might positive relationship degree days (daily average number degrees 4 C) December year (e.g. \\lambda_t = exp(l_0 + l_1*degreedays). Using ‘Weather_data’ provided WILD8370 package (accessed via: data(Weather_data) ), simulate 20 years population abundance. sure choose reasonable values l_0 l_1 \\lambda_t stays 1.1 values degree days. Don’t forget scale covariate! Use ggplot visualize realized population values across 20 years. (Note: years multiple weather stations recording degree days, use average weather stations covariate)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab2_simulation.html","id":"homework-questions-part-2-slightly-harder","dir":"Articles","previous_headings":"","what":"Homework Questions Part 2 (Slightly Harder)","title":"Simulating linear regression data","text":"GPS data leopard frogs want fit random-walk movement model. However, use real data, want make fake data set first can use develop full model. want simulation contain: locations (1 per day 50 days, continuous coordinates). day location measured (day 1 day 50) plan fit following model: s_{,x,t=1} \\sim Uniform(0, 1) s_{,y,t=1} \\sim Uniform(0, 1) s_{, x, t+1} \\sim Normal(s_{, x, t}, \\sigma ^2 ) s_{, y, t+1} \\sim Normal(s_{, y, t}, \\sigma^2 ) \\sigma \\sim Exp(3) individual, x y x y coordinates respectively, \\sigma related frog’s average movement previous location. , location frog time t+1 normal centered around location time t. assume ’ll 10 frogs dataset. Simulate data based model. Store results 3-d array matrix (10 2 50) realize ’ll need adjust simulation adjust model. Wrap simulation function allow run simulation different values \\sigma number frogs return simulated movements. (E.g. Sim <- function(sigma, nfrogs)) 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"markov-chain-monte-carlo","dir":"Articles","previous_headings":"","what":"Markov chain Monte Carlo","title":"Lab 3: Basic MCMC","text":"basic steps Metropolis sampler : Choose initial value \\theta (call \\theta^1) Propose new value \\theta^* proposal distribution Compute probability accepting \\theta^* using joint distributions evaluated \\theta^* previous value \\theta^{k-1} Accept \\theta* (.e., \\theta^{k} = \\theta^*) probability estimated step 2, otherwise retain previous value (.e., \\theta^{k} = \\theta^{k-1}","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"why-bother","dir":"Articles","previous_headings":"","what":"Why Bother?","title":"Lab 3: Basic MCMC","text":"First, let’s simulate fake data binomial distribution example. Maybe ’re flipping coin want know ’s fair. flip 20 times results get: Let’s take look data table: Looks like 16 zeros (“failures”) 4 1’s (“successes”). Probably fair coin. want estimate probability success? truly fair, ’d expect probability 0.5. reallllllly wanted , try figure entire “hand” guessing bunch values using probability density function binomial distribution tell us close correct value. ’s might look: know equation binomial distribution size 20 : \\frac{20!}{(20-X!)X!}(p^X)(1-p)^{20-X} go finding p? Well, start, ’s probability density 4 successes p = .1? Okay, high. p = .4? Hmm, little lower. p = .7? Oof, lower . eventually ’d see numbers larger (yay) smaller (less good answers). guessed enough numbers, ’d probably find highest say p pretty close whatever value . Obviously picking random values p hand isn’t efficient. Let’s function us graph result :  can see value p probably close .3 ’s highest probability density , can also see method tried bunch points (AKA wastes lot time) around values basically know aren’t going good answers. instance, ’re pretty sure coin isn’t going success probability .9, kind prefer model spend much time testing values . way function spend time “good” areas, trying estimate exact value p less time “bad” areas answer unlikely? equations math isn’t just one distribution bunch distributions combined? yes! ’s called MCMC. makes ineffective plug--chug methods much faster.","code":"set.seed(20) x <- rbinom(20, size = 1, prob = .2) x #>  [1] 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 table(x) #> x #>  0  1  #> 16  4 dbinom(4, 20, .1) #> [1] 0.08978 dbinom(4, 20, .4) #> [1] 0.03499 dbinom(4, 20, .7) #> [1] 5.008e-06 pick.ps <- function(n){   p <- runif(n, min = 0, max = 1)   d <- dbinom(4,20,p)   return(data.frame(p = p, d=d)) }"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"seed-count-model","dir":"Articles","previous_headings":"","what":"Seed count model","title":"Lab 3: Basic MCMC","text":"generated seed count data, assuming linear model linked pollination visits seed counts: y_i = \\alpha + \\beta * x_i + \\epsilon_i \\epsilon_i \\sim Normal(0, \\sigma^2) model includes 3 random variables want estimate posterior distributions : \\alpha, \\beta, \\sigma^2 (remember y_i x_i observed therefore treated fixed known rather random variables governed probability distributions). : [\\alpha, \\beta, \\sigma^2|y_i] \\propto [y_i|\\alpha, \\beta, \\sigma^2][\\alpha][\\beta][\\sigma^2] \\tag{1} Sticking data generating model, can define likelihood model using Normal distribution. given values \\alpha, \\beta, \\sigma^2, estimate likelihood : [y_i|\\alpha, \\beta, \\sigma] = \\prod_{=1}^N Normal(y_i|\\alpha + \\beta \\times x_i, \\sigma^2) \\alpha, \\beta, \\sigma^2 random variables, must define prior distributions . \\alpha \\beta can positive negative real numbers (’ll ignore now know sign data generating model), can use normal priors: [\\alpha] = Normal(\\alpha|0, 50) [\\beta] = Normal(\\beta|0, 50) relatively non-informative priors intercept slope coefficients. also need model \\sigma. Side note: lot MCMC codes (BUGS language) model normal distribution Normal(mean, precision) precision = (1/\\sigma^2). \\sigma, use relatively diffuse gamma prior: [\\sigma] = Gamma(\\sigma|1, 1)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"custom-functions","dir":"Articles","previous_headings":"","what":"Custom functions","title":"Lab 3: Basic MCMC","text":"iteration sampler, need perform set tasks, namely estimating likelihood joint distributions proposed current parameter values. copying pasting code necessary calculations point sampler code need . However, time find copying pasting code twice, ’s good idea consider wrapping code function, .","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"function-for-calculating-likelihood","dir":"Articles","previous_headings":"Custom functions","what":"Function for calculating likelihood","title":"Lab 3: Basic MCMC","text":"Given values \\alpha, \\beta, \\sigma, can estimate likelihood R using dnorm() function (note take sum log likelihoods rather product likelihoods avoid numerical issues): turn code function, first open new script call calc_like.R. Save R/ sub-directory. script contain function code. function code : allows us put data (y), covariate values (x), values \\alpha, \\beta, \\sigma arguments function return log likelihood. Save code script close script.","code":"# Calculate the predicted count for each flower   lp <- alpha + beta * x      # Calculate the likelihood of our data given the model   sum(dnorm(y, lp, sigma, log = TRUE)) calc_like <- function(y, alpha, beta, sigma, x) {   lp <- alpha + beta * x   ll <- sum(dnorm(y, lp, sigma, log = TRUE))   return(ll) }"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"functions-for-calculating-prior-probabilities","dir":"Articles","previous_headings":"Custom functions","what":"Functions for calculating prior probabilities","title":"Lab 3: Basic MCMC","text":"also need functions estimate prior probabilities specific values \\alpha, \\beta, \\sigma. Create new script titled priors.R save R/ sub-directory. following functions take values parameter estimate prior probability given prior distributions defined :","code":"priorAlpha <- function(alpha, mu = 0, sigma = 50){   prob <- dnorm(alpha, mu, sigma, log = TRUE)   return(prob) }   priorBeta <- function(beta, mu = 0, sigma = 50){   prob <- dnorm(beta, mu, sigma, log = TRUE)   return(prob) }   priorsigma <- function(sigma, shape1 = 0.1, rate1 = 0.1){   prob <- dgamma(sigma, shape = shape1, rate = rate1, log = TRUE)   return(prob) }"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"metropolis-sampler","dir":"Articles","previous_headings":"Custom functions","what":"Metropolis sampler","title":"Lab 3: Basic MCMC","text":"Now ready create sampler. Remember multiple parameters, need define full conditionals. random variable model, define full conditional including element right hand side eq. 1 contains parameter (note change \\sigma): [\\alpha|.] = [y_i|\\alpha, \\beta, \\sigma][\\alpha] [\\beta|.] = [y_i|\\alpha, \\beta, \\sigma][\\beta] [\\sigma|.] = [y_i|\\alpha, \\beta, \\sigma][\\sigma] sampler, loop parameter, going Metropolis steps treating parameters known (based current value chain). clear, walking R code hopefully make concrete (don’t worry trying copy run code section, simply illustration. next section, slightly modify code run sampler seed count data). First, set length chains, set tuning parameter proposal distribution, create empty data frame store samples binary variable estimating acceptance rate: Next, randomly generate initial values parameter. Remember quite big numbers data generating model create initial values magnitude: final “set ” step estimate likelihood data given initial values. , use calc_like() function created earlier: Now create actual sampler. , create loop implements steps iteration chains. , simply go step make sure understand ’s . ’ll start going Metropolis steps \\alpha (though remember order doesn’t matter): Let’s go line line. step 1a, generate new value \\alpha normal distribution centered previous value. Remember tuning parameter set earlier determines big jumps current proposed values. step 1b, use calc_like() function estimate likelihood data given new value \\alpha previous values \\beta \\sigma. mean say treat parameters fixed known. step 1c, estimate joint probability current proposed values adding log likelihood log prior probability (note sum log values product probabilities). step 1d, estimate probability accepting new value. Note proposed value likely current value (R > 1), function return 1. Otherwise, returns ratio. closer joint probabilities , closer R 1. less likely proposed value relative current value, smaller R . means much less likely accept new values \\alpha lot less probable current value (though impossible). Finally, step 1e accept reject proposed value. Note compare R random value generated Uniform(0,1) distribution. R=1, always greater value always accept proposed value (mcmc_df$alpha[] <- cand) update current likelihood match current value \\alpha. R<1, turns testing whether R less randomly generated value Uniform(0,1) ensures accept \\alpha probability R (’ll leave prove based properties uniform distribution). Next thing \\beta, using new value alpha (mcmc_df$alpha[]) previous value \\sigma (mcmc_df$sigma[- 1]): Finally, update \\sigma using new values \\alpha \\beta. difference \\sigma >0, normal proposal distribution used \\alpha \\beta work (create negative values). learned lecture, create proposal distribution generates positive value (e.g., gamma) use moment matching estimate parameters regards current \\sigma tuning parameter. ’ll use slightly less efficient approach demonstrate different ways enforcing behavior want sampler. cases, simply add small value current \\sigma automatically reject proposed value ’s less 0. small value can negative positive allow us explore posterior.","code":"## Length of chains   nIter <- 10000  ## Tuning parameter   tuning <- 1.5  ## Empty data frame to store posterior samples of each parameter   mcmc_df <- data.frame(x = 1:nIter,                         alpha = numeric(nIter),                         beta = numeric(nIter),                         sigma = numeric(nIter),                         accept.alpha = 0,                         accept.beta = 0,                         accept.sigma = 0) ## Initial values   mcmc_df$alpha[1] <-  runif(1, 200, 300)   mcmc_df$beta[1] <- runif(1, 25, 75)   mcmc_df$sigma[1] <- runif(1, 0, 10) ## Initial likelihood likelihood <- calc_like(y = y, alpha = mcmc_df$alpha[1],                         beta = mcmc_df$beta[1], sigma = mcmc_df$sigma[1],                         x = x) ######################## #### 1. Update alpha ########################  ## 1a: Generate candidate value   cand <- rnorm(1, mcmc_df$alpha[i - 1], tuning)  ## 1b: Calculate likelihood at candidate value   cand_like <- calc_like(y = y, alpha = cand, beta = mcmc_df$beta[i-1], sigma = mcmc_df$sigma[i-1], x = x)    ## 1c: Calculate likelihood * prior at old value and candidate value   jointOld <- likelihood + priorAlpha(mcmc_df$alpha[i-1])    jointCand <- cand_like + priorAlpha(cand)     ## 1d: Acceptance probability    R <- min(1, exp(jointCand - jointOld))  ## 1e: Decide whether to accept or not   if(R > runif(1)) {   # if accepted       mcmc_df$alpha[i] <- cand       likelihood <- cand_like     } else {       mcmc_df$alpha[i] <- mcmc_df$alpha[i-1]   } ######################## #### 2. Update beta ########################  ## 2a: Generate candidate value   cand <- rnorm(1, mcmc_df$beta[i - 1], tuning)  ## 2b: Calculate likelihood at candidate value   cand_like <- calc_like(y = y, alpha = mcmc_df$alpha[i], beta = cand,                           sigma = mcmc_df$sigma[i-1], x = x)    ## 2c: Calculate likelihood * prior at old value and candidate value   jointOld <- likelihood + priorBeta(mcmc_df$beta[i-1])    jointCand <- cand_like + priorBeta(cand)     ## 2d: Acceptance probability    R <- min(1, exp(jointCand - jointOld))  ## 2e: Decide whether to accept or not   if(R > runif(1)) {   # if accepted       mcmc_df$beta[i] <- cand       likelihood <- cand_like     } else {       mcmc_df$beta[i] <- mcmc_df$beta[i-1]     } ######################## #### 3. Update sigma ########################  ## 3a: Generate candidate value     cand <- mcmc_df$sigma[i-1] + runif(1,  -.1, 0.1)     # If candidate value is outside [0,Inf], keep the old value of sigma     if (cand < 0) {       mcmc_df$sigma[i] <- mcmc_df$sigma[i-1]     } else { ## 3b: Calculate likelihood at candidate value       cand_like <- calc_like(y = y, alpha = mcmc_df$alpha[i], beta = mcmc_df$beta[i],                            sigma = cand, x = x)        ## 3c: Calculate likelihood * prior at old value and candidate value       jointOld <- likelihood + priorsigma(mcmc_df$sigma[i-1])       jointCand <- cand_like + priorsigma(cand)  ## 3d: Acceptance probability        R <- min(1, exp(jointCand - jointOld))  ## 3e: Decide whether to accept or not       if(R > runif(1)) {   # if accepted         mcmc_df$sigma[i] <- cand         likelihood <- cand_like       } else {         mcmc_df$sigma[i] <- mcmc_df$sigma[i-1]       }     }   }"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"running-the-sampler","dir":"Articles","previous_headings":"","what":"Running the sampler","title":"Lab 3: Basic MCMC","text":"Finally, can run sampler. Create new script called metroplis.R (something similar) put scripts/. First, load packages need use source() function read custom functions (source() runs .R files given argument case make functions available sampler): Next, read simulated data set initial values MCMC: Now take sampler code put inside loop create chains. Note likelihood functions need reference simulated data covariates: point, able run script start finish create posterior samples parameter.","code":"library(WILD6900)  source(\"R/calc_like.R\") source(\"R/priors.R\") ## Read simulated data   sim_dat <- readRDS(\"data/sim_seed_counts.rds\")  ## Length of chains   nIter <- 10000  ## Tuning parameter   tuning <- 1.5  ## Empty data frame to store posterior samples of each parameter   mcmc_df <- data.frame(x = 1:nIter,                         alpha = numeric(nIter),                         beta = numeric(nIter),                         sigma = numeric(nIter),                         accept.alpha = 0,                         accept.beta = 0,                          accept.sigma = 0)  ## Initial values   mcmc_df$alpha[1] <-  runif(1, 200, 300)   mcmc_df$beta[1] <- runif(1, 25, 75)   mcmc_df$sigma[1] <- runif(1, 0, 10)    ## Initial likelihood   likelihood <- calc_like(y = sim_dat$y, alpha = mcmc_df$alpha[1],                         beta = mcmc_df$beta[1], sigma = mcmc_df$sigma[1],                         x = sim_dat$visits.c) for(i in 2:nIter){ ######################## #### 1. Update alpha ########################  ## 1a: Generate candidate value   cand <- rnorm(1, mcmc_df$alpha[i - 1], tuning)  ## 1b: Calculate likelihood at candidate value   cand_like <- calc_like(y = sim_dat$y, alpha = cand,                           beta = mcmc_df$beta[i-1], sigma = mcmc_df$sigma[i-1],                           x = sim_dat$visits.c)    ## 1c: Calculate likelihood * prior at old value and candidate value   jointOld <- likelihood + priorAlpha(mcmc_df$alpha[i-1])    jointCand <- cand_like + priorAlpha(cand)     ## 1d: Acceptance probability    R <- min(1, exp(jointCand - jointOld))  ## 1e: Decide whether to accept or not   if(R > runif(1)) {   # if accepted       mcmc_df$alpha[i] <- cand       likelihood <- cand_like       mcmc_df$accept.alpha[i] <- 1     } else {       mcmc_df$alpha[i] <- mcmc_df$alpha[i-1]     }    ######################## #### 2. Update beta ########################  ## 2a: Generate candidate value   cand <- rnorm(1, mcmc_df$beta[i - 1], tuning)  ## 2b: Calculate likelihood at candidate value   cand_like <- calc_like(y = sim_dat$y, alpha = mcmc_df$alpha[i], beta = cand,                           sigma = mcmc_df$sigma[i-1], x = sim_dat$visits.c)    ## 2c: Calculate likelihood * prior at old value and candidate value   jointOld <- likelihood + priorBeta(mcmc_df$beta[i-1])    jointCand <- cand_like + priorBeta(cand)     ## 2d: Acceptance probability    R <- min(1, exp(jointCand - jointOld))  ## 2e: Decide whether to accept or not   if(R > runif(1)) {   # if accepted       mcmc_df$beta[i] <- cand       likelihood <- cand_like       mcmc_df$accept.beta[i] <- 1     } else {       mcmc_df$beta[i] <- mcmc_df$beta[i-1]     }    ######################## #### 3. Update sigma ########################  ## 3a: Generate candidate value     cand <- mcmc_df$sigma[i-1] + runif(1,  -1, 1)     # If candidate value is outside [0,Inf], keep the old value of sigma     if (cand < 0) {       mcmc_df$sigma[i] <- mcmc_df$sigma[i-1]     } else { ## 3b: Calculate likelihood at candidate value       cand_like <- calc_like(y = sim_dat$y, alpha = mcmc_df$alpha[i], beta = mcmc_df$beta[i],                            sigma = cand, x = sim_dat$visits.c)        ## 3c: Calculate likelihood * prior at old value and candidate value       jointOld <- likelihood + priorsigma(mcmc_df$sigma[i-1])       jointCand <- cand_like + priorsigma(cand)  ## 3d: Acceptance probability        R <- min(1, exp(jointCand - jointOld))  ## 3e: Decide whether to accept or not       if(R > runif(1)) {   # if accepted         mcmc_df$sigma[i] <- cand         likelihood <- cand_like         mcmc_df$accept.sigma[i] <- 1       } else {         mcmc_df$sigma[i] <- mcmc_df$sigma[i-1]       }     } }"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"checking-the-output","dir":"Articles","previous_headings":"","what":"Checking the output","title":"Lab 3: Basic MCMC","text":"Now can determine whether sampler returns data generating values data simulation. assessing output MCMC chains, good first diagnostic usually checking trace plots:  chain \\alpha moved rapidly initial value stationary posterior distribution first samples obviously bias parameter estimates include posterior. Let’s remove using dplyr function slice():  looks better. Now can estimate mean 95% credible interval posterior: Pretty close data generating value 250. Now let’s check \\beta:  Also pretty close data generating value 50. Finally, \\sigma  Wasn’t easy?","code":"ggplot(mcmc_df, aes(x = x, y = alpha)) + geom_path() mcmc_df <- dplyr::slice(mcmc_df, 1000:nIter)  ggplot(mcmc_df, aes(x = x, y = alpha)) + geom_path() mean(mcmc_df$alpha) #> [1] 251.3 quantile(mcmc_df$alpha, probs = c(0.025, 0.975)) #>  2.5% 97.5%  #> 250.2 252.4 ggplot(mcmc_df, aes(x = x, y = beta)) + geom_path() mean(mcmc_df$beta) #> [1] 49.1 quantile(mcmc_df$beta, probs = c(0.025, 0.975)) #>  2.5% 97.5%  #> 48.02 50.17 ggplot(mcmc_df, aes(x = x, y = sigma)) + geom_path() mean(mcmc_df$sigma) #> [1] 7.424 quantile(mcmc_df$sigma, probs = c(0.025, 0.975)) #>  2.5% 97.5%  #> 6.690 8.254"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab 3: Basic MCMC","text":"simulation, set \\alpha = 250, provided reasonable, diffuse prior example . happens give restrictive (wrong) prior \\alpha? Change priorAlpha function instead match \\alpha \\sim Uniform(0, 150). Make sure also change line match new prior: Run sampler print chain \\alpha. see? Go back original prior alpha. didn’t discuss much lab, tuning parameter affects fast sampler explore state space. ’s really small, might take forever. ’s really large, can make giant jumps reject lot possible values, wastes time causes sampler ‘stall’. can see acceptance rate sampler calculating mean accept.alpha, accept.beta accept.sigma columns. Run sampler tuning parameter .05 50. Calculate acceptance rates 3 parameters. notice?","code":"mcmc_df$alpha[1] <-  runif(1, 200, 300) #change this to something reasonable"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab3_BasicMCMC.html","id":"homework-1-bonus-point","dir":"Articles","previous_headings":"","what":"Homework (+1 Bonus Point)","title":"Lab 3: Basic MCMC","text":"generated seed count data, assuming linear model linked pollination visits seed counts: y_i = \\alpha + \\beta * x_i + \\epsilon_i. used simulated data, know model ahead time. However real life, wouldn’t know true model ahead time might try fit different model data. Adding already , create MCMC also includes second beta term looks visits squared: y_i = \\alpha + \\beta * x_i + \\beta_2 * x_i^2 + \\epsilon_i. Run MCMC look output. MCMC estimate \\beta_2? Hint: Approach problem one step time. - Begin changing mcmc_df allow store additional beta parameter. - Next, choose initial value beta2. - Change calc_like function include beta2. Remember lp just alpha + betax + beta2x*x. - Optionally, create another function priorBeta2 (just re-use priorBeta function beta2 well). Next create section sampler update Beta2. look almost identical one already present Beta. - Run model. - Finally, use quantile function look estimate: quantile(mcmc_df$beta2, probs = c(0.025,0.5, 0.975)) 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"GLMs for modeling count data","text":"data activity comes long-term project monitored population peregrine falcons nesting French Jura 1964 2003 1.  Load inspect data: falcons dataframe 4 columns: Year: year (integer). Pairs: number adult pairs (integer). R.pairs: number reproductive pairs (integer). Eyasses: number fledged young (integer).","code":"library(WILD8370) data(\"falcons\") head(falcons)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"analysis-1-change-in-population-size","dir":"Articles","previous_headings":"","what":"Analysis 1: Change in population size","title":"GLMs for modeling count data","text":"first model fit examines change number adult falcon pairs time. Plotting data shows change linear:  short decline beginning study period, population increased dramatically perhaps reaching carrying capacity.","code":"ggplot(falcons, aes(x = Year, y = Pairs)) + geom_point() + stat_smooth(se = FALSE)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"modeling-non-linear-effects-using-linear-models","dir":"Articles","previous_headings":"Analysis 1: Change in population size","what":"Modeling non-linear effects using linear models","title":"GLMs for modeling count data","text":"can model non-linear change abundance , definition, linear models model linear effects? Using polynomials! Remember equation curved line single peak (bottom): <<<<<<< HEAD y = + b \\times x + c \\times x^2 ======= \\Large y = + b \\times x + c \\times x^2 >>>>>>> ce5d54b074616c65c7754d9e4f0af1acd8908fa4  maximum (minimum) value y, b value x maximum (minimum) occurs c determines whether peak maximum (c<0) minimum (c>0). can add complex shape adding additional polynomial terms. example, including cubic term creates s-shaped curve: <<<<<<< HEAD y = + b \\times x + c \\times x^2 + d \\times x^3 ======= \\Large y = + b \\times x + c \\times x^2 + d \\times x^3 instance: >>>>>>> ce5d54b074616c65c7754d9e4f0af1acd8908fa4  Including polynomial terms linear predictor model gives us enormous flexibility model non-linear relationships using GLMs.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"modeling-change-in-falcon-counts","dir":"Articles","previous_headings":"Analysis 1: Change in population size","what":"Modeling change in falcon counts","title":"GLMs for modeling count data","text":"build model falcon data, need define components required GLMs (distribution, link function, linear predictor). counts, natural choice distribution : C_t \\sim Poisson(\\lambda_t) C_t observed count year t \\lambda_t expected count. learned lecture, conventional link function count data log-link: log(\\lambda_t) = log(E(\\lambda_t)) Finally, need write linear predictor. Based preliminary visualization data, cubic polynomial might appropriate capture non-linear change time: log(\\lambda_t) = \\alpha + \\beta_1 \\times year_t + \\beta_2 \\times year^2_t + \\beta_3 \\times year^3_t","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"accessing-and-viewing-the-nimble-model","dir":"Articles","previous_headings":"Analysis 1: Change in population size","what":"Accessing and viewing the Nimble model","title":"GLMs for modeling count data","text":"can copy paste following code R script. file, can see use relatively non-informative normal priors regression coefficients. can also see likelihood statement similar linear regression model last lecture, minor differences. First, assume observed falcon counts come Poisson distribution, use dpois(lambda[]) rather dnorm(mu[], tau). Also, apply log-link function predicted counts (log(lambda[])=...). Notice Nimble allows model transformed predicted counts left hand side linear predictor equation Lab Questions Plot histogram random samples normal prior used model (remember convert precision 0.33 standard deviation). can see, vague normal priors used past. advantage using less-vague normal priors? linear regression model fit last lecture, also prior \\tau, (inverse) process variance. include parameter model? Creating lambda[] object strictly necessary since deterministic function year. wanted fewer lines code, include linear predictor directly inside dpois() function instead lambda[], though need appropriately transform linear predictor. transformation use put linear predictor count scale?","code":"library(nimble) falcon_mod <- nimbleCode({   # Priors   alpha ~ dnorm(0, 0.33)   beta1 ~ dnorm(0, 0.33)   beta2 ~ dnorm(0, 0.33)   beta3 ~ dnorm(0, 0.33)    # Likelihood   for (i in 1:n){     C[i] ~ dpois(lambda[i])     log(lambda[i]) <- alpha + beta1 * year[i] + beta2 * pow(year[i],2) + beta3 * pow(year[i],3)   } #end i })"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"fitting-the-model","dir":"Articles","previous_headings":"Analysis 1: Change in population size","what":"Fitting the model","title":"GLMs for modeling count data","text":"fitting model, need prepare input Nimble includes: storing data named list storing constants named list creating function randomly generate initial values parameter creating vector parameters want Nimble monitor set MCMC settings ’ve mentioned several times, ’s often bad idea include covariate values far 0. reason, first scale year mean=0 sd=1: Now ’re ready run model. easiest way run Nimble model run one command: However, sometimes (get advanced), ’ll want able customize Nimble runs little bit . ’s also much easier error check initial values models take long time run run step step. ’s look like one chain: View portion results (printing lambda values takes much room): Note beta2 appears cross 0 - ’s much effect year^2 lambda. seems reasonable, let’s make sure Rhat values less 1.1. , ’ll use gelman.diag() function coda package. parameters appear converged. usual, let’s check trace plots see look:  monitoring lambda can also plot predicted counts along observed counts. First, need calculate posterior means upper/lower bounds 95% credible interval add falcons data frame, use ggplot visualize:","code":"year <- (falcons$Year - mean(falcons$Year))/sd(falcons$Year)  nimdat <- list(C = falcons$Pairs)  nimconsts <- list(year = year, n = nrow(falcons))  niminits <- function(){list(alpha = rnorm(1), beta1 = rnorm(1), beta2 = rnorm(1), beta3 = rnorm(1))}  params <- c(\"alpha\", \"beta1\", \"beta2\", \"beta3\", \"lambda\")  nC <- 3 #chains  nI <- 10000 #iterations  nB <- 2500 #burnin  nT <- 1 #thin falcon_res <- nimbleMCMC(code = falcon_mod,                      data = nimdat,                      constants = nimconsts,                      inits = niminits(),                      monitors = params,                      thin = nT,                      niter = nI,                      nburnin = nB,                      nchains = nC,                      samplesAsCodaMCMC = TRUE                       ) prepnim <- nimbleModel(code = falcon_mod, constants = nimconsts,                            data = nimdat, inits = niminits(), calculate = T) prepnim$initializeInfo() #will tell you what is or isn't intialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = params, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = nI, nburnin = nB, thin = nT) falcon_res <- (as.mcmc(as.matrix(Compnim$mvSamples))) summary(falcon_res[,c('alpha', 'beta1', 'beta2', 'beta3', 'lambda[1]', 'lambda[2]', 'lambda[3]')])$quantiles library(coda) gelman.diag(falcon_res, multivariate = F)$psrf[1:10,] #don't want to print out all 40 lambdas #>           Point est. Upper C.I. #> alpha          1.002      1.008 #> beta1          1.004      1.011 #> beta2          1.004      1.012 #> beta3          1.004      1.009 #> lambda[1]      1.003      1.007 #> lambda[2]      1.003      1.006 #> lambda[3]      1.003      1.006 #> lambda[4]      1.002      1.006 #> lambda[5]      1.002      1.006 #> lambda[6]      1.002      1.007 # View traceplots for alpha, beta1, beta2, and beta3 (not for lambda) MCMCvis::MCMCtrace(falcon_res[,params[-5],], Rhat = T, pdf = F) #get the quantiles for just the lambda parameters: lambdas <- summary(falcon_res[,paste0('lambda[', 1:40, ']')])$quantiles   falcons <- dplyr::mutate(falcons, lambda = lambdas[,3],                                    q2.5 = lambdas[,1],                                    q97.5 = lambdas[,5])  ggplot(falcons) +    geom_ribbon(aes(x = Year, ymin = q2.5, ymax = q97.5), fill = \"grey90\") +   geom_path(aes(x = Year, y = lambda), color = \"red\") +   geom_point(aes(x = Year, y = Pairs)) +   scale_y_continuous(\"Pairs\")+   theme_bw()"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"analysis-2-nest-success-model","dir":"Articles","previous_headings":"","what":"Analysis 2: Nest success model","title":"GLMs for modeling count data","text":"Next, let’s use falcons data set fit another type GLM - binomial GLM. Hopefully exercise show ’re comfortable writing coding GLM components (distribution, link function, linear predictor), extremely easy fit models different distributional assumptions. estimate reproductive success (.e., probability pair successfully produces offspring), model number reproductive pairs (falcons$R.Pairs) function total number pairs (falcons$Pairs). total number reproductive pairs exceed total number pairs, counts falcons$.RPairs bounded less falcons$Pairs. case, Poisson distribution appropriate count model. Instead, use binomial distribution: C_t \\sim binomial(N_t, p_t) goal model p_t, probability nesting successfully year. case, log link appropriate - p_t bound 0 1. probabilities, logit link generally appropriate link function: logit(p_t) = log\\bigg(\\frac{p_t}{1-p_t}\\bigg) Following Kéry & Schaub, ’ll model probability quadratic function year: logit(p_t) = \\alpha + \\beta_1 \\times year_t + \\beta_2 \\times year^2_t last example, can copy code . Note Nimble order arguments binomial probability size, unlike type R: , prepare data run model:","code":"pairs_mod <- nimbleCode({   # Priors   alpha ~ dnorm(0, 0.33)   beta1 ~ dnorm(0, 0.33)   beta2 ~ dnorm(0, 0.33)    # Likelihood   for (t in 1:nyears){     C[t] ~ dbinom(p[t], N[t])     logit(p[t]) <- alpha + beta1 * year[t] + beta2 * pow(year[t],2)   } #end i }) #using standardized year from above nimdat2 <- list(C = falcons$R.Pairs) nimconsts2 <- list(N = falcons$Pairs, year = year, nyears = nrow(falcons)) niminits2 <- function(){list(alpha = rnorm(1), beta1 = rnorm(1), beta2 = rnorm(1))}  params2 <- c(\"alpha\", \"beta1\", \"beta2\", \"p\")  nC <- 3  nI <- 10000  nB <- 2500  nT <- 1  pairs_res <- nimbleMCMC(code = pairs_mod,                      data = nimdat2,                      constants = nimconsts2,                      inits = niminits2(),                      monitors = params2,                      thin = nT,                      niter = nI,                      nburnin = nB,                      nchains = nC,                      samplesAsCodaMCMC = TRUE                       ) summary(pairs_res[,c('alpha', 'beta1', 'beta2', 'p[1]', 'p[2]', 'p[3]')])$quantiles # View traceplots for alpha, beta1, and beta2(not for p) MCMCvis::MCMCtrace(pairs_res[,params2[-4],], pdf = F, Rhat = T) ps <- summary(pairs_res[,paste0('p[', 1:40, ']')])$quantiles  falcons <- dplyr::mutate(falcons, p = ps[,3],                                    q2.5_p = ps[,1],                                    q97.5_p = ps[,5])  ggplot(falcons) +    geom_ribbon(aes(x = Year, ymin = q2.5_p, ymax = q97.5_p), fill = \"grey90\") +   geom_path(aes(x = Year, y = p), color = \"red\") +   geom_point(aes(x = Year, y = R.Pairs/Pairs)) +   scale_y_continuous(\"Pairs\")+   theme_classic()"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab4_glms.html","id":"homework-questions","dir":"Articles","previous_headings":"","what":"Homework Questions","title":"GLMs for modeling count data","text":"Using ’ve learned lab, write model expected number nestlings (Eyasses) per reproducing pair (R.pairs) year. Begin writing mathematical formulation model Write Nimble code. Provide model data, constants initial values Check output convergence Display ggplot showing expected number nestlings per reproducing pair year (95% credible interval) vs raw data. Don’t forget title axis labels. may help plot data first see type data working :  second analysis lab, used binomial GLM describe proportion successful peregrine pairs per year French Jura mountains. see connections three important types GLMs, first use Poisson GLM model number successful pairs (thus disregarding fact binomial total varies year), second, use normal GLM . graph, compare predicted numbers successful pairs every year three models (binomial, Poisson, normal GLMs). [assignment stolen directly WinBUGS book, blame Marc Kéry Michael Schaub one.] Note: find normal distribution model predicts extremely low counts, sure look priors. selected correctly, see 3 models roughly overlap raw data. 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ? 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":"ggplot(falcons, aes(x = Year, y = Eyasses/R.Pairs))+   geom_line(lwd = 1, lty = 2)+   geom_point()+   geom_smooth()+   theme_bw()"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"objectives","dir":"Articles","previous_headings":"","what":"Objectives","title":"Lab6: Missing Data","text":"Analyze data using random fixed effects Deal missing data Gain experience Nimble","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"european-hares","dir":"Articles","previous_headings":"","what":"European Hares","title":"Lab6: Missing Data","text":"Data today comes Swiss hare data included Marc Kéry’s 2010 book, Introduction WinBUGS Ecologists. data contain replicated counts Brown hares (Lepus europaeus) conducted 17 years (1992-2008) 56 sites 8 regions Switzerland. year, two counts conducted two-week period. Sites vary area, elevation, belong two types habitat (arable grassland). can read study : https://www.sciencedirect.com/science/article/pii/S0006320710004921?via=ihub  Let’s begin taking look data: now, let’s focus hares Central region. can start simple model random effects binomial observation process. Since count data, natural choice process model poisson log link. Perhaps hare abundance affected size site (area) landuse type (landuse) detection probability different different sites (site1). Let site (site1) study area surveyed year t.: log(\\lambda_{,t}) = \\beta_0 + \\beta_1[landuse_i] + \\beta_2*area_{}   N_{,t} \\sim Pois(\\lambda_{,t})  , let y represent number hares observed survey k given year: y_{,t,k} \\sim Binomial(N_{,t}, p_{}) p_i = ln(\\frac{\\alpha[site]}{1-\\alpha[site]}) (Remember writing ln(\\frac{x}{1-x}) logit(f(x)). )","code":"data(\"swiss_hares\") head(hares) #>   no site     region    site2 area elevation landuse year count1 count2 #> 1  1 AG01 CH.Central Reusstal 2.23       384  arable 1992     NA     NA #> 2  2 AG01 CH.Central Reusstal 2.23       384  arable 1993     NA     NA #> 3  3 AG01 CH.Central Reusstal 2.23       384  arable 1994     NA     NA #> 4  4 AG01 CH.Central Reusstal 2.23       384  arable 1995      6      4 #> 5  5 AG01 CH.Central Reusstal 2.23       384  arable 1996      7      5 #> 6  6 AG01 CH.Central Reusstal 2.23       384  arable 1997      3      3 #>   mean.density #> 1           NA #> 2           NA #> 3           NA #> 4        2.691 #> 5        3.139 #> 6        1.345 library(dplyr) Central_hares <- hares %>% filter(region == 'CH.Central')"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"building-the-nimble-model","dir":"Articles","previous_headings":"","what":"Building the Nimble model","title":"Lab6: Missing Data","text":"can less work backwards mathematical model write nimble model. Ignoring correct formatting, : Now can just add loops get indexing correct choose priors. Let’s start making landuse fixed effect. Now can get data, constants, inits ready.","code":"library(nimble) library(coda) nimbleCode({   #Process model    log(lambda[i,t]) <- beta0 + beta1[landuse[i]] + beta2*site_area[i]    N[i,t] ~ dpois(lambda[i,t])        #observation model    y[i,t,k] ~ dbinom(size = N[i,t],p =p[i])    logit(p[i]) <- alpha[site[i]] }) hare_mod <- nimbleCode({   for(i in 1:nsites){     for(t in 1:nyears){       #Process model       log(lambda[i,t]) <- beta0 + beta1[landuse[i]] + beta2*site_area[i]       N[i,t] ~ dpois(lambda[i,t])              for(k in 1:2){ #could also do 1:ncounts[t] if we wanted to be fancy         #observation model         y[i,t,k] ~ dbinom(size = N[i,t],p =p[i])       }     }     logit(p[i]) <- alpha[site[i]]   } #priors:   beta0 ~ dnorm(0, 1)   beta2 ~ dnorm(0, 1)   for(m in 1:nlandtypes){     beta1[m] ~ dnorm(0, 1)   }   for(j in 1:nsites){     alpha[j] ~ dnorm(0, 1)   } })"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"in-class-question","dir":"Articles","previous_headings":"Building the Nimble model","what":"In Class Question","title":"Lab6: Missing Data","text":"parameters need initial values? parameters objects need included constants?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"missing-data-values","dir":"Articles","previous_headings":"Building the Nimble model","what":"Missing Data Values","title":"Lab6: Missing Data","text":"Now come across common issue - missing data. sites surveyed years. ? cut years sites surveyed, ’d end throwing away lot data. ’re also still going need deal sites surveyed given year. solve , can adjust code also include parameter effort indicating site surveyed session. effort 0, detections survey round also 0 (since didn’t survey). Based code, see need observations 3-d array, sites rows, time columns sessions (1 2) third dimension. easy way turn character strings numbers first turn factors, convert numeric. can create empty array y (counts) effort (binary indicating surveyed). Now can fill arrays using… wait … -loops!","code":"hare_mod <- nimbleCode({   for(i in 1:nsites){     for(t in 1:nyears){       #Process model       log(lambda[i,t]) <- beta0 + beta1[landuse[i]] + beta2*site_area[i]       N[i,t] ~ dpois(lambda[i,t])              for(k in 1:2){ #could also do 1:ncounts[t] if we wanted to be fancy         #observation model         y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])       }     }          logit(p[i]) <- alpha[site[i]]   } #priors:   beta0 ~ dnorm(0, 1)   beta2 ~ dnorm(0, 1)   for(m in 1:nlandtypes){     beta1[m] ~ dnorm(0, 1)   }   for(j in 1:nsites){     alpha[j] ~ dnorm(0, 1)   } }) Central_hares$site_n <- as.numeric(as.factor(Central_hares$site)) y <- array(0, dim = c(length(unique(Central_hares$site_n)),                                 length(1992:2008),                                  2)) effort <- array(1, dim = c(length(unique(Central_hares$site_n)),                                 length(1992:2008),                                  2)) for(i in 1:nrow(Central_hares)){   mysite <- Central_hares$site_n[i]   myyear <- Central_hares$year[i] - 1991 #so that year 1 will be 1992   y[mysite, myyear, ] <- c(Central_hares$count1[i], Central_hares$count2[i]) } effort[is.na(y)] <- 0 #if y is NA, we didn't survey y[is.na(y)] <- 0 #if y is NA, that means our count is 0  hare.dat <- list(y =y) #for nimble"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"constants","dir":"Articles","previous_headings":"Building the Nimble model","what":"Constants","title":"Lab6: Missing Data","text":"nimble need separate data (stochastic known information) constants (unchanging known information). Let’s make constants list next.","code":"constants_prep <- Central_hares %>%     mutate(landuse_n = as.numeric(factor(landuse))) %>%     group_by(site) %>%     summarize(         landuse = first(landuse_n),  # Get the landuse type for each site         area = first(area),        # Get the area for each site         .groups = \"drop\"           # To remove grouping after summarizing     ) constants_prep$area_s <- scale(constants_prep$area) #scale the area head(constants_prep) #> # A tibble: 6 × 4 #>   site  landuse  area area_s[,1] #>   <chr>   <dbl> <dbl>      <dbl> #> 1 AG01        1  2.23    -0.861  #> 2 AG02        1  3.58    -0.573  #> 3 AG03        1  4.79    -0.315  #> 4 AG04        1  5.8     -0.0999 #> 5 LU01        2 16.5      2.18   #> 6 LU07A       2  5.85    -0.0892 hare.consts <- list(nsites = length(unique(Central_hares$site_n)),                     nyears = length(1992:2008),                      landuse = constants_prep$landuse,                     site_area = c(constants_prep$area_s), #remove the 'attributes' part                     site = 1:nrow(constants_prep),                     effort = effort,                     nlandtypes = 2)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"initial-values","dir":"Articles","previous_headings":"Building the Nimble model","what":"Initial Values","title":"Lab6: Missing Data","text":"Now need initialize model. Reminder need initialize anything comes distribution already known. case, means N, beta0, beta1, beta2 alpha need initial values. many ways initialize model, easy way often set betas 0 except intercepts. N, know y result N*p, can start N max(y)/p site.","code":"N.init <- array(NA, dim = c(hare.consts$nsites, hare.consts$nyears)) for(i in 1:nrow(N.init)){   N.init[i,] <- max(y[i,,])/.5  #can use .5 b/c the mean of the prior alpha is 0, which becomes p = .5     } hare.inits <- list(beta0 = rnorm(1, 0, 1), #match the prior                    beta1 = rep(0, 2), #effect of two land types                    beta2 = 0, #effect of area                    alpha = rnorm(hare.consts$nsites, 0, 1), #match prior                    N = N.init                    )"},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"parallel-processing","dir":"Articles","previous_headings":"Run the Model","what":"Parallel Processing","title":"Lab6: Missing Data","text":"past, ’ve run Nimble 3 chains, one . fine, means wait chain finished start next one. save time, let’s run three chains time using computer’s multi-core system. work, ’ll need use parallel package R. several ways run parallel chains R, ’s preferred method. begin loading parallel coda packages R, using makeCluster() command prepare run three chains parallel Next export needed information cluster. includes initial values, data, constants, model code. Finally, put relevant parts model run clusterEvalQ() function. like method can first test code indivdiually running line inside function run together ’re positive ’ve de-bugged everything. computer, takes minute. ’ve run model parallel, ’s good practice close Cluster.","code":"library(parallel) cl <- makeCluster(3) #each chain will be run separately  library(coda) clusterExport(cl = cl, varlist = c(\"hare.inits\", \"hare.dat\", \"hare.consts\", \"hare_mod\")) hares.out <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) #Specify params: pars = c('beta0', 'beta1', 'beta2', 'alpha', 'N')  prepnim <- nimbleModel(code = hare_mod, constants = hare.consts,                            data = hare.dat, inits = hare.inits, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = pars, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 150000, nburnin = 50000, thin = 2) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res)  }) #this will take awhile and not produce any progress bar.  #You know it's done when the hares.out object is created in your environment. hares.out <- as.mcmc.list(hares.out) stopCluster(cl)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"inspecting-output","dir":"Articles","previous_headings":"","what":"Inspecting Output","title":"Lab6: Missing Data","text":"Let’s take look model told us. can start beta alpha values. can use grep function select variables don’t contain word ‘N’ (abundance) . case, ’ll ask R column names first list (chain 1) use grep tell us names contains ‘N’. ’ll save parameters without N new object. Let’s plot beta values using MCMCvis package Oh dear. alpha parameters look great, beta parameters look pretty terrible. can see chains lot trouble mixing. Often can happen parameters identifiable. ’s pretty easy see happen - effect area abundance 0, ’d left equation says abundance sum two numbers without information ’s impossible solve variables! aren’t sure two parameters correlated (potentially unidentifiable), can graph correlation chains using mcmcOutput package. plot, can see beta0 strong negative correlation (-1) beta1[1] beta1[2]. good.","code":"betas <- hares.out[,-grep('N', colnames(hares.out[[1]])),] library(MCMCvis) MCMCvis::MCMCtrace(betas, pdf = F) library(mcmcOutput) mcmcOutput::crosscorrPlot(betas[,8:11,], addSpace = c(0, 1))"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"re-write-the-model","dir":"Articles","previous_headings":"","what":"Re-write the Model","title":"Lab6: Missing Data","text":"Let’s fix model removing beta0. also need remove initial value beta0 inits object. Time re-run. Grab alpha beta params plot Much better!","code":"hare_mod <- nimbleCode({   for(i in 1:nsites){     for(t in 1:nyears){       #Process model       log(lambda[i,t]) <- beta1[landuse[i]] + beta2*site_area[i]       N[i,t] ~ dpois(lambda[i,t])              for(k in 1:2){ #could also do 1:ncounts[t] if we wanted to be fancy         #observation model         y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])       }     }          logit(p[i]) <- alpha[site[i]]   } #priors:   beta2 ~ dnorm(0, 1)   for(m in 1:nlandtypes){     beta1[m] ~ dnorm(0, 1)   }   for(j in 1:nsites){     alpha[j] ~ dnorm(0, 1)   } }) hare.inits$beta0 <- NULL cl <- makeCluster(3) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"hare.inits\", \"hare.dat\", \"hare.consts\", \"hare_mod\")) hares.out <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) #Specify params: pars = c('beta1', 'beta2', 'alpha', 'N') #removed beta0  prepnim <- nimbleModel(code = hare_mod, constants = hare.consts,                            data = hare.dat, inits = hare.inits, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = pars, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 150000, nburnin = 50000, thin = 2) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res)  }) #this will take awhile and not produce any progress bar.  #You know it's done when the hares.out object is created in your environment. hares.out <- as.mcmc.list(hares.out) stopCluster(cl) betas <- hares.out[,-grep('N', colnames(hares.out[[1]])),] MCMCvis::MCMCtrace(betas, pdf = F)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"random-effects","dir":"Articles","previous_headings":"","what":"Random Effects","title":"Lab6: Missing Data","text":"point, ’ve treating detection probability fixed effect site. , expected similarity detection probabilities different sites. Let’s try model random effect instead see changes estimates. , let’s grab current model’s alpha values can see compare. Okay, now model: See works? use random effect, ’re saying average effect 0, site’s specific beta higher lower. amount variation mean controlled sd.site, require prior . can describe alpha parameter sd.site ‘hyper-parameter.’ Time update inits object value sd.site. Run model : Let’s check alpha parameters version model:  seem mixed nicely. compare modeled alpha fixed effect? Let’s make quick graph compare.  can see numerically spread alphas little smaller second model.","code":"alphas1 <-  hares.out[,grep('alpha', colnames(hares.out[[1]])),] hare_mod <- nimbleCode({   for(i in 1:nsites){     for(t in 1:nyears){       #Process model       log(lambda[i,t]) <- beta1[landuse[i]] + beta2*site_area[i]       N[i,t] ~ dpois(lambda[i,t])              for(k in 1:2){ #could also do 1:ncounts[t] if we wanted to be fancy         #observation model         y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])       }     }          logit(p[i]) <- alpha[site[i]]   } #priors:   beta2 ~ dnorm(0, 1)   for(m in 1:nlandtypes){     beta1[m] ~ dnorm(0, 1)   }   for(j in 1:nsites){     alpha[j] ~ dnorm(0, sd = sd.site) #this is the change   }   sd.site ~ dexp(3) }) hare.inits$sd.site <- rexp(1,1)  #from the prior cl <- makeCluster(3) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"hare.inits\", \"hare.dat\", \"hare.consts\", \"hare_mod\")) hares.out <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) #Specify params: pars = c('beta1', 'beta2', 'sd.site', 'alpha', 'N') #add sd.site  prepnim <- nimbleModel(code = hare_mod, constants = hare.consts,                            data = hare.dat, inits = hare.inits, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = pars, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 250000, nburnin = 150000, thin = 2) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res)  }) #this will take awhile and not produce any progress bar.  #You know it's done when the hares.out object is created in your environment. hares.out <- as.mcmc.list(hares.out) stopCluster(cl) alphas2 <- hares.out[,grep('alpha', colnames(hares.out[[1]])),] MCMCvis::MCMCtrace(alphas2, pdf = F) mod1_alphas <- summary(alphas1)$quantiles  mod2_alphas <- summary(alphas2)$quantiles gg_alphas <- data.frame(param = rep(paste0('alpha', 1:7), 2),                         LCI = c(mod1_alphas[,1], mod2_alphas[,1]),                         Median = c(mod1_alphas[,3], mod2_alphas[,3]),                         UCI = c(mod1_alphas[,5], mod2_alphas[,5]),                         Model = rep(c(\"Fixed Effect\", \"Random Effect\"), each = 7)) library(ggplot2) ggplot(gg_alphas, aes(x = param, y = Median, col = Model))+   geom_pointrange(aes(ymin = LCI, ymax = UCI), position=position_dodge(width = .25))+   xlab(\"Parameter\")+   scale_color_manual(values = c('#7C94EC', '#C88B37'))+   geom_hline(yintercept = mean(mod1_alphas[,3]), lty = 2, col = '#7C94EC')+   geom_hline(yintercept = mean(mod2_alphas[,3]), lty = 2, col = '#C88B37')+   theme(axis.text.x = element_text(angle = 30, hjust = .75)) var(mod1_alphas[,3]) #> [1] 0.4728 var(mod2_alphas[,3]) #> [1] 0.4"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab6_MissingData.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab6: Missing Data","text":"Last week talked prior predictive checks. Try running model just ran lab (detection random effect site) running prior predictive check. Plot posteriors lambda y. Hint: Instead putting data “data” object, put initial value. data, information (constants, params, etc.) still . data formally analyzed, authors used random effect year process model random effect site detection process. Run model random effects. Use ggplot compare beta1 values (effect landuse type expected abundance) produced new mode vs model Question 1. site average area (scaled area = 0), expected abundance land use type? Since abundance model, makes sense make plot abundance time. Inside model used Question 2, create 2 derived parameterS represents total abundance sites landtype (one parameter landtype) year. Use ggplot graph estimated total average density landtype time, including CIs. (Note: get density, determine total area plots landtype, divide estimates number). practice converting model code mathematical notation expected scientific paper, write model Question 2 full mathematical form, including priors. 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"objectives","dir":"Articles","previous_headings":"","what":"Objectives","title":"Lab7: Goodness of Fit","text":"Assess models goodness fit using posterior predictive checks Bayesian p-values Review occupancy N-mixture models","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"what-is-goodness-of-fit","dir":"Articles","previous_headings":"","what":"What is Goodness of Fit?","title":"Lab7: Goodness of Fit","text":"Goodness--ﬁt assessment key assessment assumption violations model. excellent tool : Identifying glaring flaws model Testing model assumptions met Double checking model think ’s Goodness Fit tell us: model best available model data made coding error small violation model assumptions","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"bayesian-p-values","dir":"Articles","previous_headings":"","what":"Bayesian p-values","title":"Lab7: Goodness of Fit","text":"basic idea posterior predictive check see data fit model can predicted model fit. similar (identical) maximum likelihood approach boostrapping. model ﬁts observed data adequately (.e., underlying model assumptions met sufﬁciently well) possible generate, simulation, new replicate data set using model structure estimated parameters similar observed data set. Posterior predictive checks compute one discrepancy measures observed replicate data sets. general method usually: Simulate new data fit model. Calculate discrepancy measure observed data expected value process model. Calculate discrepancy measure simulated data expected value process model. Calculate proportion data points observed data’s discrepancy value extreme simulated data’s discrepancy value. proportion called Bayesian p-value.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"a-simple-example","dir":"Articles","previous_headings":"Bayesian p-values","what":"A Simple Example:","title":"Lab7: Goodness of Fit","text":"Let’s imagine extremely simple example model weight birds 500 sites study area random variable normal distribution. data says observed abundances : fit model, posterior estimate \\mu = 45 \\sigma = 1. ’re kind suspicious mean estimate, simulate data Normal(5, 1) calculate average discrepancy simulated results expected value (5). distribution, ’d expect correlation values 1: Uh oh! simulated discrepancy value greater real data’s discrepancy 100% time! ’s sign model probably good. look like perfect model? Let’s see: Nice!","code":"set.seed(1) abundance <- rnorm(500, mean = 50, sd = 1) fakedat <- rnorm(500, mean = 45, sd = 1) sim.discrep <- fakedat-45 real.discrep <- abundance - 45 plot(sim.discrep, real.discrep, asp =1) abline(a =0, b=1) fakedat <- rnorm(500, mean = 50, sd = 1) sim.discrep <- fakedat-50 real.discrep <- abundance - 50 plot(sim.discrep, real.discrep, asp =1) abline(a =0, b=1)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"discrepancy-measures-in-nimble","dir":"Articles","previous_headings":"","what":"Discrepancy Measures in Nimble","title":"Lab7: Goodness of Fit","text":"Obviously examples real life going bit complicated. Let’s take example lecture week see implement Nimble looking N-mixture model. begin, ’s important note many ways calculate discrepancy. one way necessarily better others. 2 common options: Compare raw residuals: y - E(y) Use Pearson’s residuals (y - E(y))^2/var(E(y)) ’ll implement model, just see work. ’s hare model looked Lab 6. reminder, full data contain replicated counts Brown hares 1992-2008 56 sites 8 regions Switzerland. chose model hares just one region, Central region. reduces data 7 study sites. data y, number hares seen survey. convenience, ’s relevant line: first step simulate new observation data inside model. Now can calculate various discrepancy measures.","code":"hare_mod <- nimbleCode({   for(i in 1:nsites){     for(t in 1:nyears){       #Process model       log(lambda[i,t]) <- beta1[landuse[i]] + beta2*site_area[i]       N[i,t] ~ dpois(lambda[i,t])              for(k in 1:2){         #observation model         y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])       } #end k     } #end t          logit(p[i]) <- alpha[site[i]]   } #priors:   beta2 ~ dnorm(0, 1)   for(m in 1:nlandtypes){     beta1[m] ~ dnorm(0, 1)   }   for(j in 1:nsites){     alpha[j] ~ dnorm(0, sd = sd.site)    }   sd.site ~ dexp(3) }) y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k]) y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k]) y.new[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"compare-the-raw-residuals","dir":"Articles","previous_headings":"Discrepancy Measures in Nimble","what":"Compare the raw residuals","title":"Lab7: Goodness of Fit","text":"First option compare raw residuals. ’s easiest personal favorite. , subtract y y.new expected value binomial (N*p): bayesian p-value proportion observations y.res > y.res.new. want value 0.3 0.7. Much like maximum likelihood ‘p-values’, cutoffs somewhat arbitrary. Note nimble gets cranky try sum 2 dimensions . either split sums get result year. model, might well just calculate p-value site, can identify sites noticeably worse fitting others.","code":"y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k]) y.new[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])  y.res[i,t,k] <- y[i,t,k] - (N[i,t]*p.det*effort[i,t,k]) #residual y.res.new[i,t,k] <- y.new[i,t,k] - (N[i,t]*p.det*effort[i,t,k]) #predicted residual y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k]) y.new[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])  y.res[i,t,k] <- y[i,t,k] - (N[i,t]*p[i]*effort[i,t,k]) #residual y.res.new[i,t,k] <- y.new[i,t,k] - (N[i,t]*p[i]*effort[i,t,k]) #predicted residual  y.res.dif[i,t,k] <- y.res[i,t,k] - y.res.new[i,t,k] }} #end k and t p.val1[i] <- mean(y.res.dif[i,1:nyears,1:2] > 0) }#end i"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"pearsons-residuals","dir":"Articles","previous_headings":"Discrepancy Measures in Nimble","what":"Pearson’s Residuals","title":"Lab7: Goodness of Fit","text":"Pearson’s residuals use observation, expected value variance expected value: (y - E(y))^2/var(E(y)). variance binomial Np(1-p). looks gross, least don’t calculate hand! Let’s run model monitor p.val1 p.val2.","code":"y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k]) y.new[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])   y.pear[i,t,k] <- (y[i,t,k] - N[i,t]*p[i]*effort[i,t,k])^2/   (N[i,t]*(p[i]*effort[i,t,k])*(1-*p[i]*effort[i,t,k])) y.pear.new[i,t,k] <- (y.new[i,t,k] - N[i,t]*p[i]*effort[i,t,k])^2/   (N[i,t]*(p[i]*effort[i,t,k])*(1-*p[i]*effort[i,t,k]))  y.pear.dif[i,t,k] <- y.pear[i,t,k] - y.pear.new[i,t,k] }} #end k and t p.val2[i] <- mean(y.pear.dif[i,1:nyears,1:2] > 0)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"run-the-model","dir":"Articles","previous_headings":"","what":"Run the model","title":"Lab7: Goodness of Fit","text":"convenience, data cleanup lecture saved object WILD8370 package. just need monitor parameters interest (p.val1 p.val2), add initial values y.new ’re good go. ’s full model:","code":"data(Lab7hares) Lab7hares$inits$y.new <- Lab7hares$dat$y hare_mod <- nimbleCode({   for(i in 1:nsites){     for(t in 1:nyears){       #Process model       log(lambda[i,t]) <- beta1[landuse[i]] + beta2*site_area[i]       N[i,t] ~ dpois(lambda[i,t])              for(k in 1:2){         #observation model         y[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])         y.new[i,t,k] ~ dbinom(size = N[i,t],p =p[i]*effort[i,t,k])                           y.res[i,t,k] <- y[i,t,k] - (N[i,t]*p[i]*effort[i,t,k]) #residual         y.res.new[i,t,k] <- y.new[i,t,k] - (N[i,t]*p[i]*effort[i,t,k]) #predicted residual         y.res.dif[i,t,k] <- y.res[i,t,k] - y.res.new[i,t,k]                  y.pear[i,t,k] <- (y[i,t,k] - N[i,t]*p[i]*effort[i,t,k])^2/           (N[i,t]*(p[i]*effort[i,t,k])*(1-p[i]*effort[i,t,k])) #residual         y.pear.new[i,t,k] <- (y.new[i,t,k] - N[i,t]*p[i]*effort[i,t,k])^2/           (N[i,t]*(p[i]*effort[i,t,k])*(1-p[i]*effort[i,t,k])) #predicted residual         y.pear.dif[i,t,k] <- y.pear[i,t,k] - y.pear.new[i,t,k]         } #end k       } #end t           p.val1[i] <- mean(y.res.dif[i,1:nyears,1:2] > 0)     p.val2[i] <- mean(y.pear.dif[i,1:nyears,1:2] > 0)          logit(p[i]) <- alpha[site[i]]   } #priors:   beta2 ~ dnorm(0, 1)   for(m in 1:nlandtypes){     beta1[m] ~ dnorm(0, 1)   }   for(j in 1:nsites){     alpha[j] ~ dnorm(0, sd = sd.site)    }   sd.site ~ dexp(3) }) params <- c('p.val1', 'p.val2') ni <- Lab7hares$inits nd <- Lab7hares$dat nc <- Lab7hares$consts  library(parallel) cl <- makeCluster(3) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"ni\",  \"nc\", 'nd', \"hare_mod\", 'params')) hares.out <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) prepnim <- nimbleModel(code = hare_mod, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = params, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 50000, nburnin = 25000, thin = 1) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res) }) #this will take awhile and not produce any progress bar hares.out <- as.mcmc.list(hares.out) stopCluster(cl)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"check-output","dir":"Articles","previous_headings":"Run the model","what":"Check Output","title":"Lab7: Goodness of Fit","text":"find? According raw residuals (pval1), can see data sites 4, 6 7 potentially fitting model well. However none values glaringly low, note discussion clear sign throw results. p-value calculated using pearson residuals (pval2), can see values look reasonable except maybe site 7. Remember p-values tell us extreme error model glaring problem fit, sensitive small violations.","code":"summary(hares.out)$quantiles #>             2.5%    25%    50%    75%  97.5% #> p.val1[1] 0.2059 0.2941 0.3235 0.3824 0.4706 #> p.val1[2] 0.2059 0.2941 0.3529 0.4118 0.5000 #> p.val1[3] 0.2353 0.3235 0.3824 0.4118 0.5000 #> p.val1[4] 0.1471 0.2059 0.2353 0.2647 0.3235 #> p.val1[5] 0.2941 0.3824 0.4412 0.5000 0.5882 #> p.val1[6] 0.1765 0.2353 0.2941 0.3235 0.4118 #> p.val1[7] 0.1471 0.2059 0.2353 0.2941 0.3529 #> p.val2[1] 0.2647 0.3529 0.4118 0.4706 0.5588 #> p.val2[2] 0.2941 0.3529 0.4118 0.4412 0.5294 #> p.val2[3] 0.3529 0.4412 0.4706 0.5294 0.5882 #> p.val2[4] 0.2647 0.3235 0.3529 0.4118 0.4706 #> p.val2[5] 0.4412 0.5294 0.5882 0.6176 0.7059 #> p.val2[6] 0.2647 0.3529 0.4118 0.4412 0.5294 #> p.val2[7] 0.1765 0.2647 0.2941 0.3235 0.4118"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"posterior-predictive-checks---predictive-fit","dir":"Articles","previous_headings":"","what":"Posterior Predictive Checks - Predictive Fit","title":"Lab7: Goodness of Fit","text":"can also use posterior prediction assess good model predicting future observations assess sensitivity predictions subsets data. Usually ’ll want run two checks - one observation model one latent variable interest. easiest way implement checks follows: Run model data monitor posterior latent variable interest (usually N N-mixture model z occupancy model). Remove data provided model. Run model trimmed data set monitor posterior latent variable earlier. Also monitor posterior data points removed. Compare posterior estimates latent variable model runs estimate sensitivity predictions. Compare posterior estimate missing data observed data evaluate predictive ability observation model.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"predict-fit-on-simulated-data","dir":"Articles","previous_headings":"","what":"Predict Fit on Simulated Data","title":"Lab7: Goodness of Fit","text":"Let’s use simulated dynamic occupancy data. ’ll pretend gamma increases percent forest higher. ’s model want run. Note ’re modeling \\gamma constant, even though simulated dependent forest cover. assess sensitivity estimates Total Occupancy, ’ll first run model data run model years 14 15 removed.","code":"set.seed(55) nyears <- 15 nsites <- 40 phi <- .8 forest <- runif(nsites, 0, .5) gamma <- .1  psi1 <- .62 nvisits <- 4 p <- .4  z <- y <-array(NA, c(nsites, nyears)) z[,1] <- rbinom(nsites, 1, psi1) for(i in 1:nsites){   y[i,1] <- rbinom(1, z[i,1]*nvisits, p)     for(t in 2:nyears){       z[i, t] <- rbinom(1, 1, z[i,t-1]*(1-phi)+(1-z[i,t-1])*(gamma+.2*forest[i]))       y[i, t] <- rbinom(1, z[i,t]*nvisits, p)   } } occ.mod <- nimbleCode({ for(i in 1:nsites){   z[i,1] ~dbern(psi)   y[i,1] ~dbinom(p, z[i,1]*nvisits)      for(t in 2:nyears){     z[i, t] ~dbern(z[i,t-1]*(1-phi)+(1-z[i,t-1])*gamma)     y[i, t] ~dbinom(p, z[i,t]*nvisits)   } #end t } #end i    for(t in 1:nyears){   Tot.occ[t] <- sum(z[1:nsites,t]) }    gamma ~ dbeta(1, 1)  phi ~ dbeta(1, 1)  p ~ dbeta(1, 1) psi ~ dbeta(1,1) })"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"run-the-model-with-full-data","dir":"Articles","previous_headings":"Predict Fit on Simulated Data","what":"Run the model with full data","title":"Lab7: Goodness of Fit","text":"Excellent. Now let’s give Nimble data years 1 - 13 see fares. also want monitor y parameter can look observation model.","code":"params <- c('Tot.occ') n.c <- list(nsites = nsites, nvisits = nvisits, nyears = nyears) n.d <- list(y =y) z.init <- matrix(rbinom(nsites*nyears, 1, .5), ncol = nyears) z.init[which(y > 0)] <- 1  n.i <- function()list(p = runif(1), gamma = runif(1), phi = runif(1), psi = runif(1), z =z.init) occ_out1 <- nimbleMCMC(code = occ.mod,                      data = n.d,                      constants = n.c,                      inits = n.i(),                      monitors = params,                      thin = 1,                      niter = 5000,                      nburnin = 2500,                      nchains = 3,                      samplesAsCodaMCMC = TRUE                       ) params <- c('Tot.occ') y.partial <- y y.partial[,14:15] <- NA n.d2 <- list(y =y.partial) #only some of the data y.init <- y y.init[,1:13] <- NA n.i2 <- function()list(p = runif(1), gamma = runif(1), phi = runif(1), psi = runif(1), z =z.init, y = y.init)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"run-the-model-with-partial-data","dir":"Articles","previous_headings":"Predict Fit on Simulated Data","what":"Run the model with partial data","title":"Lab7: Goodness of Fit","text":"Let’s start comparing predictions total occupancy. Remember latent, don’t know ‘right’ answer, knowing sensitive model amount data can help us understand useful model predicting future dynamics. ### Compare Total Occupancy  can see without last 2 years data model predicts slightly higher number sites occupied, 95% credible intervals still overlap. However, probably cautious predicting model 2 years future.","code":"occ_out2 <- nimbleMCMC(code = occ.mod,                      data = n.d2,                      constants = n.c,                      inits = n.i2(),                      monitors = params,                      thin = 1,                      niter = 5000,                      nburnin = 2500,                      nchains = 3,                      samplesAsCodaMCMC = TRUE                       ) Tot_occ1 <- summary(occ_out1[,grep('Tot', colnames(occ_out1[[1]])),])$quantiles Tot_occ2 <- summary(occ_out2[,grep('Tot', colnames(occ_out2[[1]])),])$quantiles  gg_tot <- data.frame(Median = c(Tot_occ1[,3], Tot_occ2[,3]),                      LCI = c(Tot_occ1[,1], Tot_occ2[,1]),                      UCI = c(Tot_occ1[,5], Tot_occ2[,5]),                      Year = rep(1:15, 2),                      Model = rep(c('All Data', 'Incomplete Data'), each = 15))  ggplot(gg_tot, aes(x = Year, y= Median, col = Model, group = Model))+   geom_pointrange(aes(ymin = LCI, ymax = UCI), position = position_dodge(width = .75))+   scale_color_manual(values = c('firebrick4', 'tan3'))"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab7_GOF.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab7: Goodness of Fit","text":"Using Central hares dataset used , analyze data dynamic N-mixture model framework using constant parameterization Dail-Madsen model. put covariates \\gamma \\phi keep equation \\lambda year 1 used \\lambda lab example. Assess goodness fit using raw residuals discrepancy meaure calculate Bayesian p-value. goodness fit test suggest glaring issues model fit? Write mathematical equation model used Question 1. Include priors. Using simulated dynamic occupancy model, calculate Bayesian p-value year data. goodness fit test detect issues? Use Pearson residuals calculate discrepancy. 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab8_Movement.html","id":"objectives","dir":"Articles","previous_headings":"","what":"Objectives","title":"Lab8: Resource Selection Functions","text":"Learn convert landscape metrics partial derivatives Run simple resource selection function Nimble","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab8_Movement.html","id":"what-is-a-resource-selection-function","dir":"Articles","previous_headings":"","what":"What is a resource selection function?","title":"Lab8: Resource Selection Functions","text":"Resource selection functions (RSFs) class functions used spatial ecology assess habitat characteristics important population species. RSF literature thinks selection 4 scales: First order selection: entire range species Second order selection: home range individual group animals Third order selection: Resource habitat usage within individual’s group’s home range Fourth order selection: procurement specific resources, food, specific sites","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab8_Movement.html","id":"habitat-gradients","dir":"Articles","previous_headings":"","what":"Habitat Gradients","title":"Lab8: Resource Selection Functions","text":"often think continuous environmental covariates value covariate (percent tree cover, proportional early successional agriculture, distance road, etc.) specific location, another way consider world think relative difference one location locations around . can using potential functions. Potential functions provide straightforward mathematical framework movement along environmental gradients. Much like marble rolling along hilly surface, potential functions can steer movements towards (away ) particular habitat features based gradients. gradient vector field partial derivatives pointing direction greatest rate increase habitat covariate. simple language, essentially want cosine sine aspect surface. requires first calculating aspect (angle slope landscape facing). ’s might look like realistic landscape. Imagine interested role canopy cover plays habitat selection spotted skunks. go favorite data warehouse acquire canopy cover layer. Interestingly, looks exactly like simulated dataset scaling:  First, want aspect beautiful landscape. can easily terra package.  resulting graph weird, indicates direction slope raster facing pixel. value 1.5 (90°) indicates movement straight take higher values. value 3.14 (180°) says going left take higher values. value (0) 0° says “nope, ’m highest point nearby area!” Now can take values get easting (value higher left right?) northing (value higher direction?). take back high school math, cosine angle tell relative x value sine tell y value. Positive values cosine(x) tell go right, negative till go left. Note R expects input radians, degrees.  Positive values sine(x) tell go , negative till go . Note R expects input radians, degrees.","code":"set.seed(55) library(terra) delta <- 0.025 ## Resolution grid0 <- seq(delta/2, 1-delta/2, delta) grid <- cbind(rep(grid0, each=length(grid0)),               rep(grid0, times=length(grid0))) distmat <- as.matrix(dist(grid)) npixels <- nrow(distmat) ## Covariance matrix V <- exp(-0.4*distmat) R <- chol(V) X <- t(R) %*% rnorm(npixels)  Canopy <- rast(nrows = length(grid0), ncols = length(grid0)) Canopy[] <- X[,1] plot(Canopy) aspect <- terra::terrain(Canopy, \"aspect\", neighbors = 8, unit = 'radians') plot(aspect) easting <- cos(aspect) plot(easting) northing <- sin(aspect) plot(northing)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab8_Movement.html","id":"why-does-this-matter","dir":"Articles","previous_headings":"","what":"Why does this matter?","title":"Lab8: Resource Selection Functions","text":"One thing ’s difficult resource selection consider two different things - animal chose (based movement data) animal chosen didn’t (aka, availability habitat). using partial derivatives, can model time! something rarely done classic RSF functions leads sorts mathematical headaches.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab8_Movement.html","id":"rsf-models","dir":"Articles","previous_headings":"","what":"RSF Models","title":"Lab8: Resource Selection Functions","text":"Now ’ve learned find partial derivatives landscape, let’s put practice. Recall model Brownian Bridge movement: \\Large \\mu_t = \\mu_{t-1} + \\epsilon_{t} \\sim Normal(0, \\sigma^2) can add parameter predicts next location based environmental conditions current location. \\Large \\mu_t = \\mu_{t-1} + \\omega*\\nabla(\\mu_{t-1}) + \\epsilon_{t} \\sim Normal(0, \\sigma^2) \\omega strength response habitat gradient \\nabla(\\mu_{t-1}) habitat gradient previous location (\\mu_{t-1}). x coordinates, \\nabla(\\mu_{t-1}) = cos(aspect) y, \\nabla(\\mu_{t-1}) = sin(aspect). Let’s go back example collared deer lab see deer movement responds elevation. Due rds irregularities, raster currently saved matrix time .  Let’s grab aspect, easting northing raster:  Let’s get nimble model written : Side note: name model manuscript, call approximate Langevin diffusion movement model. anyway. Let’s clean data. also need know easting/northing value location dataset. Time setup nimble: run!  appears slight negative effect elevation deer movements. words, deer likely move towards areas lower elevation stay elevation range previous location.","code":"data('Arkansas_dem') dem <- rast(Arkansas_dem, type = 'xyz', crs = 'EPSG:26915') plot(dem) aspect <- terra::terrain(dem, \"aspect\", neighbors = 8, unit = 'radians') easting <- cos(aspect) northing <- sin(aspect) plot(easting) plot(northing) pd_walk <- nimbleCode({   for(i in 1:nind){     s[i,1,1] ~ dunif(xmin, xmax)     s[i,2,1] ~ dunif(ymin, ymax)     for(t in 2:nTelemetry[i]){       s[i,1,t] ~ dnorm(s[i,1,t-1] + omega*easting[i,t-1], sd = sigma)       s[i,2,t] ~ dnorm(s[i,2,t-1] + omega*northing[i,t-1], sd= sigma)     }   }   sigma ~ dgamma(1,1)   omega ~ dnorm(0, 1) }) data(collars) collars$Collar.ID_f <- as.numeric(as.factor(collars$Collar.ID)) nind <- length(unique(collars$Collar.ID_f))  collars$easting <- terra::extract(easting, collars[,c('x','y')]) collars$northing <- terra::extract(northing, collars[,c('x','y')])  nTelemetry <- as.vector(table(collars$Collar.ID_f)) s <- array(NA, c(nind, 2, max(nTelemetry))) east <- north <- array(NA, c(nind, max(nTelemetry))) for(i in 1:nind){   me <- collars[collars$Collar.ID_f == i,]   s[i,1,1:nTelemetry[i]] <- me$x/1e6   s[i,2,1:nTelemetry[i]] <- me$y/1e6   east[i,1:nTelemetry[i]] <- me$easting$aspect   north[i,1:nTelemetry[i]] <- me$northing$aspect }  xmin <- min(s[,1,], na.rm = T)-1 #add some space around locations xmax <- max(s[,1,], na.rm = T)+1 #add some space around locations ymin <- min(s[,2,], na.rm = T)-1 #add some space around locations ymax <- max(s[,2,], na.rm = T)+1 #add some space around locations consts <- list(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, nind = nind, nTelemetry = nTelemetry, northing = north, easting = east) dat <- list(s = s) inits <- function(){list(sigma = rgamma(1,1,1), omega = rnorm(1))} params <- c('sigma', 'omega') rsf_out <- nimbleMCMC(code = pd_walk,                      data = dat,                      constants = consts,                      inits = inits(),                      monitors = params,                      thin = 1,                      niter = 5000,                      nburnin = 2500,                      nchains = 3,                      samplesAsCodaMCMC = TRUE                       ) MCMCvis::MCMCtrace(rsf_out, pdf = F, Rhat = T, n.eff = T) MCMCvis::MCMCsummary(rsf_out, Rhat = F, n.eff= F)*1e6 #>          mean    sd    2.5%     50%   97.5% #> omega  -3.461 2.633  -8.704  -3.519   1.692 #> sigma 247.796 1.333 245.309 247.811 250.368"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab8_Movement.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab8: Resource Selection Functions","text":"WILD8370 package, ’ll find dataset called coyote_moves. object contains named list. first list data collected Dr. Chamberlain et al collared coyotes. second matrix version raster containing information distance roads (meters). EPSG code raster 26917. Use data fit RSF like 2019 coyote points. Several steps required: Make sure points order date individual Convert Latitude Longitude UTMs (EPSG 26917 aka UTM 17N) Find easting northing values collar location based aspect roads tif Remove points don’t fall tif Fit model Using deer collar data, fit correlated random walk (see slide 16 lecture refresher). Report estimates \\gamma \\sigma. 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"butterflies","dir":"Articles","previous_headings":"","what":"Butterflies!","title":"Lab9: Spatial Capture Recapture","text":"Today’s dataset study false heath fritillary butterflies (Melitaea diamina) conducted Finland. can find details study : Fabritius, H., Rönkä, K. & Ovaskainen, O. dual role rivers facilitating hindering movements false heath fritillary butterfly. Mov Ecol 3, 4 (2015). https://doi.org/10.1186/s40462-015-0031-z full study uses three datasets today using data 1995. **case authors read wonder heck teaching American students, modified habitat information simpler original study. results slightly original study. authors: Details three data sets summarized . number search days without () parenthesis indicates length study period (versus number effective search days search activity). Location: Siitama, Pirkanmaa, Finland (61.6°N, 24.2°E) Year data collection: 1995 Coordinator: Niklas Wahlberg Coordinate reference system: Finland Zone 3 (EPSG:2393) work 4 files get information need: Effort log - 1/0 matrix indicating ‘trap’ operational Records time individual captured (trap) Trap locations Finland Zone 3 units Habitat information trapping area","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"summary-info","dir":"Articles","previous_headings":"","what":"Summary Info","title":"Lab9: Spatial Capture Recapture","text":"Let’s see many individuals effort looks like: appear 14 trap occasions 112 traps. data gives us one row per capture. individual ID given capture, well day, trap capture. also indication sex (binary variable). individuals captured , individuals capture 8 times!","code":"data('butterflies') effort <- Finnish_butterflies$effort str(effort) #> 'data.frame':    112 obs. of  15 variables: #>  $ Trap: int  1 2 3 4 5 6 7 8 9 10 ... #>  $ V1  : int  1 1 0 0 0 1 1 1 0 0 ... #>  $ V2  : int  1 1 1 1 0 1 1 1 1 0 ... #>  $ V3  : int  0 0 0 0 0 0 0 0 1 0 ... #>  $ V4  : int  0 0 0 0 0 1 0 1 1 1 ... #>  $ V5  : int  1 1 1 1 1 1 0 1 1 1 ... #>  $ V6  : int  0 0 0 1 1 1 0 1 0 0 ... #>  $ V7  : int  0 0 0 0 1 1 0 1 1 1 ... #>  $ V8  : int  1 0 1 1 1 1 0 1 0 0 ... #>  $ V9  : int  0 0 1 0 1 1 0 1 0 0 ... #>  $ V10 : int  0 0 0 0 1 1 0 1 0 1 ... #>  $ V11 : int  0 0 0 0 1 1 0 1 0 0 ... #>  $ V12 : int  0 0 0 0 1 1 0 1 0 0 ... #>  $ V13 : int  0 0 0 0 1 1 0 1 0 0 ... #>  $ V14 : int  0 0 0 0 1 1 0 1 0 0 ... caps <- as.data.frame(Finnish_butterflies$cap_hist) head(caps) #>   Individual Day Trap Sex #> 1          1  10   72   0 #> 2          1   2   72   1 #> 3          1   3   72   1 #> 4          1   4   72   1 #> 5          1   6   72   1 #> 6          1   1   74   0 hist(rowSums(table(caps$Individual, caps$Day)))"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"non-spatial-cmr","dir":"Articles","previous_headings":"","what":"Non-Spatial CMR","title":"Lab9: Spatial Capture Recapture","text":"Let’s begin analyzing data non-spatial framework. know sometimes areas surveyed, ’ll model detection probability function number traps open. can start putting capture data different format. want one row per individual, one column per occasion. also need choose value M. goal simply choose value M N/M < 1. course, don’t know N, tricky. Let’s start choosing value likely small can see happens. ’ll choose M = nind + 10, suggest missed 10 butterflies entire population sampling. Note create array M rows, nind rows. need tell model capture history EVERY individual potentially population, including ones never saw. Next can cleanup effort, tell model number traps open per occasion. , ’re taking column sums, give us count many traps open. Notice column 1 trap ID, don’t add one effort vector.","code":"cmr_flies <- nimbleCode({   psi ~ dbeta(1, 1) #N/M   alpha0 ~ dnorm(0, 1)   alpha1 ~ dnorm(0, 1)   for(t in 1:nocc){     logit(p[t]) <- alpha0 + alpha1*effort[t]   }      for(i in 1:M){     z[i] ~ dbern(psi)          for(t in 1:nocc){       y[i, t] ~ dbern(p[t] * z[i])     } # end t   } # end i      N <- sum(z[1:M])   EN <- M*psi }) nind <- length(unique(caps$Individual)) nocc <- 14 M <- nind + 10  y <- array(0, c(M, nocc)) for(i in 1:nrow(caps)){   y[caps$Individual[i],caps$Day[i]] <- 1 } head(y) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    1    1    1    1    0    1    0    0    0     1     0     0     0     0 #> [2,]    1    0    0    0    1    1    0    0    0     0     0     0     0     0 #> [3,]    1    0    0    0    0    0    0    1    0     0     0     0     0     0 #> [4,]    1    0    0    0    1    1    0    0    0     0     0     0     0     0 #> [5,]    0    1    0    1    0    0    0    0    0     0     0     0     0     0 #> [6,]    0    1    0    1    1    1    0    0    0     0     0     0     0     0 tail(y) #>        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] #> [161,]    0    0    0    0    0    0    0    0    0     0     0     0     0 #> [162,]    0    0    0    0    0    0    0    0    0     0     0     0     0 #> [163,]    0    0    0    0    0    0    0    0    0     0     0     0     0 #> [164,]    0    0    0    0    0    0    0    0    0     0     0     0     0 #> [165,]    0    0    0    0    0    0    0    0    0     0     0     0     0 #> [166,]    0    0    0    0    0    0    0    0    0     0     0     0     0 #>        [,14] #> [161,]     0 #> [162,]     0 #> [163,]     0 #> [164,]     0 #> [165,]     0 #> [166,]     0 head(effort) #>   Trap V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 #> 1    1  1  1  0  0  1  0  0  1  0   0   0   0   0   0 #> 2    2  1  1  0  0  1  0  0  0  0   0   0   0   0   0 #> 3    3  0  1  0  0  1  0  0  1  1   0   0   0   0   0 #> 4    4  0  1  0  0  1  1  0  1  0   0   0   0   0   0 #> 5    5  0  0  0  0  1  1  1  1  1   1   1   1   1   1 #> 6    6  1  1  0  1  1  1  1  1  1   1   1   1   1   1 eff <- colSums(effort[,-1])"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"first-try-cmr","dir":"Articles","previous_headings":"Non-Spatial CMR","what":"First Try CMR","title":"Lab9: Spatial Capture Recapture","text":"Now let’s make objects run model! ’ll start long way running model look problems initial values: Uh oh! Let’s figure went wrong. Nothing wrong z values. y problem? Yes! log probability -Inf. ’s great. Individual 1 seems issues: Hmm, doesn’t like didn’t see . initialize p ? Oops! told model detection probability way high! Let’s try lower values: Much better. can now run model.","code":"nc <- list(effort = eff,nocc = nocc, M = M) nd <- list(y = y) z.init <- c(rep(1, nind), rep(0, M-nind)) ni <- list(psi = sum(z.init)/M, alpha0 = rnorm(1), alpha1 = rnorm(1), z = z.init) params <- c('alpha0', 'alpha1', 'psi', 'N') prepnim <- nimbleModel(code = cmr_flies, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong #> [1] -Inf sum(prepnim$logProb_z) #> [1] -37.79 sum(prepnim$logProb_y) #> [1] -Inf prepnim$logProb_y[1,] #>  [1]  0.000e+00  0.000e+00 -8.874e-09  0.000e+00       -Inf  0.000e+00 #>  [7]       -Inf       -Inf       -Inf  0.000e+00       -Inf       -Inf #> [13]       -Inf       -Inf prepnim$y[1,] #>  [1] 1 1 1 1 0 1 0 0 0 1 0 0 0 0 prepnim$p #>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ni <- list(psi = sum(z.init)/M, alpha0 = rnorm(1),             alpha1 = 0, z = z.init) prepnim <- nimbleModel(code = cmr_flies, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong #> [1] -1070"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"second-try-cmr","dir":"Articles","previous_headings":"Non-Spatial CMR","what":"Second Try CMR","title":"Lab9: Spatial Capture Recapture","text":"Let’s see model output gives us.  Uh oh! value M clearly way low. posterior psi pushing 1, means ’ve enforced total population lower number data suggesting. Let’s try higher M. One way choose value M look raw detection frequency: average, saw butterfly 2.6 times, despite going 14 days. tells us good starting M might nind * 14/2.6 ~ nind*5.4","code":"library(parallel) cl <- makeCluster(3) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"ni\",  \"nc\", 'nd', \"cmr_flies\", 'params')) cmr1 <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) prepnim <- nimbleModel(code = cmr_flies, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = params, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 50000, nburnin = 25000, thin = 1) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res) }) #this will take awhile and not produce any progress bar cmr1 <- as.mcmc.list(cmr1) stopCluster(cl) MCMCvis::MCMCtrace(cmr1, pdf = F, Rhat = T, n.eff = T) mean(rowSums(table(caps$Individual, caps$Day))) #> [1] 2.628 M <- nind*6 y <- array(0, c(M, nocc)) for(i in 1:nrow(caps)){     y[caps$Individual[i],caps$Day[i]] <- 1 } nc <- list(effort = eff,nocc = nocc, M = M) nd <- list(y = y) z.init <- c(rep(1, nind), rep(0, M-nind)) ni <- list(psi = sum(z.init)/M, alpha0 = rnorm(1), alpha1 = 0, z = z.init)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"third-try-cmr","dir":"Articles","previous_headings":"Non-Spatial CMR","what":"Third Try CMR","title":"Lab9: Spatial Capture Recapture","text":"Let’s see model output gives us.  Much better! can see time M large enough - 18% M individuals probably even real. fact chose M large doesn’t matter, slow model runs little bit. Remember biologically, psi means nothing. run , drop value M hundred still get result less computer time.","code":"library(parallel) cl <- makeCluster(3) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"ni\",  \"nc\", 'nd', \"cmr_flies\", 'params')) cmr2 <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) prepnim <- nimbleModel(code = cmr_flies, constants = nc,                            data = nd, inits = ni, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = params, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 50000, nburnin = 25000, thin = 1) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res) }) #this will take awhile and not produce any progress bar cmr2 <- as.mcmc.list(cmr2) stopCluster(cl) MCMCvis::MCMCtrace(cmr2, pdf = F, Rhat = T, n.eff = T) MCMCsummary(cmr2) #>             mean       sd      2.5%       50%     97.5% Rhat n.eff #> N      169.66491 4.437504 162.00000 169.00000 179.00000    1 33068 #> alpha0  -3.46048 0.237253  -3.93749  -3.45911  -2.97955    1   527 #> alpha1   0.03619 0.004392   0.02728   0.03615   0.04498    1   516 #> psi      0.18199 0.013482   0.15643   0.18173   0.20910    1 53989"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"graphing-detection-probability","dir":"Articles","previous_headings":"Non-Spatial CMR","what":"Graphing Detection Probability","title":"Lab9: Spatial Capture Recapture","text":"move , let’s graph detection probability expected increase increasing effort (traps open). Remember equation detection : can replicate R code MCMC chains get CI detection different levels effort. ’ll first create vector possible values effort want plot along, use equation estimate value p level effort using iteration MCMC. ’ll take quantiles estimates get CI posterior. Time graph!","code":"logit(p[t]) <- alpha0 + alpha1*effort[t] alphas <- as.matrix(cmr2[,c('alpha0', 'alpha1'),]) effort_seq <- 1:112  niter <- nrow(alphas) p_out <- array(NA, c(niter, length(effort_seq))) for(j in 1:niter){   p_out[j,] <- plogis(alphas[j,'alpha0'] + alphas[j,'alpha1']*effort_seq) } p.CI <- apply(p_out, 2, FUN = function(x){quantile(x, c(0.025, .5, .975))}) gg.p <- data.frame(effort = effort_seq,                    LCI = p.CI[1,],                    UCI = p.CI[3,],                    Median = p.CI[2,]) library(ggplot2) ggplot(gg.p, aes(x = effort, y = Median))+   geom_line()+   geom_ribbon(aes(ymin = LCI, ymax = UCI), alpha = .4, fill = 'lightblue')+   theme_bw()+   ylim(0, 1)+   ylab(\"Detection Probability\")+   xlab(\"Number of Traps Open\")+   theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25))"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"spatial-capture-recapture","dir":"Articles","previous_headings":"","what":"Spatial Capture Recapture","title":"Lab9: Spatial Capture Recapture","text":"Now ’ve analyzed data non-spatial manner, let’s try fitting data spatial model. 3 types habitat designated authors: - Low quality matrix (habitat type 1) - High quality matrix (habitat type 3) - Breeding area (habitat 2) ’s likely butterfly abundance different habitat types. ’ll want include model lambda. spatial model, ’s little data cleanup . One part haven’t dealt lookup table habitat. Let’s take look habitat information.","code":"scr_flies <- nimbleCode({ for(j in 1:3){   beta0[j] ~ dnorm(1, 1)  } g0 ~ dunif(0, 1) sigma ~ dgamma(1, 1)  for(g in 1:G) { ## Loop over pixels   lambda[g] <- exp(beta0[habitat[g]])*pixelArea   pi[g] <- lambda[g]/SumLambda } SumLambda <- sum(lambda[1:G]) psi <- SumLambda/M #proportion real  for(i in 1:M) {   s[i,1] ~ dunif(xlim[1], xlim[2])    s[i,2] ~ dunif(ylim[1], ylim[2])   pixel[i] <- lookup[round((ylim[2]-s[i,2])/delta+0.5),  ## raster row                      round((s[i,1]-xlim[1])/delta+0.5)]  ## raster column   logProb[i] <- log(pi[pixel[i]])   zeros[i] ~ dpois(-logProb[i]) ## zeros trick for IPP   z[i] ~ dbern(psi)   for(j in 1:ntraps) {     dist[i,j] <- sqrt((s[i,1]-x[j,1])^2 + (s[i,2]-x[j,2])^2)     p[i,j] <- g0*exp(-dist[i,j]^2/(2*sigma^2))   } } for(i in 1:M) {  ## Model for capture histories   for(j in 1:ntraps) {     for(t in 1:nocc){       y[i,j,t] ~ dbern(p[i,j]*z[i]*effort[j,t])     }   } }  EN <- M*psi N <- sum(z[1:M]) })"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"pixel-lookup","dir":"Articles","previous_headings":"Spatial Capture Recapture","what":"Pixel Lookup","title":"Lab9: Spatial Capture Recapture","text":"can see habitat low-quality (type 3) high quality (type 3) breeding area (type 2) mixed . look habitat information vector, look like: order pixels goes top left pixel top right, one row starting left going across , etc. end bottom right pixel. Therefore making habitat object Nimble really easy: Let’s say point coordinates c(3237000, 6832000). figure pixel ’m ? First need pixel resolution raster minimum maximum values state space. can subtract point limit value divide width pixel find . habitat resolution 500 500 m. point pixel 5th left 3rds top. point pixel 27! need implement sort system inside model can lookup pixel given point use habitat covariate log likelihood.","code":"habitat <- Finnish_butterflies$habitat habitat_sp <- terra::rast(habitat, type = 'xyz', crs = 'EPSG:2393') plot(habitat_sp) c(habitat$layer) #>  [1] 1 1 1 1 1 1 1 1 1 3 2 1 3 3 1 1 1 1 1 3 3 2 3 3 3 1 3 3 3 3 2 3 2 1 3 2 1 1 #> [39] 1 1 3 3 3 2 3 3 2 1 1 1 1 1 1 2 3 1 1 1 1 1 1 1 3 3 2 1 1 1 1 1 1 1 1 3 3 3 #> [77] 1 dim(habitat_sp) #> [1]  7 11  1 pixel_order <- matrix(1:prod(dim(habitat_sp)), byrow = T, ncol = dim(habitat_sp)[2]) pixel_order #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] #> [1,]    1    2    3    4    5    6    7    8    9    10    11 #> [2,]   12   13   14   15   16   17   18   19   20    21    22 #> [3,]   23   24   25   26   27   28   29   30   31    32    33 #> [4,]   34   35   36   37   38   39   40   41   42    43    44 #> [5,]   45   46   47   48   49   50   51   52   53    54    55 #> [6,]   56   57   58   59   60   61   62   63   64    65    66 #> [7,]   67   68   69   70   71   72   73   74   75    76    77 hab <- c(habitat$layer) myx <- 3237000/1e5 myy <- 6832000/1e5 res(habitat_sp)/1e5 #> [1] 0.005 0.005 lims <- unname(as.vector(ext(habitat_sp)))/1e5 lims #> [1] 32.35 32.40 68.30 68.33 c(round((lims[4] - myy)/.005+.5), round((myx - lims[1])/.005+.5)) #> [1] 3 5 pixel_order[3,5] #> [1] 27 plot(habitat_sp) points(3237000, 6832000, pch = 21, col = 'red', cex = 2)"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"gathering-information-for-nimble","dir":"Articles","previous_headings":"Spatial Capture Recapture","what":"Gathering Information for Nimble","title":"Lab9: Spatial Capture Recapture","text":"Time get information order Nimble. First constants data. three types data: + 0’s trick model isn’t using point process distribute points + observations butterflies, array dimension M ntraps occasion + lookup table, isn’t actually data nimble doesn’t like constants whatever reason Finally, initial values. Also parameters interest: Time see initial values working!","code":"npix = length(hab) pixelArea = prod(res(habitat_sp)/1e5) M = nind*1.5 traps <- Finnish_butterflies$traps trap_locs <- traps[,c('x', 'y')]/1e5 nconsts <- list(G = npix, pixelArea = pixelArea, habitat = hab, M = M, xlim = lims[1:2],                  ylim = lims[3:4], delta = .005, x = as.matrix(trap_locs),                  effort = as.matrix(effort[,-1]), ntraps = nrow(traps),                 nocc = nocc) zeros <- array(0, M) y <- array(0, c(M, nrow(traps), nocc)) for(i in 1:nrow(caps)){   me <- caps$Individual[i]   day <- caps$Day[i]   trap <- caps$Trap[i]   y[me, trap, day] <- 1  }  ndat <- list(y = y, zeros = zeros, lookup = pixel_order) ninits <- list(beta0 = rnorm(3,1,1), g0 = runif(1), sigma = runif(1, 3, 5)) #give a big sigma z.init <- c(rep(1, nind), rep(0, M-nind)) s.init <- array(NA, c(M, 2)) for(i in 1:M){   mytraps <- which(rowSums(y[i,,]) >0)   if(length(mytraps) >0 ){ #guys we saw     s.init[i, ] <- colMeans(trap_locs[mytraps, ])   } else { #augmented guys     s.init[i, 1] <- runif(1, lims[1], lims[2]) #x coord      s.init[i, 2] <- runif(1, lims[3], lims[4]) #y coord   } } ninits$z <- z.init ninits$s <- s.init prms <- c('beta0', 'g0', 'sigma', 'N', 'EN', 'psi')"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"running-the-scr-model","dir":"Articles","previous_headings":"Spatial Capture Recapture","what":"Running the SCR Model","title":"Lab9: Spatial Capture Recapture","text":"*Note, model take decent amount time run. Time plot:  Exact estimates can summarized well (remember sigma needs multiplied 1e5) Let’s compare CMR method:","code":"library(parallel) cl <- makeCluster(2) #each chain will be run separately  clusterExport(cl = cl, varlist = c(\"ninits\",  \"nconsts\", 'ndat', \"scr_flies\", 'prms')) scr1 <- clusterEvalQ(cl = cl,{ library(nimble) #reload packages for each core library(coda) prepnim <- nimbleModel(code = scr_flies, constants = nconsts,                            data = ndat, inits = ninits, calculate = T) prepnim$initializeInfo() #will tell you what is or isn't initialized prepnim$calculate() #if this is NA or -Inf you know it's gone wrong mcmcnim <- configureMCMC(prepnim, monitors = prms, print = T) nimMCMC <- buildMCMC(mcmcnim) #actually build the code for those samplers Cmodel <- compileNimble(prepnim) #compiling the model itself in C++; Compnim <- compileNimble(nimMCMC, project = prepnim) # compile the samplers next Compnim$run(niter = 50000, nburnin = 25000, thin = 1) res <- (as.mcmc(as.matrix(Compnim$mvSamples))) return(res) }) #this will take awhile and not produce any progress bar scr1 <- as.mcmc.list(scr1) stopCluster(cl) MCMCvis::MCMCtrace(scr1, pdf = F, Rhat = T, n.eff = T) MCMCsummary(scr1) #>               mean        sd       2.5%       50%     97.5% Rhat n.eff #> EN       1.668e+02 8.4416561 150.386066 1.669e+02 1.834e+02 1.00  7082 #> N        1.706e+02 4.5663976 163.000000 1.700e+02 1.800e+02 1.00  9965 #> beta0[1] 1.195e+01 0.0507878  11.848617 1.195e+01 1.205e+01 1.00  7098 #> beta0[2] 1.001e+00 1.0062739  -0.965923 1.004e+00 3.000e+00 1.00 11447 #> beta0[3] 9.927e-01 1.0112792  -1.006757 9.886e-01 2.977e+00 1.00 11274 #> g0       3.576e-03 0.0002033   0.003192 3.573e-03 3.982e-03 1.00  7847 #> psi      7.130e-01 0.0360755   0.642675 7.131e-01 7.837e-01 1.00  7082 #> sigma    1.186e+00 1.0453899   0.146388 8.694e-01 3.935e+00 1.01  4343 CMR_est <- MCMCsummary(cmr2, params = 'N') SCR_est <- MCMCsummary(scr1, params = 'N') rbind(CMR_est, SCR_est) #>     mean    sd 2.5% 50% 97.5% Rhat n.eff #> N  169.7 4.438  162 169   179    1 33068 #> N1 170.6 4.566  163 170   180    1  9965"},{"path":"http://rushinglab.github.io/WILD8370/articles/Lab9_SCR.html","id":"homework","dir":"Articles","previous_headings":"","what":"Homework","title":"Lab9: Spatial Capture Recapture","text":"lab (), used Mt version closed CMR model (detection probability changed time). Perform prior predictive check model. Report posterior p occasion. notice informative, adjust alpha values posteriors look reasonable. Re-write closed CMR model Mb model (trap shyness). Compare abundance estimates got Mt version model graphing CIs (medians) models. SCR model, detection probability predicts male female butterflies equally likely captured. Re-write model allow male female butterflies different detection probabilities. can adjust either g0 sigma. need run model, just write code. Using SCR beta outputs lab (one just wrote Q3), make map showing expected abundance pixel study area well trap locations. Notice model predicts high abundance poor quality habitat. think ? (Note: inaccurate model, clearly ?) 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"what-is-markdown","dir":"Articles","previous_headings":"","what":"What is Markdown?","title":"Introduction to R Markdown","text":"discuss R Markdown , need discuss Markdown . Markdown? Let’s start ’s . Many probably created report paper using word processor like Microsoft Word Google Docs. Word processors referred “see get” (wysiwyg) text editors. means highlight text click boldface icon Word, text appears bold screen. sorts formatting options, including making headers, inserting figures, adding page numbers, etc., possible clicking buttons. code behind scenes creates changes users don’t see code, formatting output. makes wysiwyg editors relatively easy use beginners. advanced users, can actually problematic. ever Word act ways don’t fully understand? course! . ever tried opening .docx file using older version Word, find doesn’t look way thought ? ever inserted figure jump another page get ‘anchored’ bottom page? just problems occur document bunch hidden formatting code see understand. Markdown different. Markdown files plain text files, meaning can created edited using text editors (like NotePad Windows TextEdit Mac). biggest difference Markdown files Word documents formatting Markdown documents occurs document rather behind scenes. make something boldface tell Markdown putting two **asterisks** either side word phrase. Italics done putting one *asterisk* around text. Hyperlinks written like : [Hyperlinks](https://en.wikipedia.org/wiki/Markdown). just many formatting options can include Markdown document. ’ll learn options like headers, lists, mathematical symbols equations, figures later tutorial throughout semester. ’re writing, text won’t look bold italic whatever (‘see get’, ’s ‘see type’). formatting shows render Markdown file create another type document (pdf, html, even Word). nice thing Markdown uses standard ways express specific formatting options, can convert documents different output formats easily.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"what-is-r-markdown","dir":"Articles","previous_headings":"","what":"What is R Markdown?","title":"Introduction to R Markdown","text":"course, use specific ‘flavor’ Markdown called ‘R Markdown’. R Markdown gives us formatting options available Markdown plus ability embed, display, run R code documents. mixing R code plain text, can create dynamic reports replicate analytical processes, show code underlying processes, create output analysis (figures, summary statistics, etc.), provide necessary text explanations go along code output.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"why-use-r-markdown","dir":"Articles","previous_headings":"","what":"Why use R Markdown","title":"Introduction to R Markdown","text":"R Markdown many advantages compared creating reports Word GoogleDocs. advantages include: Versatility- Want convert Word document pdf? ’s hard. pdf Word? ’s pain. PDF HTML? Maybe know don’t. R Markdown, can change formats single click (single line code). can even convert pretty nice slide shows. Embed code text - running analysis, get results Word? Type hand? Copy--paste? pain error prone. Rerun analysis using new data? Oops, now copy paste new results figures. R Markdown, embed code directly text results figures get added reports automatically. means copying pasting updating reports new results come . Annotate code - Using # great adding small annotations R scripts definitely get habitat . sometimes need add lot details help users (future self) make sense complex code. R Markdown allows create documents much text formatting need, along code. Version control - Tired saving manuscript_v1.doc, manuscript_v2.doc, manuscript_final.doc, manuscript_final_v2.doc? version control . won’t go specifics R Markdown allows seamlessly use version control systems like git Github document changes reports. Edit text files - R Markdown files easily created edited within RStudio don’t way. can opened edited base R even using text editors. means can create edit platform (Windows, Mac, Linux) using free software already installed computer Stability - many us Word crash ’re working paper? save working? Hope . R Markdown files smaller lightweight, tend cause computer crash ’re working . Focus text, formatting - spend lot time tweaking formatting Word document rather writing? R Markdown allows separate writing process formatting process, allows focus former without worrying later (theory least). Plus lots templates can use ensure formatting taken care without anything special!","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"why-not-use-r-markdown","dir":"Articles","previous_headings":"","what":"Why not use R Markdown?","title":"Introduction to R Markdown","text":"disadvantages R Markdown. adviser doesn’t use - Try sending .Rmd file adviser get feedback. ’ll wait… Like , folks still use word processors, adopt R Markdown still create edit Word documents collaborators stuck ways track changes - Even ’re lucky adviser review .Rmd file, won’t get nice track changes like Word. alternative (version control helps) none quite easy track changes. Fewer formatting options - better worse, limited set formatting options R Markdown. can constraining (often ’s actually freeing!) ’s learning curve - already know use Word. R Markdown new. make something bold? insert equations? get figures go end document? first, almost certainly google almost every thing need R Markdown (number 1 problem). pretty simple still means going can slow first.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"creating-a-new-r-markdown-file","dir":"Articles","previous_headings":"","what":"Creating a new R Markdown file","title":"Introduction to R Markdown","text":"Click File -> New File -> R Markdown... Choose title format (HTML, pdf, Word) document Click Ok Save newly created document Pretty easy","code":""},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"the-yaml-header","dir":"Articles","previous_headings":"Basic formatting","what":"The YAML header","title":"Introduction to R Markdown","text":"top .Rmd file, see several line three blue dashes: called “YAML header” ’s can control lot major formatting options documents. example, change output pdf, just switch html_document pdf_document (note, may need install Latex distribution knit pdf. get error message step, see suggestions ) click Knit button Pretty cool, right? YAML header allows control many “high level” options document. example, change font size, type following directly output: pdf_document argument: Check see font size changed clicking Knit. Changing font type little trickier. Behind scenes, R Markdown turns document Latex code, converted pdf. don’t need know much Latex (though little knowledge helpful) conversion mean formatting options passed Latex converter specific ways. tell Latex want use Arial font, modify output: argument follows: Make sure include spaces indent pdf_document: latex_engine: xelatex. indent first line paragraph, add following header: many possible options header (see additional examples). ’ll learn options later semester.","code":"--- title: \"Test document\" author: \"Clark Rushing\" output: html_document --- fontsize: 12pt title: \"FANR6750\" subtitle: \"Homework 1\" author: \"YOUR NAME HERE\" date: \"2025-04-18\" output:    pdf_document:     latex_engine: xelatex  mainfont: Arial indent: true"},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"content-headers","dir":"Articles","previous_headings":"Basic formatting","what":"Content headers","title":"Introduction to R Markdown","text":"Using headers natural way break document report smaller sections. can include headers putting one # signs front text. One # main header, ## secondary header, etc.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"paragraph-and-line-breaks","dir":"Articles","previous_headings":"Header 1","what":"Paragraph and line breaks","title":"Introduction to R Markdown","text":"writing chunks text R Markdown (e.g., report manuscript), can create new paragraphs leaving empty line paragraph: want force line break, include two spaces end line want break:","code":"This is one paragraph.  This is the next paragraph This is one line   This is the next line"},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"bold-italics","dir":"Articles","previous_headings":"Header 1","what":"Bold, Italics","title":"Introduction to R Markdown","text":"mentioned earlier, create boldface surrounding text two asterisks (**bold**) use single asterisks italics (*italics*)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"code-type","dir":"Articles","previous_headings":"Header 1","what":"Code type","title":"Introduction to R Markdown","text":"highlight code (note, actually insert functioning code, just formats text show code rather plain text), surround text back ticks: mean() can include multiple lines code including three back ticks line code three back ticks line code:","code":"Multiple lines of code look like  this"},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"bulleted-lists","dir":"Articles","previous_headings":"Header 1","what":"Bulleted lists","title":"Introduction to R Markdown","text":"Bulleted lists can included starting line asterisk can also start lines single dash - sub-sub-bullets, indent twice (press tab two times) start -","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"numbered-lists","dir":"Articles","previous_headings":"Header 1","what":"Numbered lists","title":"Introduction to R Markdown","text":"Numbered lists look like can also include sub-levels number lists can lower case roman numerals lowercase letters B. uppercase letters","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"quotations","dir":"Articles","previous_headings":"Header 1","what":"Quotations","title":"Introduction to R Markdown","text":"highlight quotations starting line >, produces: models wrong useful (George E.P. Box)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"hyperlinks","dir":"Articles","previous_headings":"Header 1","what":"Hyperlinks","title":"Introduction to R Markdown","text":"Insert hyperlinks putting text want displayed square brackets followed link parentheses: [RStudio cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"equations","dir":"Articles","previous_headings":"Header 1","what":"Equations","title":"Introduction to R Markdown","text":"Inserting equations R Markdown knowing Latex really comes handy equations written using Latex code. part, difficult need insert complex equations probably need look code symbols. many good resources including feeling particularly ambitious .","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"inline-vs--block-equations","dir":"Articles","previous_headings":"Header 1 > Equations","what":"Inline vs. block equations","title":"Introduction to R Markdown","text":"can include equations either inline (e = mc^2) stand-alone block: e=mc^2 Inline equations added putting single dollar sign $ either side equation ($e=mc^2$). Equation blocks create starting ending new line double dollar signs $$e=mc^2$$","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"greek-letters","dir":"Articles","previous_headings":"Header 1","what":"Greek letters","title":"Introduction to R Markdown","text":"Statistical models include lot Greek letters (\\alpha, \\beta, \\gamma, etc.). can add Greek letters equation typing backslash \\ followed name letter \\alpha. Uppercase lower case letters possible capitalizing name (\\Delta = $\\Delta$) (\\delta = $\\delta$).","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"subscripts-and-superscripts","dir":"Articles","previous_headings":"Header 1","what":"Subscripts and superscripts","title":"Introduction to R Markdown","text":"can add superscripts using ^ (\\pi r^2=$\\pi r^2$) symbol subscripts using underscore _ (N_t = $N_t$). superscript subscript includes one character, put entire script within curly brackets {}: N_t-1 \\neq N_{t-1} $N_t-1 \\neq N_{t-1}$","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"brackets-and-parentheses","dir":"Articles","previous_headings":"Header 1","what":"Brackets and parentheses","title":"Introduction to R Markdown","text":"can add normal sized brackets parenthesis just typing equation: (x + y) = (x + y) need bigger sizes, using $\\big($, $\\bigg($, $\\Bigg($ produces \\big(, \\bigg(, \\Bigg( (switch opening parenthesis closing parenthesis square bracket needed)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"fractions","dir":"Articles","previous_headings":"Header 1","what":"Fractions","title":"Introduction to R Markdown","text":"Fractions can either inline (1/n = $1/n$) stacked (\\frac{1}{n} = $\\frac{1}{n}$). stacked equations, terms first curly brackets numerator terms second curly brackets denominator.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"operators","dir":"Articles","previous_headings":"Header 1","what":"Operators","title":"Introduction to R Markdown","text":"Pretty much every operator need can written latex. common ones include \\times ($\\times$), \\lt ($\\lt$), \\gt ($\\gt$), \\leq ($\\leq$), \\geq ($\\geq$), \\neq ($\\neq$), \\sum ($\\sum$), \\prod ($\\prod$), \\infty ($\\infty$), \\propto ($\\propto$). See documents Equations section list operators.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"adding-code","dir":"Articles","previous_headings":"","what":"Adding code","title":"Introduction to R Markdown","text":"ability format create pdf html documents great real strength R Markdown ability include run code within document. Code can included inline chunks","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"inline-code","dir":"Articles","previous_headings":"Adding code","what":"Inline code","title":"Introduction to R Markdown","text":"Inline code useful including (simple) R output directly text. Inline code can added enclosing R code `r `. example, typing `r mean(c(3,7,4,7,9))` compute print mean given vector. , print 6 instead code . can useful including summary statistics reports. example, vector indicating number individuals captured occasion mark-recapture study (e.g., n <- c(155, 132, 147, 163)) want include number occasions report, instead typing 4, can type `r length(n)`. prevent typos, extremely useful length(n) might change future. Instead manually changing number occasions, just re-render document new number occasions printed automatically.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"code-chunks","dir":"Articles","previous_headings":"Adding code","what":"Code chunks","title":"Introduction to R Markdown","text":"complicated code, generally useful use chunks inline code. Chunks start separate line ```{r} end ``` line (instead manually, can click Insert button top right script window, click R). two lines, can include many lines code want. example, ```{r}n1 <- 44     # Number individuals captured first occasionn2 <- 32     # Number individuals captured second occasionm2 <- 15     # Number previously marked individuals captured second occasionN <- n1 * n2 / m2     # Lincoln-Peterson estimate abundance```","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"chunk-options","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Chunk options","title":"Introduction to R Markdown","text":"Code chunks can take lot options control code run displayed documents. options go {r closing } (see options put cursor {r, hit space bar, hit tab). example: echo = FALSE shows output code code include = FALSE runs code display code output (useful chunks read format data) eval = FALSE shows code run (useful showing code) warning = FALSE message = FALSE can include ensure error messages warnings printed, can useful cleaning appearance documents cache = TRUE save results R code doesn’t rerun chunk unless code changed (useful chunks take long time run) .height .width control size figures pdf document inches centimeters (e.g., `.height = “3in”, notice quotation marks) See main R Markdown page complete list possible options.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"setting-defaults-for-all-chunks","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Setting defaults for all chunks","title":"Introduction to R Markdown","text":"Often useful set default behavior chunks rather including, example, warning = FALSE beginning one. , can include chunk beginning document: ```{r include = FALSE}opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)``` options can included chuck set default behaviors. can -ride defaults within chunks needed. can also load common packages chunk streamline chunks later document.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"tables","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Tables","title":"Introduction to R Markdown","text":"nicely print matrices data frames R Markdown document, use kable() function: Table 1: Automobile data 1974 Motor Trends US. kableExtra package provides even advanced options creating nice looking tables. See overview options provided package.","code":"library(knitr) kable(head(mtcars), caption= 'Table 1: Automobile data from 1974 *Motor Trends US*.',       align = 'l')"},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"figures","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Figures","title":"Introduction to R Markdown","text":"can also easily produce figures directly RMarkdown file. , simple demonstration produce many better looking figures throughout semester. Figure 1: Automobile mpg function horsepower.","code":"plot(mtcars$hp, mtcars$mpg)"},{"path":"http://rushinglab.github.io/WILD8370/articles/RMarkdown.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional resources","title":"Introduction to R Markdown","text":"RStudio tool bar, click Help -> Cheatsheets select R Markdown cheat sheet (lots good cheat sheets well) RStudio’s R Markdown tutorial Tom Edward’s R Markdown tutorial Coding Club’s Getting Started R Markdown Cosma Shalizi’s Using R Markdown Class Reports","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/graphics.html","id":"attributes-of-good-figures","dir":"Articles","previous_headings":"","what":"Attributes of good figures","title":"Checklist and tips for publication-quality graphics in R","text":"Effective data visualizations : Tell story. figure manuscript report communicate one main findings want readers know. casual reader able understand primary findings looking figures. intuitive. Strive figures readers can understand without read caption text. Readers least grasp main gist figure just looking . Err towards simplicity. Avoid clutter. Avoid gratuitous use visual attributes (color, size, shape) show attributes data. Avoid background colors. Use grid lines sparingly make interpretation data easier. readable. default sizes axis labels text rarely big enough. almost always need make bigger. Maximize data-ink ratio, within reason. Related point 3, “ink” figure show data rather non-data (axis ticks/labels, titles, legends, grid lines, etc.). Maximize ink shows data relative non-data, within reason. visual hierarchy. important information plot visible. Use layering, color, brightness, size, transparency, etc. make important attributes (generally data, sometimes even specific portions data) jump less important attributes (axis ticks/lines, grid lines, less important data points) fade background.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/graphics.html","id":"what-type-of-figure-should-i-use","dir":"Articles","previous_headings":"","what":"What type of figure should I use?","title":"Checklist and tips for publication-quality graphics in R","text":"Trends time series - line chart Amounts/comparisons discrete groups - bar chart, dot plot error bars Frequencies/distributions - histogram Associations 2 continuous variables - scatterplot Comparing distributions discrete groups - box--whisker plot, violin plt Spatial relationships - map","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/graphics.html","id":"visualization-checklist","dir":"Articles","previous_headings":"","what":"Visualization checklist","title":"Checklist and tips for publication-quality graphics in R","text":"list considerations making high-quality figures publications. complete list considered suggestions. figure include caption clearly explains elements needed interpret visualization? Remember caption figure understandable without referencing main text. figure accurately show variability data uncertainty estimates? axes start stop reasonable values? axis titles clearly indicate variables shown? include units? axis labels, axis titles, text annotations appropriately sized? figure includes different colors, shapes, sizes, attributes communicate properties data? figure includes different colors, shapes, sizes, legend show values represented attributes? color palettes colorblind-friendly? overlapping elements (points, bars, etc.), can clearly distinguished? figure includes multiple panels, axis limits consistent? figure includes multiple panels, panel labeled? grid lines? , show relevant information needed interpret figure? grid lines? , appropriately sized colored distract data? use 3D pie chart? 😠","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/graphics.html","id":"saving-high-resolution-figures-for-publication","dir":"Articles","previous_headings":"","what":"Saving high-resolution figures for publication","title":"Checklist and tips for publication-quality graphics in R","text":"many cases, need save figures rather render directly .rmd file. ggsave() function (opinion) easiest way save high-resolution figures. default, ggsave() save last plot created R using ssame size graphics device. tell function save figure name file. , example, following code save ggsave() also uses whatever file type name figure file guess format save figure (case, png). Options include: “eps”, “ps”, “tex”, “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg”. can also change defaults , example, save specific figure object, change figure size, change figure resolution. example, Easy!","code":"libary(ggplot2)  df <- data.frame(x = seq(1:10),                   y = seq(1:10))  ggplot(df, aes(x = x, y = y)) +   geom_point()  ggsave(\"figs/scatterplot.png\") libary(ggplot2)  df <- data.frame(x = seq(1:10),                   y = seq(1:10))  p <- ggplot(df, aes(x = x, y = y)) +       geom_point()  ggsave(filename = \"figs/scatterplot.png\",        plot = p,        width = 5,         height = 8,        units = \"in\",        dpi = 600)"},{"path":"http://rushinglab.github.io/WILD8370/articles/graphics.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional resources","title":"Checklist and tips for publication-quality graphics in R","text":"Fundamentals Data Visualization, Claus Wilke. comprehensive book data visualization, lots R code.","code":""},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/homework.html","id":"step-1-create-a-new-directory-to-store-your-homework-files","dir":"Articles","previous_headings":"Homework steps","what":"Step 1: Create a new directory to store your homework files","title":"Homework instructions","text":"Create new directory (ideally subdirectoy WILD8370\\homework directory) call LastNameFirstName-Homework#, replacing LastNameFirstName last first names # appropriate homework number","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/homework.html","id":"step-2-create-a-new-r-markdown-file","dir":"Articles","previous_headings":"Homework steps","what":"Step 2: Create a new R Markdown file","title":"Homework instructions","text":"2a) Click File -> New File -> R Markdown... 2b) Title: window type Lab # assignment, substituting correct lab number. 2c) Author window, type name 2d) Click Ok 2e) Save R Markdown file LastnameFirstname-homework# directory created step 1, replacing LastNameFirstName last first names # appropriate homework number","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/homework.html","id":"step-3-complete-the-assignment","dir":"Articles","previous_headings":"Homework steps","what":"Step 3: Complete the assignment","title":"Homework instructions","text":"Complete assignment, following instructions lab document go","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/homework.html","id":"step-4-submit-your-assignment","dir":"Articles","previous_headings":"Homework steps","what":"Step 4: Submit your assignment","title":"Homework instructions","text":"4a) submitting assignment, always click “Knit” button sure .Rmd file can rendered HTML page. problems rendering file, please contact TA prior submission deadline. 4b) know file can rendered, upload LastnameFirstname-homework#.Rmd LastnameFirstname-homework#.html files eLC correct assignment folder Assignments fail follow instructions graded","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/homework.html","id":"notes","dir":"Articles","previous_headings":"Homework steps","what":"Notes:","title":"Homework instructions","text":"Homework graded complete/incomplete basis based effort. find homework difficult encounter error solve, write problem , ’ve tried, solutions didn’t work. Homework fully answered detailed explanation steps tried solve issue given full credit help us understand need focus future lectures.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"what-is-r","dir":"Articles","previous_headings":"","what":"What is R?","title":"Lab 0: Introduction to R","text":"R free, open-source programming language software environment statistical computing, bioinformatics, visualization general computing. based ever-expanding set analytical packages perform specific analytical, plotting, programming tasks.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"why-r","dir":"Articles","previous_headings":"","what":"Why R?","title":"Lab 0: Introduction to R","text":"R free(!), runs pretty much every operating system, huge user base. R far programming language working data, widely used language fields ecology, evolution, wildlife sciences. plan pursue career fields, proficiency Ris quickly becoming prerequisite many jobs. Even don’t pursue career one fields, ability manipulate, analyze, visualize data (otherwise known data science) extremely marketable skill many professions right now.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"additional-resources-and-where-to-get-help","dir":"Articles","previous_headings":"","what":"Additional resources and where to get help","title":"Lab 0: Introduction to R","text":"go basics using R lab sessions many good online resources learning R getting help. favorites (material developed) include: Tom Edward’s online Learning R course John Fieberg’s online Statistics Ecologists book Data Analysis Visualization R Ecologists course, encounter error messages don’t understand need help figuring accomplish something R, google best friend (even experienced R users use google daily basis). key finding answers google asking right questions. spend much time topic lab, please refer links advice formulating R-related questions: ask R help Seeking help Data Analysis Visualization R Ecologists","code":""},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"the-rstudio-interface-and-panes","dir":"Articles","previous_headings":"Using R- the very basics","what":"The RStudio interface and panes","title":"Lab 0: Introduction to R","text":"Although users can work directly R, choose use RStudio IDE (Integrated Development Environment) R. use RStudio, must first R installed. opening RStudio, see 3 panes1. Console: console appear left side screen. can type code directly console (also known command line) executed immediately. console also output shown tasks executed R. Environment pane: environment pane appear top right screen. , can see objects created R well values objects R interprets (later). environment pane also includes tabs require use class. Plot pane: plot pane appear bottom right screen. might imagine, graphics displayed created R. pane also includes several useful tabs including Files tab (allows navigate manage files), Packages tab (can install manage additional R packages), Help tab can search R documentation pages.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"using-r-as-a-calculator","dir":"Articles","previous_headings":"Using R- the very basics","what":"Using R as a calculator","title":"Lab 0: Introduction to R","text":"statistical programming tool, one thing R good math. starting point, let’s treat R like fancy calculator. interact calculator typing numbers operators (+, -, *, /) Console window. Let’s try - bottom left window (Console), write Rcode required add two plus two press enter: run code, see answer printed window. Play code bit - try changing number operators run code .","code":"2+2"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"creating-objects","dir":"Articles","previous_headings":"Using R- the very basics","what":"Creating objects","title":"Lab 0: Introduction to R","text":"can run R like calculator typing equations directly console printing answer. usually don’t want just calculation see answer. Instead, assign values objects. object saved R’s memory allows us use object later analysis. might seem bit confusing new programming let’s try . following code creates object called x assigns value 3: operator <-2 3 assignments R. Whatever left <- object’s name whatever right value. see later, objects can much complex simply number now, ’ll keep simple. try - change code create object called new.x. Instead assigning new.x number, give calculation, example 25/5. think value new.x ?","code":"x <- 3"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"naming-objects","dir":"Articles","previous_headings":"Using R- the very basics","what":"Naming objects","title":"Lab 0: Introduction to R","text":"’s good idea give objects names tell something object represents. Names can long want spaces (also remember long names require typing brevity good rule thumb). Names can contain numbers letters begin number. R also case-sensitive , example, Apple apple. creating object names, also good idea avoid words show R functions. R generally smart enough distinguish attempting create object vs use function, avoiding practice save headache interpreting code (especially code looked ).","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"working-with-objects","dir":"Articles","previous_headings":"Using R- the very basics","what":"Working with objects","title":"Lab 0: Introduction to R","text":"exercise , may noticed running code, R print anything. simply told R create object (top right window, click Environment tab, see x new.x). Now stored R’s memory, can lot things . one, can print see value. , simply type name object run code4: can also use objects create new objects. think following code ? running , print new object y see value. right?","code":"new.x <- 25/5 new.x #> [1] 5 x <- 3 y <- x*4"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"r-scripts","dir":"Articles","previous_headings":"","what":"R scripts","title":"Lab 0: Introduction to R","text":"console useful simple tasks analyses become complicated, console efficient. need go back change line code? want show code someone else get help? Instead using console, work done using scripts (source editor pane). Scripts special files allow us write, save, run many lines code. Scripts can saved can work later send collaborators. create script, click File -> New File -> R Script. new file show new window.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"commenting-your-code","dir":"Articles","previous_headings":"R scripts","what":"Commenting your code","title":"Lab 0: Introduction to R","text":"R ignore code follows #. useful making code readable others. Use comments remind newly created object , explain line code , leave reminder later, etc. example, previous code, might good idea use comments define object represents: Notice run code, R ignores comments.","code":"n1 <- 44     # Number of individuals captured on first occasion  n2 <- 32     # Number of individuals captured on second occasion    m2 <- 15     # Number of previously marked individuals captured on second occasion"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"the-working-directory","dir":"Articles","previous_headings":"R scripts","what":"The working directory","title":"Lab 0: Introduction to R","text":"Now created new R script, need able save file somewhere computer. , can set working directory. addition providing place save script, setting working directory also tells R like put files come data management analyses (e.g. spreadsheets graphics) well find source data plan use particular project. two methods exist set working directory within R. can choose set working directory clicking Session –> Set working directory –> Choose directory navigating folder like store files. opened R script unsure current working directory located, can run getwd() see current working directory. can set working directory directly R script using setwd() function. example, set working directory folder called Lab_1 desktop, run following line code: C:/Users/mab46065/Desktop/Lab_1. Notice although computer probably create folder pathway using backslash (\\), R require forward slashes (/) instead. Also, using Mac, omit c: directory name.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"r-data-object-types","dir":"Articles","previous_headings":"","what":"R data object types","title":"Lab 0: Introduction to R","text":"point, briefly talked creating objects R. , discuss different object types R. important know types objects (e.g. vectors, lists, matrices, factors, data frames, arrays) working R interpret differently different object types required perform certain tasks. learn data structures encounter lab exercises.","code":""},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"integer-class","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Integer class","title":"Lab 0: Introduction to R","text":"far, working objects store single number. However, often convenient store string numbers single object. R, strings called vectors usually created enclosing string c( ): can also create sequences consecutive numbers different ways: seq() function flexible useful familiar , sure look help page better understand use . Another useful function creating vectors rep(), repeats values vector: : sure notice difference using times argument vs argument! function class() indicates class (type element) object:","code":"x <- c(3,5,2,5) x #> [1] 3 5 2 5 x <- 1:10 x #>  [1]  1  2  3  4  5  6  7  8  9 10  x2 <- seq(from = 1, to = 10, by = 1) x2 #>  [1]  1  2  3  4  5  6  7  8  9 10 rep(x2, times = 2) #>  [1]  1  2  3  4  5  6  7  8  9 10  1  2  3  4  5  6  7  8  9 10 rep(x2, each = 2) #>  [1]  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 class(x) #> [1] \"integer\""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"character-class","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Character class","title":"Lab 0: Introduction to R","text":"vector can also contain characters (though mix numbers characters vector!): quotes around “Occasion1”, “Occasion2”, “Occasion3” critical. Without quotes, R assume objects called Occasion1, Occasion2 Occasion3. objects don’t exist R’s memory, error message. Vectors can length (including 1. fact, numeric objects ’ve working just vectors length 1). function length() tells long vector : class vector numeric characters entries? Hint: can also use c() function add elements vector:","code":"occasions <- c(\"Occasion1\", \"Occasion2\", \"Occasion3\") occasions #> [1] \"Occasion1\" \"Occasion2\" \"Occasion3\" class(occasions) #> [1] \"character\" length(x) #> [1] 10 mixed <- c(1, 2, \"3\", \"4\") y <- c(x, 4,8,3)"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"factor-class","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Factor class","title":"Lab 0: Introduction to R","text":"Another class vectors referred factors. Factors similar character vectors R interpreting text strings perform math . difference, however, R sees factors grouping variables. category within factor referred ‘level’5.","code":"Species <- as.factor(c(1,3,4,2,3,3,4,1,1,1)) Species #>  [1] 1 3 4 2 3 3 4 1 1 1 #> Levels: 1 2 3 4 class(Species) #> [1] \"factor\""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"vectorized-arithmetic","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Vectorized arithmetic","title":"Lab 0: Introduction to R","text":"One useful properties vectors R can use simplify basic arithmetic operations need done multiple observations. example, consider following data wing chord (measure wing length) body mass Swainson’s thrushes (Catharus ustulatus): Swainson’s Thrush. Image courtesy VJAnderson via Wikicommons Perhaps want derive body condition individual based measures. One common metric body condition used ornithologists \\frac{mass}{size}, wing chord used proxy body size. calculate body condition individual: time consuming error prone. Luckily, R vectorize basic arithmetic: can see, divide one vector another, R divides first element first vector first element second vector, etc. returns vector. Vectorized arithmetic works well vectors using length. happen though perform arithmetic vectors different lengths? Try running following code seeing R vectors. Notice way R recycles vector depends longer.","code":"cond1 <- 36.2/95.1 # Body condition of the first individual  cond2 <- 34.6/88.4 # Body condition of the second individual mass <- c(36.2, 34.6, 31.0, 31.8, 29.4, 32.0) wing <- c(95.1, 88.4, 97.9, 96.8, 92.3, 90.6)  cond <- mass/wing cond #> [1] 0.3807 0.3914 0.3166 0.3285 0.3185 0.3532 a <- c(1,10,100,1000) b <- c(1,2,3,4,5) c <- a/b c #> [1]   1.00   5.00  33.33 250.00   0.20  x <- c(1,10,100,1000, 10000) y <- c(1,2,3,4) z <- x/y z #> [1]     1.00     5.00    33.33   250.00 10000.00"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"indexing-vectors","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Indexing vectors","title":"Lab 0: Introduction to R","text":"Often need work just subset vector. example, maybe vector plant biomass measured along transects need first third observations. Notice index certain elements vector y, use square brackets. Inside brackets, provided integer vector, integer refers position elements first vector. indexing vector can length (including 1). can also index vectors using logical vector. logical vector special type object contains values TRUE FALSE. using logical vector indexing, logical vector indicates elements keep (TRUE) remove (FALSE) original vector. reason, indexing vector must length focal vector; .e., length() == length(v) can also use indexing remove elements vector: rearrange order vector","code":"y <- c(2, 4, 8, 4, 25) y[c(1,3)] #> [1] 2 8 # Logical vector (which elements of y are greater than 4?) y > 4 #> [1] FALSE FALSE  TRUE FALSE  TRUE # Indexing using a logical vector (keep elements 3 and 5) y[y > 4] #> [1]  8 25 # Remove the second element y[-2] #> [1]  2  8  4 25 y[c(5,4,3,2,1)] #> [1] 25  4  8  4  2"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"functions","dir":"Articles","previous_headings":"R data object types","what":"Functions","title":"Lab 0: Introduction to R","text":"power R apparent large number built-functions available users. Functions small bits code perform specific task. functions accept one inputs called arguments return value new object. Let’s say following data number ticks recorded 5 dogs: total number ticks recorded study? , can use built-sum() function: mean number ticks per dog? variance?","code":"ticks <- c(4,7,2,3,150)  sum(ticks) #> [1] 166 mean(ticks) #> [1] 33.2 var(ticks) #> [1] 4267"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"arguments","dir":"Articles","previous_headings":"R data object types > Functions","what":"Arguments","title":"Lab 0: Introduction to R","text":"Every function takes different set arguments many cases need look arguments . best way get help specific function type question mark followed function name, bring help page bottom right panel. example, round function rounds number specified number decimal places. useful function don’t want print really large number digits: see round takes argument called x, number want round, number digits want round . provide arguments exact order defined don’t name . example, : name arguments, can switch order: Although don’t name arguments, ’s good idea get habit naming . make code easier read, help avoid mistakes can occur don’t put arguments correct order, makes easier trouble shoot code doesn’t expect .","code":"?round y <- mean(ticks) y #> [1] 33.2  round(y, 0) #> [1] 33 round(digits = 0, x = y) #> [1] 33"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"matrices","dir":"Articles","previous_headings":"R data object types","what":"Matrices","title":"Lab 0: Introduction to R","text":"Matrices similar vectors two dimensions. first dimension shows number rows matrix second shows number columns. , combined multiple vectors create matrix. Notice vectors need length. Notice matrices can contain one data class, numeric vectors coerced characters. matrices many uses R, one drawback lead us directly next object type.","code":"Site <- c(1,2,3,4,5) Species <- c('Alasmidonta varicosa',              'Alasmidonta varicosa',              'Alasmidonta varicosa',               'Lasmigona decorata',               'Lasmigona decorata') Year <- c(rep(2023,5)) mymatrix <- cbind(Site, Species, Year) mymatrix #>      Site Species                Year   #> [1,] \"1\"  \"Alasmidonta varicosa\" \"2023\" #> [2,] \"2\"  \"Alasmidonta varicosa\" \"2023\" #> [3,] \"3\"  \"Alasmidonta varicosa\" \"2023\" #> [4,] \"4\"  \"Lasmigona decorata\"   \"2023\" #> [5,] \"5\"  \"Lasmigona decorata\"   \"2023\""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"data-frames","dir":"Articles","previous_headings":"R data object types","what":"Data frames","title":"Lab 0: Introduction to R","text":"Although useful many applications, vectors matrices limited ability store multiple types data (numeric character). data frames become useful. Perhaps common type data object use R data frame. Data frames tabular objects (rows columns) similar structure spreadsheets (think Excel GoogleSheets). effect, data frames store multiple vectors - column data frame vector. advantage matrices column can different class (numeric, character, etc.) values within column must class. Just first row Excel spreadsheet can list column names, column data frame name (hopefully) provides information values column represent. see data frames work, let’s load data frame called jayData comes FANR6750 package.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"an-aside-about-packages","dir":"Articles","previous_headings":"R data object types > Data frames","what":"An aside about packages","title":"Lab 0: Introduction to R","text":"One R’s primary strengths large number packages available users. Packages units shareable code data created R users. already seen built-functions R comes . Packages allow users share lots lots functions serve specific purposes. Packages also allow users share data sets. packages cleaning data, visualizing data, making maps, fitting specialized models, basically anything else can think . Accessing code package first requires installing package. needs done per computer usually done using install.packages() function: Note name package (case devtools) must quotation marks. Packages installed using install.packages() stored centralized repository called CRAN (Comprehensive R Archive Network). devtools (package) installed computer, need re-run install.packages() function unless re-install/update R need update package newer version. Installing package automatically make functions package available given R session. tell R functions come , must load package using library() function6: Unlike install.packages(), library() must re-run time open R. people include calls library() beginning script packages needed run code loaded beginning script. Occasionally, packages stored places (e.g., github). packages can installed using different functions. example, created package course contains small data sets use labs throughout semester. package stored github can installed running: Note install_github() function devtools package need run library(devtools) install package. Make sure install FANR6750 package now access data sets.","code":"install.packages(\"devtools\") library(devtools) install_github(\"RushingLab/WILD8370\")"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"back-to-dataframes","dir":"Articles","previous_headings":"R data object types > Data frames","what":"Back to dataframes","title":"Lab 0: Introduction to R","text":"Note - discussed , want access function data sets come packages, first need load package current working environment. , use library() function, unquoted package name argument. loaded, package’s functions available use. Alternatively, can access functions given package without loading package using package.name::function.name(). example, want use filter() function dplyr package, type dplyr::filter(). Although less commonly used, method advantages: Sometimes different packages functions names. R default using function package loaded last. example, raster package also function called filter() load dplyr first (using library()raster, R default using raster’s filter() function, cause problems. share code others, :: method makes clear packages use functions. additional clarity often helpful reason often use :: course. get quick idea information data frame contains, can use head() tail() functions, print first last 6 rows data frame: can see jaydata contains eight columns: x, y, elevation, forest, chaparral, habitat, seeds, jays. ’ll learn columns represents later semester, though just like functions, many data sets help pages also can access help pages using ?jaydata. Several useful functions investigating structure data frames str() summary() str() tells us structure data frame, example x y numeric columns habitat contains character strings. summary() provides simple summary statistics variable. Another useful function nrow(), tells us now many rows data frame (similar length() vectors):","code":"library(WILD8370) data(\"jaydata\") # the data() function loads data sets the come with packages  head(jaydata) #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423   0.00      0.02     Oak   Med   34 #> 2 261937 3769224       506   0.10      0.45     Oak   Med   38 #> 3 246337 3764124       859   0.00      0.26     Oak  High   40 #> 4 239437 3763524      1508   0.02      0.03    Pine   Med   43 #> 5 239437 3767724       483   0.26      0.37     Oak   Med   36 #> 6 236437 3769524       830   0.00      0.01     Oak   Low   39  tail(jaydata) #>          x       y elevation forest chaparral habitat seeds jays #> 95  258937 3767124       804   0.19      0.68     Oak   Med   40 #> 96  259837 3768024       210   0.00      0.00     Oak   Low   33 #> 97  249337 3769524       467   0.70      0.09    Pine   Med   36 #> 98  262237 3767424      1318   0.02      0.23     Oak   Med   44 #> 99  261937 3770124       354   0.00      0.05    Bare   Low   33 #> 100 247837 3769524       686   0.10      0.32     Oak   Med   40 str(jaydata) #> 'data.frame':    100 obs. of  8 variables: #>  $ x        : num  258637 261937 246337 239437 239437 ... #>  $ y        : num  3764124 3769224 3764124 3763524 3767724 ... #>  $ elevation: int  423 506 859 1508 483 830 457 304 834 164 ... #>  $ forest   : num  0 0.1 0 0.02 0.26 0 0.02 0 0.54 0 ... #>  $ chaparral: num  0.02 0.45 0.26 0.03 0.37 0.01 0.22 0.09 0.21 0.11 ... #>  $ habitat  : chr  \"Oak\" \"Oak\" \"Oak\" \"Pine\" ... #>  $ seeds    : chr  \"Med\" \"Med\" \"High\" \"Med\" ... #>  $ jays     : int  34 38 40 43 36 39 38 35 41 33 ...  summary(jaydata) #>        x                y             elevation        forest       #>  Min.   :230737   Min.   :3761424   Min.   :  12   Min.   :0.0000   #>  1st Qu.:238762   1st Qu.:3765324   1st Qu.: 365   1st Qu.:0.0000   #>  Median :245587   Median :3766824   Median : 548   Median :0.0000   #>  Mean   :246949   Mean   :3767130   Mean   : 659   Mean   :0.0553   #>  3rd Qu.:254662   3rd Qu.:3768699   3rd Qu.: 929   3rd Qu.:0.0300   #>  Max.   :266137   Max.   :3773724   Max.   :1537   Max.   :0.7000   #>    chaparral       habitat             seeds                jays      #>  Min.   :0.000   Length:100         Length:100         Min.   :30.0   #>  1st Qu.:0.080   Class :character   Class :character   1st Qu.:36.0   #>  Median :0.210   Mode  :character   Mode  :character   Median :38.0   #>  Mean   :0.241                                         Mean   :38.6   #>  3rd Qu.:0.370                                         3rd Qu.:41.0   #>  Max.   :0.850                                         Max.   :48.0 nrow(jaydata) #> [1] 100"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"subsetting-data-frames","dir":"Articles","previous_headings":"R data object types > Data frames","what":"Subsetting data frames","title":"Lab 0: Introduction to R","text":"see shortly, one common tasks working data frames creating new objects parts full data frame. task involves subsetting data frame - selecting specific rows columns. many ways subsetting data frames R, many discuss learn .","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"selecting-columns","dir":"Articles","previous_headings":"R data object types > Data frames > Subsetting data frames","what":"Selecting columns","title":"Lab 0: Introduction to R","text":"First, may want select subset columns big data frame. Data frames essentially tables, means can reference rows columns number: data.frame[row#, column#]. row column numbers put inside square brackets following name data frame object. row number always comes first column number second. want select rows specific column, just leave row# blank. example, wanted vector containing number jays survey location: can also select columns using data.frame$column (data.frame name data frame object column name column). example, Notice hit tab type $, RStudio bring columns can use buttons find one want. Sometimes may want select one column. One way indexing using column names7: can also use select remove columns:","code":"jaydata[,8] #>   [1] 34 38 40 43 36 39 38 35 41 33 34 37 37 38 42 43 39 37 38 40 37 35 37 44 45 #>  [26] 37 36 34 48 43 39 41 45 38 35 38 39 38 41 38 36 43 38 36 33 41 38 30 39 36 #>  [51] 39 36 34 30 38 37 44 36 36 40 44 48 37 41 42 30 41 39 43 30 42 42 41 38 36 #>  [76] 37 33 44 38 35 45 41 35 38 37 45 33 42 34 45 40 42 40 44 40 33 36 44 33 40 jaydata$jays #>   [1] 34 38 40 43 36 39 38 35 41 33 34 37 37 38 42 43 39 37 38 40 37 35 37 44 45 #>  [26] 37 36 34 48 43 39 41 45 38 35 38 39 38 41 38 36 43 38 36 33 41 38 30 39 36 #>  [51] 39 36 34 30 38 37 44 36 36 40 44 48 37 41 42 30 41 39 43 30 42 42 41 38 36 #>  [76] 37 33 44 38 35 45 41 35 38 37 45 33 42 34 45 40 42 40 44 40 33 36 44 33 40 head(jaydata[, c('x', 'y', 'jays')]) #>        x       y jays #> 1 258637 3764124   34 #> 2 261937 3769224   38 #> 3 246337 3764124   40 #> 4 239437 3763524   43 #> 5 239437 3767724   36 #> 6 236437 3769524   39 head(subset(jaydata, select= -c(seeds))) #>        x       y elevation forest chaparral habitat jays #> 1 258637 3764124       423   0.00      0.02     Oak   34 #> 2 261937 3769224       506   0.10      0.45     Oak   38 #> 3 246337 3764124       859   0.00      0.26     Oak   40 #> 4 239437 3763524      1508   0.02      0.03    Pine   43 #> 5 239437 3767724       483   0.26      0.37     Oak   36 #> 6 236437 3769524       830   0.00      0.01     Oak   39"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab01_intro_to_R.html","id":"filtering-rows","dir":"Articles","previous_headings":"R data object types > Data frames > Subsetting data frames","what":"Filtering rows","title":"Lab 0: Introduction to R","text":"select specific rows, can use row# method learned , time leaving columns blank: want one row, just put vector rows want: Note can use square brackets also subset vectors, case don’t need comma long tell R column want first: Sometimes, may know specific row number(s) want know value one columns want keep. can R indexing using logical subsetting. example, want just surveys conducted oak habitat, use: Notice need two equals signs (==) telling R want row habitat equals Oak. also select multiple rows using operators like greater , less , etc. slightly complicated example:","code":"jaydata[1,] #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423      0      0.02     Oak   Med   34 jaydata[1:2,] #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423    0.0      0.02     Oak   Med   34 #> 2 261937 3769224       506    0.1      0.45     Oak   Med   38  jaydata[c(1,30),] #>         x       y elevation forest chaparral habitat seeds jays #> 1  258637 3764124       423      0      0.02     Oak   Med   34 #> 30 259537 3765924      1419      0      0.07    Pine   Med   43 jaydata$jays[1] #> [1] 34 head(jaydata[jaydata$habitat == \"Oak\",]) #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423   0.00      0.02     Oak   Med   34 #> 2 261937 3769224       506   0.10      0.45     Oak   Med   38 #> 3 246337 3764124       859   0.00      0.26     Oak  High   40 #> 5 239437 3767724       483   0.26      0.37     Oak   Med   36 #> 6 236437 3769524       830   0.00      0.01     Oak   Low   39 #> 7 263737 3766524       457   0.02      0.22     Oak   Med   38 head(jaydata[jaydata$elevation > 1000,]) #>         x       y elevation forest chaparral habitat seeds jays #> 4  239437 3763524      1508   0.02      0.03    Pine   Med   43 #> 24 261637 3768324      1276   0.02      0.36     Oak  High   44 #> 25 248737 3766524      1024   0.03      0.41    Pine   Low   45 #> 29 255937 3765024      1400   0.02      0.45     Oak  High   48 #> 30 259537 3765924      1419   0.00      0.07    Pine   Med   43 #> 32 245737 3762924      1004   0.02      0.32     Oak   Low   41 head(jaydata[jaydata$elevation < 1000 & jaydata$habitat == \"Oak\",]) #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423   0.00      0.02     Oak   Med   34 #> 2 261937 3769224       506   0.10      0.45     Oak   Med   38 #> 3 246337 3764124       859   0.00      0.26     Oak  High   40 #> 5 239437 3767724       483   0.26      0.37     Oak   Med   36 #> 6 236437 3769524       830   0.00      0.01     Oak   Low   39 #> 7 263737 3766524       457   0.02      0.22     Oak   Med   38"},{"path":"http://rushinglab.github.io/WILD8370/articles/lab02.html","id":"analysis-flowchart","dir":"Articles","previous_headings":"","what":"Analysis Flowchart","title":"lab01b","text":"One hardest part Bayesian analysis converting idea ecological system English math math English. Unlike lot maximum likelihood packages, get decide exact variable model defined, distributions think parameters come , assumptions making underlying system trying understand. process made confusing fact aren’t directly math ! can think process fun little flow chart:  Steps 1 -3 specific Bayesian analysis – need thing maximum likelihood analysis well. Notice steps 4 5 simply Bayesian analysis class. need MCMC software R perform Bayesian analysis, just happens easiest way . today, ’ll focus going steps 2a 3.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/moment_matching.html","id":"moments","dir":"Articles","previous_headings":"Beta distribution","what":"Moments","title":"Probability distribution cheatsheet","text":"\\Large \\mu = \\frac{\\alpha}{\\alpha + \\beta} \\Large \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/moment_matching.html","id":"moment-matching","dir":"Articles","previous_headings":"Beta distribution","what":"Moment matching","title":"Probability distribution cheatsheet","text":"\\Large \\alpha = \\bigg(\\frac{1-\\mu}{\\sigma^2}- \\frac{1}{\\mu} \\bigg)\\mu^2 \\Large \\beta = \\alpha \\bigg(\\frac{1}{\\mu}-1\\bigg) ## Gamma distribution","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/moment_matching.html","id":"moments-1","dir":"Articles","previous_headings":"Beta distribution","what":"Moments","title":"Probability distribution cheatsheet","text":"\\Large \\mu = \\frac{\\alpha}{\\beta} \\Large \\sigma^2 = \\frac{\\alpha}{\\beta^2}","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/moment_matching.html","id":"moment-matching-1","dir":"Articles","previous_headings":"Beta distribution","what":"Moment matching","title":"Probability distribution cheatsheet","text":"\\Large \\alpha = \\frac{\\mu^2}{\\sigma^2} \\Large \\beta =  \\frac{\\mu}{\\sigma^2}","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"question-and-model-development","dir":"Articles","previous_headings":"","what":"Question and model development","title":"Lab5: Priors","text":"One issue many ecological studies logistically financially difficult collect data large number individuals. result, vital rate estimates often based relatively small sample sizes, can lead high uncertainty possibly spurious results. precisely instances Bayesian methods can shine - saw lecture, ability incorporate previous knowledge parameter values via informative priors equivalent increasing sample size. explore different ways generating informative priors, borrow data clever ideas recently published paper dynamics Polar Bear (Ursus maritimus) populations Chukchi Sea (Regehr et al. 2018). example, focus subset full model, namely estimating annual survival adult female bears. model developed Regehr et al. (2018), female survival identifiable estimates imprecise using available data due small sample sizes. result, authors chose develop informative priors based published survival estimates populations. case, authors fortunate handful previous studies lean-later exercise learn developed priors studies. first, let’s assume published studies still want benefits informative priors.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"example-1-domain-expertise","dir":"Articles","previous_headings":"","what":"Example 1: Domain expertise","title":"Lab5: Priors","text":"Even previous estimates polar bear survival, completely ignorant survival species. can use domain expertise develop prior improves upon non-informative option. Polar bears large, long-lived carnivore life expectancy 20-25 years (Ramsay & Stirling 1988). simple fact suggests adult survival must pretty high - even annual survival probability 95\\%, individual 36% probability surviving 20 years. survival probabilities much less \\sim 90\\% pretty surprising pretty suspicious estimates less \\sim 80\\% (imply 1\\% female polar bears make 20 years age). course, polar bear populations many areas declining pretty quickly, suggesting maybe adult survival rates lower historical levels. although can pretty survival high (e.g., >80\\%), ’re super confident actual survival rate. regards prior, suggests want prior puts weight >80\\% wide enough reflect uncertainty actual survival rates.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"turning-our-expertise-into-a-prior","dir":"Articles","previous_headings":"Example 1: Domain expertise","what":"Turning our expertise into a prior","title":"Lab5: Priors","text":"modeling continuous random variable must 0 1 (.e., probability), beta distribution natural choice. shape beta distribution governed two parameters, \\alpha \\beta  learned lecture turn mean variance random variable parameters probability distribution using moment matching. beta distribution: \\alpha = \\bigg(\\frac{1-\\mu}{\\sigma^2}- \\frac{1}{\\mu} \\bigg)\\mu^2 \\tag{1}\\beta = \\alpha \\bigg(\\frac{1}{\\mu}-1\\bigg) \\tag{2} Given domain expertise, reasonable choice mean variance prior might \\mu=0.9 \\sigma^2=0.005 (may seem like small variance ’ll see , actually produces relatively diffuse prior. Figuring appropriate variance probabilities easy best option usually play values explore properties resulting distribution make sure seems reasonable). Plugging values equations 1 2 gives: \\alpha = \\bigg(\\frac{1-0.9}{0.005}- \\frac{1}{0.9} \\bigg)0.9^2 = 15.3 \\beta = 15.3 \\bigg(\\frac{1}{0.9}-1\\bigg) = 1.7 always, visualize distribution make sure matches think look like:  seems reasonable given understanding species biology. However, essentially made variance term give relatively diffuse prior, likely careful justified choice probably need include formal prior sensitivity analyses.","code":"## Target mean and variance mu <- 0.9 var <- 0.005  ## Beta parameters alpha <- mu^2 * ((1-mu)/var - (1/mu)) beta <- alpha * (1/mu - 1)  ## Range of possible survival probabilities phi <- seq(0, 1, by = 0.001)   beta_df <- data.frame(phi = phi,                       density = dbeta(phi, alpha, beta),                       Approach = \"Domain expertise\")  ggplot(beta_df, aes(x = phi, y = density, color = Approach)) +    geom_path() +   scale_x_continuous(expression(phi), limits = c(0.5, 1))"},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"example-2-published-studies","dir":"Articles","previous_headings":"","what":"Example 2: Published studies","title":"Lab5: Priors","text":"Polar bears relatively well-studied surprised find least published estimates adult female survival. turns , . Estimates studies included small data frame WILD8370 package: Rather rely solely domain expertise better option convert published estimates prior distribution. many ways done. explore several.","code":"data(\"SurvPriorData\")"},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"approach-1-mean-and-variance-of-published-estimates","dir":"Articles","previous_headings":"Example 2: Published studies","what":"Approach 1: Mean and variance of published estimates","title":"Lab5: Priors","text":"One reasonable way turn 6 published estimates single prior take mean variance 6 estimates use moment matching convert single beta distribution. Looks like domain knowledge wasn’t far ! Now can covert estimates \\alpha \\beta parameters :  can see, even though means two priors different, variance much smaller prior based published estimates, resulting much informative prior (prior suggests ’re much sure plausible values \\phi).","code":"(mu2 <- mean(SurvPriorData$phi)) #> [1] 0.935 (var2 <- sd(SurvPriorData$phi) ^ 2) #> [1] 0.00027 ## Beta parameters alpha2 <- mu2^2 * ((1-mu2)/var2 - (1/mu2)) beta2 <- alpha2 * (1/mu2 - 1)  beta_df2 <- data.frame(phi = phi,                       density = dbeta(phi, alpha2, beta2),                       Approach = \"Published means\")  beta_df <- dplyr::bind_rows(beta_df, beta_df2)  ggplot(beta_df, aes(x = phi, y = density, color = Approach)) + geom_path() +   scale_x_continuous(expression(phi), limits = c(0.5, 1))"},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"approach-2-mean-and-variance-of-published-estimates","dir":"Articles","previous_headings":"Example 2: Published studies","what":"Approach 2: Mean and variance of published estimates","title":"Lab5: Priors","text":"One reasons informative prior based published survival estimates narrow ignores study-specific uncertainties associated survival estimate. One argue ignoring source uncertainty leads prior confident possible values \\phi. However, incorporating study-specific uncertainty straightforward. One clever approach convert estimates study separate beta distributions, simulate survival estimates distribution, estimate “hyper-parameters” simulations together1. Understanding approach works best done example. Let’s start using moment matching estimate \\alpha \\beta parameters associated individual study. , ’ll take advantage way R performs operations vectors can just plug columns data frame directly formulas: Next, simulate bunch survival estimates distribution. easiest way create empty matrix hold values loop study simulate survival probabilities: Now hopefully can see approach clever. now 30000 plausible values adult female survival probability consistent published studies reflect multiple sources uncertainty vital rate2. simulated values, can now estimate “hyper-parameters” single beta distribution serve prior. First, estimate mean variance simulated values: can see variance samples much larger approach 1, reflecting additional source uncertainty approach 2. Now use moment matching code generate visualize prior:","code":"(alpha3 <- SurvPriorData$phi^2 * ((1-SurvPriorData$phi)/SurvPriorData$se^2 - (1/SurvPriorData$phi))) #> [1] 111.86  41.40  27.25 131.60 131.60  81.90 (beta3 <- alpha3 * (1/SurvPriorData$phi - 1)) #> [1] 5.887 3.600 1.434 8.400 8.400 8.100 ## Number of simulated values from each distribution. nSims <- 5000  ## Empty matrix to store the simulated survival probabilities sim_phi <- matrix(NA, nrow = nSims, ncol = dim(SurvPriorData)[1])  for(j in 1:dim(SurvPriorData)[1]){  sim_phi[,j] <- rbeta(n = nSims, alpha3[j], beta3[j]) } (mu3 <- mean(sim_phi)) #> [1] 0.9352 (var3 <- sd(sim_phi) ^ 2) #> [1] 0.001093 ## Beta parameters alpha4 <- mu3^2 * ((1-mu3)/var3 - (1/mu3)) beta4 <- alpha4 * (1/mu3 - 1)  beta_df3 <- data.frame(phi = phi,                       density = dbeta(phi, alpha4, beta4),                       Approach = \"Simulated\")  beta_df <- dplyr::bind_rows(beta_df, beta_df3)  sim_df <- data.frame(x = c(sim_phi))  ggplot() +    geom_histogram(data = sim_df, aes(x, stat(density)), alpha = 0.4,                  color = \"grey50\",                  fill =  WILD6900_colors$value[WILD6900_colors$name == \"success\"],                   binwidth = 0.005) +   geom_path(data = beta_df, aes(x = phi, y = density, color = Approach)) +   scale_x_continuous(expression(phi), limits = c(0.7, 1))"},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"questions-to-consider3","dir":"Articles","previous_headings":"","what":"Questions to consider3","title":"Lab5: Priors","text":"Informative vs vague: one hand, want posteriors reflect (affected ) prior knowledge. hand, collect data basically know answer? assess ‘sensitivity prior’: Ideally, test sensitive inferences choice prior distribution. quantified change posterior mean, standard deviation, etc? posterior unaffected prior, go trouble using informative priors? Process vs sampling noise: studies used create priors reflect range conditions population trajectories. approach 1, assume point estimates survival reflect process noise across populations. approach 2, use uncertainty study capture sampling process uncertainty. sources uncertainty care developing prior? Domain expertise: Incorporating sampling uncertainty may increased prior weight survival values conflict domain expertise (example, values much lower think plausible). However, values consistent possible survival values published studies. treat priors boundary statistical domain expertise?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"prior-predictive-checks","dir":"Articles","previous_headings":"","what":"Prior Predictive Checks","title":"Lab5: Priors","text":"One issue priors can unexpected things undergo transformations. means prior might look diffuse can accidentally informative model written. See https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13407 great paper . instance, let’s say polar bear survival \\phi function age individual, cubs much lower survival adults: logit(\\phi_i) = \\beta_0 + \\beta_1*age_i might tempted use something simple like uniform \\beta terms: \\beta_0 \\sim Uniform(-5, 5) However, imply \\phi? instance, \\beta_1 0 data, \\phi looks like:  Uh-oh! Nimble, can run model without data monitor latent variables double check priors expect. called Prior Predictive Check useful tool.","code":"beta0 <- runif(10000, -5, 5) par(mfrow = c(1,2)) hist(beta0, main = \"Prior\") hist(plogis(beta0), main = \"Posterior\", xlab = 'Survival')"},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Lab5: Priors","text":"Ramsay, M.. Stirling, ., 1988. Reproductive biology ecology female polar bears (Ursus maritimus). Journal Zoology, 214(4), pp.601-633. Regehr, E.V., Hostetter, N.J., Wilson, R.R., Rode, K.D., Martin, M.S. Converse, S.J., 2018. Integrated population modeling provides first empirical estimates vital rates abundance polar bears Chukchi Sea. Scientific reports, 8(1), p.16780.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/priors.html","id":"homework-questions","dir":"Articles","previous_headings":"","what":"Homework Questions:","title":"Lab5: Priors","text":"studying population gopher tortoises, don’t lot data survival. want model juvenile adult survival : (\\phi_i = \\beta_0[age_i]). comb literature find following information. Using information, choose prior \\beta_0. Defend choice prior. (Note: can choose model \\beta_0 random fixed effect, sure defend choice.) study sites, apparent survival immature tortoises significantly lower adult tortoises, averaging 82.4 ± 3.8% Conecuh 69.7 ± 9.1% Green Grove (Tuberville et al 2014) Additionally, annual survivorship immature tortoises (73.8%, 95% CI = 56.3–86.1%) significantly lower adults (93.4%, 95% CI = 88.6–96.9%) (Howell et al 2019) Quarterly apparent survival differ significantly size classes, although point estimates lower small tortoises (91.6% ± 3.5, 95% CI: 81.6%–96.4%; n = 19) large tortoises (96.4% ± 2.5, 95% CI: 86.8%–99.1%; n = 13) (Stemle et. al 2024) Apparent annual survival adult tortoises averaged 0.98 ± 0.01 years excluding first two (Fig. 4). […] Survival immature tortoises lower adults averaged 0.84 ± 0.05 across years. (Tuberville et al 2008) 2021 Species Status Assessment (SSA) gopher tortoises used informed priors juvenile adult survival model population viability across Southeast USA. juvenile tortoises, used beta distribution mean 0.75 standard deviation 0.06. adult tortoises, used beta mean 0.96 standard deviation 0.03. Write two beta distributions used math form. Visually compare distributions prior chose Question 1. Imagine intercept-occupancy model logit(\\psi) = \\alpha. want put uninformative prior \\alpha aren’t sure happen goes logit transformation. Use R simulate following priors 5000 times, graph prior resulting posterior \\psi ggplot. ones, , seem reasonably uninformative? \\alpha \\sim Normal(0, 10) \\alpha \\sim Gamma(10, 10) (shape, rate parameterization) \\alpha \\sim Beta(.3, .3) \\alpha \\sim Exp(1) \\alpha \\sim Cauchy(0, 1) (location, scale) 1-10 scale, 1 worst week ever 10 best, rate week’s content? lingering questions/confusion lecture lab still ?","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"common-workflows-and-their-problems","dir":"Articles","previous_headings":"","what":"Common workflows and their problems","title":"Improving your workflow through projects","text":"common workflow used many novice R users goes something like : Open base R maybe RStudio Create script save analysis_for_my_project.R Start script : Create bunch objects stuff : Realize need working dissertation instead side project open another script started earlier Get confused objects current environment created script rm(list = ls()) start scratch Rinse repeat","code":"rm(list = ls()) setwd(\"C:\\Users\\crushing\\path\\that\\only\\I\\have\")  library(package1) library(package2) x <- 1:10 y <- \"blah\""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"whats-wrong-with-this","dir":"Articles","previous_headings":"Common workflows and their problems","what":"What’s wrong with this?","title":"Improving your workflow through projects","text":"Remember open R (RStudio), create global workspace. objects create get stored. type ls() can see everything currently stored workspace. working different analyses global environment, easy accidentally create different objects name, write code depends code happens run previously one R session (example, loaded needed package one script didn’t add library(package) next script. happens try run code next time? often called “hidden dependency” can make life pretty miserable), many confusing behaviors. workflow also makes difficult share work. Remember R default looking /saving current working directory. control behavior, people taught use setwd(). problem path set using setwd() almost certainty specific computer used write code. use different computer later send code adviser help, able run without changing working directory. move files new place computer, code break. Got new computer? Oops, none code work anymore. running multiple analyses R session, ’ll also constantly change working directory, eventually lead confusion problems rerunning code. Manually setting working directory seems like minor annoyance constantly (example, instructor grade bunch homework assignments), minor annoyances add big headache. good news , ’s easy solution make life much easier.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"rstudio-projects","dir":"Articles","previous_headings":"","what":"RStudio Projects","title":"Improving your workflow through projects","text":"Experienced programmers learned long ago problems outlined . solve problems, typically use self-contained directory (.e., folder) project. directory contains data code needed create research output project. Notably, code reference data objects found another directory. helps ensure portability - can move entire project still able rerun code. RStudio makes easy create Projects (clarity, use “Project” referring specifically RStudio Projects “project” refer generic research projects, .e., chapter dissertation). highly recommend create new Project project working . several ways easiest way : Open RStudio Click File -> New Project click New Directory (already folder associated project, can click Existing Directory navigate folder) Click New Project choose name location Project (tip - don’t include spaces project name). now worry creating git repository using packrat Click Create Project bottom. RStudio create new directory name Project file called project_name.Rproj inside directory. new RStudio window open create new project. future, want work Project, just double click project_name.Rproj file open new instance RStudio.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"whats-so-special-about-rstudio-projects","dir":"Articles","previous_headings":"RStudio Projects","what":"What’s so special about RStudio Projects?","title":"Improving your workflow through projects","text":"Projects solve two big problems discussed earlier. First, project opens fresh instance R global environment projects self-contained. means can multiple projects open one behave independently. means rm(list = ls()) objects environment ones associated project (doesn’t totally resolve problem hidden dependencies ’ll talk ). Second, RStudio automatically set working directory root directory associate Project. means whatever folder .Rproj file automatically set working directory open project. can check typing getwd() console new project. setting avoids need use setwd() makes code much portable. can move send entire directory, double-click .Rproj file pick right left .","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"a-note-on-saving-your-workspace-to--rdata","dir":"Articles","previous_headings":"RStudio Projects","what":"A note on saving your workspace to .RData","title":"Improving your workflow through projects","text":"Another common mistake made many novice (novice) R users save current workspace .RData time quit R RStudio. surface, seems like really convenient shortcut. next time open R, objects created previously right waiting keep going! problem , , creates hidden dependencies. .RData file re-load packages using previous session. re-set options may set previous session (setwd(), stringsAsFactors = FALSE). objects needed , e.g., making figure may loaded others may . means even saved workspace, still end needing re-run chunks code get back left previously. chunks? knows. probably think ’ll remember won’t. better workflow start every R session empty workspace. means go session mindset need re-run code scratch every time. Although seems like pain, promise make life easier. way ensuring minimum, can reproduce analyses anytime need . RStudio, can enforce workflow going RStudio -> Tools -> Global options General tab setting Save workspace .Rdata exit Never. . Trust . re-running code time consuming? really need re-run code cleans raw data every time need reanalyze ? course . can always save objects (along scripts used create !) next time need , can just re-read R can everything downstream steps without problem. One benefits workflow treats raw data code real objects workflow. objects lose , big trouble (’re computer backed , right?). Cleaned data created raw data isn’t real. can just re-run code create objects (use R scripts instead manipulating raw data excel. See details). workspace isn’t real - just discussed, can () disappear time. Results figures real can re-create code whenever need . get mindset treating raw data code real objects, workflow automatically become organized reproducible. (Manuscripts also real save back !)","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"organizing-your-project-directories","dir":"Articles","previous_headings":"","what":"Organizing your project directories","title":"Improving your workflow through projects","text":"already saw, .Rproj file treats directory working directory project. means can read files location write files location without changing settings. example, can stick filled called data.csv directory following code work: natural instinct us inclined less-organized might put files associated project directory: nothing inherently wrong projects, end big directory hard navigate need find specific files. better idea put files intuitive subdirectories. can pick whatever structure makes sense , generally use something like: data-raw/ contains , guessed , raw data files R script reads raw data , cleans , saves clean data data/ data/ contains cleaned data files ready go straight analysis R/ contains scripts custom functions use data cleaning, analysis, making figures (’ll talk later) figs/ contains figure files doc/ contains files reports manuscripts related projects output/ includes output analysis, example results fitting model scripts/ contains R scripts used analyze data, make figures, whatever. put subdirectory related function script (data_cleaning_script.R) makes sense . can lump different scripts single script prefer. ’s .","code":"data <- read.csv(\"data.csv\") my_proj/ |─── data.csv |─── script_that_does_everything.R |─── test_script_that_does_something_else.R |─── Figure1.png |─── manuscript_v1.doc |─── manuscript_v2.doc . . . |─── manuscript_final_v2-4.doc my_proj/ |─── data-raw/       |─── data.csv       |─── data_cleaning_script.R |─── data/       |─── clean_data.rds |─── R/       |─── function1.R       |─── function2.R |─── figs/       |─── fig1.png       |─── fig2.png |─── doc/       |─── manuscript.Rmd |─── output/       |─── model_results.rds |─── scripts/       |─── analysis.R       |─── figures.R"},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"readingwriting-from-subdirectories","dir":"Articles","previous_headings":"Organizing your project directories","what":"Reading/writing from subdirectories","title":"Improving your workflow through projects","text":"RStudio treats project root directory working directory, following , example, data stored subdirectory: Instead, need use relative paths. Remember everything relative working directory: prefer, can many layers subdirectories want. important thing long move entire project directory, relative paths still work.","code":"data <- read.csv(\"data.csv\") data <- read.csv(\"data-raw/data.csv\")"},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"a-suggested-structure-for-this-class","dir":"Articles","previous_headings":"Organizing your project directories","what":"A suggested structure for this class","title":"Improving your workflow through projects","text":"scripts/ contains R scripts created lab. homework/ contains sub-directories files associated homework. exams/ contains RMarkdown files exam (yes, exams completed RMarkdown).","code":"FANR6750/ |─── scripts/       |─── lab1.R       |─── lab2.R |─── homework/       |─── LastnameFirstname-homework1/             |─── LastnameFirstname-homework1.Rmd             |─── LastnameFirstname-homework1.html       |─── LastnameFirstname-homework2/             |─── LastnameFirstname-homework2.Rmd             |─── LastnameFirstname-homework2.html |─── exams/       |─── exam1.Rmd       |─── exam2.Rmd"},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"a-note-on-handling-raw-data","dir":"Articles","previous_headings":"Organizing your project directories","what":"A note on handling raw data","title":"Improving your workflow through projects","text":"Raw data sacred. purposely accidentally change raw data, everything comes downstream (analysis, results, reports) also change. maintain integrity work, get habit never making changes raw data files entered data. One great things using scripts manipulate data can leave raw data untouched paper trail every change made prepare data analysis. means adding new variables composites raw data (e.g., temp_c <- (temp_f -32)*(5/9)), removing outliers, joining different data sets together, whatever. done R ’ve read raw data rather Excel! Just important, don’t save new data objects raw data even directory. files go data/. means can read data-raw/ write . R , course, allow write data-raw/, just suggestion help maintain firewall keep raw data ’s raw form.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/projects_and_directories.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional resources","title":"Improving your workflow through projects","text":"RStudio Project webpage Nice R Code Jenny Bryan’s 🔥 take setwd() rm(list=ls()) STAT545’s tutorial workspaces projects","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"logistics","dir":"Articles","previous_headings":"","what":"LOGISTICS","title":"Syllabus","text":"Lecture: Monday, Wednesday 9:10-10:00Location: 1-307 Lab: Wednesday 1:50 – 3:50Location: 1-307","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"instructors","dir":"Articles","previous_headings":"","what":"INSTRUCTORS","title":"Syllabus","text":"Dr. Clark Rushingclark.rushing@uga.eduOffice: Warnell 3-409Office hours: Monday 1:30 - 3:00 Dr. Heather Gayaheather.gaya@uga.eduOffice: Warnell 3-410Office hours: Friday 10:00 - 11:00 appointment","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"course-description","dir":"Articles","previous_headings":"","what":"COURSE DESCRIPTION","title":"Syllabus","text":"Quantitative models play central role linking data inferences ecological processes. Although quantitative ecological modeling involves diverse array questions techniques, many common modeling frameworks based set general underlying principles. course aims provide students firm understanding principles application, particular focus modeling dynamics plant animal populations using Bayesian methods. building common principles specific procedures, students better equipped tailor analyses specific data questions, ultimately leading deeper robust understanding ecological systems study. Concepts discussed lectures reinforced lab exercises focused implementing statistical models using modern software tools (e.g. R, Nimble, git).","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"course-objectives","dir":"Articles","previous_headings":"","what":"COURSE OBJECTIVES","title":"Syllabus","text":"primary objective course give tools tackle research questions using rigorous statistical models appropriate data. particular, leave course : firm understanding foundational principles underlying common ecological models ability express mathematical theoretical models apply common ecological datasets ability convert models working code Bayesian data analysis confidence needed design, analyze, report original ecological research using sound quantitative approaches secondary objective course provide tools best practices storing, manipulating, analyzing ecological data developing reproducible code analyses. experience, graduate students well trained methods data collection , degree, data analysis. However, students receive formal training steps data collection reporting results statistical models (e.g., proofing, storing, formatting data; developing well-documented, reproducible analyses; preparing reports). Advancing field ecology also requires scientific community capable judging quality code, interpretation, reporting quantitative models. Graduate students often get opportunities critically review work peers get constructive feedback reviews. lab portion course specifically designed provide hands experience : Best practices cleaning, formatting, storing data using R Generating reproducible analyses reports using R R Markdown Providing critical peer review scientific code reports","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"learning-outcomes","dir":"Articles","previous_headings":"","what":"LEARNING OUTCOMES","title":"Syllabus","text":"Understand common deterministic stochastic models used analyze ecological data Understand key principles linear models, including design matrices, linear predictors, model interpretations Understand key principles Bayesian statistics, including Bayes theorem, Markov Chain Monte Carlo methods, prior distributions, posterior sampling Understand concepts underlying Bayesian hierarchical models, including fixed vs. random effects, hyperparameters, shrinkage Develop custom Bayesian models using MCMC software (e.g., Nimble) estimate parameter values derived quantities interest common ecological processes able evaluate model convergence/fit Bayesian models able prepare reproducible reports using R R Markdown","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"PREREQUISITES","title":"Syllabus","text":"Students least one semester basic ecology introductory statistics. Although thoroughly cover foundational principles common statistical models, basic understanding ecological theory statistical inference helpful. lab activities course rely heavily statistical programming language R (associated software, including RStudio, Nimble, git). first weeks, lab session begin tutorial tools students expected experts prior course. However, quickly move activities require degree R proficiency highly recommend basic understanding programming R (e.g., importing/exporting & manipulating data objects, visualizing data) prior course. find struggling aspects using R, please seek individual help office hours. earlier can get speed, painless remainder semester .","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"course-format","dir":"Articles","previous_headings":"","what":"COURSE FORMAT","title":"Syllabus","text":"course taught using lectures labs. Lectures focus conceptual basis ecological modeling Bayesian methods. Labs designed reinforce clarify lecture topics, allowing students get hands-experience manipulating, analyzing, visualizing data. class materials, including lecture slides, computer code, lab documents, data, posted course website prior class.","code":""},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"textbooks","dir":"Articles","previous_headings":"COURSE RESOURCES","what":"Textbooks","title":"Syllabus","text":"Although many excellent texts covering various aspects ecological modeling, lectures course closely follow two: Hobbs, N.T. M.B. Hooten, 2015. Bayesian Models: Statistical Primer Ecologists (available ) Kéry, M. M. Schaub, 2011. Bayesian population analysis using WinBUGS: hierarchical perspective (available ). Weekly readings assigned books highly recommend students purchase prior start semester. Kéry & Schaub book also contains wealth useful code , although optional, students encouraged implement read text. Chapters books may occasionally supplemented primary journal articles. applicable PDF’s articles posted course website. Although two text books lab materials sufficient mastering material presented course, number excellent books available students may find /books helpful. following books 100% optional course found helpful learning material: Kéry, M. & Royle, J.. 2016. Applied Hierarchical Modeling Ecology: Analysis distribution, abundance, species richness R BUGS. Academic Press. Kéry, M., 2010. Introduction WinBUGS ecologists: Bayesian approach regression, ANOVA, mixed models related analyses. Academic Press. Williams, B.K., Nichols, J.D., Conroy, M.J., 2001. Analysis management animal populations. Academic Press. Bolker, B.M., 2008. Ecological models data R. Princeton University Press.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"lab-materials","dir":"Articles","previous_headings":"COURSE RESOURCES","what":"Lab materials","title":"Syllabus","text":"Materials labs provided HTML R Markdown files course webpage. materials include step--step tutorials lab exercises well links additional online resources, problem sets, homework assignments.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"software","dir":"Articles","previous_headings":"COURSE RESOURCES","what":"Software","title":"Syllabus","text":"lab computers R RStudio installed students required install software computers. However, students free use laptops lab R RStudio installed make easier complete lab assignments outside class. students wishing use computers R, RStudio, Nimble installed running prior first lab. Detailed instructions installing R RStudio can found . plan use computer, sure recent versions three software programs installed. greatly decrease chances running issue running code provide lab. Prior start semester, test R RStudio installed correctly following: Launch RStudio Put cursor window labelled Console. Type following code followed enter return: x <- 2 * 4. Next type x followed enter return. see value 8 print screen. yes, ’ve succeeded installing R RStudio. encounter problems previous steps, please contact instructors prior first class.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"attendance","dir":"Articles","previous_headings":"","what":"ATTENDANCE","title":"Syllabus","text":"graduate students willingly signed course, presume eager learn material self-motivated enough put required effort. reason, set formal attendance policy. However, cover lot material course semester topic build concepts previous weeks. result, missing even lectures labs make difficult fully master learning outcomes described . know missing lectures labs, please contact us advance can make sure get far behind material.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"a-note-on-fieldwork","dir":"Articles","previous_headings":"ATTENDANCE","what":"A note on fieldwork","title":"Syllabus","text":"realize many students field work obligations spring semester. need take course know advance field portion semester, please let us know ASAP can discuss whether field work barrier taking course merely inconvenience. distinction mainly function long miss class. absences relatively , taking course may still option. Students still expected complete turn assignments miss. going miss many classes unable complete assignments ’re field, may better take class time field commitments smaller.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"grading","dir":"Articles","previous_headings":"","what":"GRADING","title":"Syllabus","text":"grade course based total 200 possible points. entire grade come homework assignments. homework assignments build concepts skills cover lecture lab. Specific objectives tasks assignment, along necessary data, posted course website. general, students provided ‘raw’ data need clean/prepare (document!) data prior analysis. assignment due class week assignment posted. assignments must prepared R Markdown files include text, code, model output, figures necessary fully document work conclusions (spend first several labs going preparation reports using R Markdown , , previous experience necessary). See instructions submitting assignment","code":""},{"path":[]},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"university-honor-code-academic-honesty","dir":"Articles","previous_headings":"","what":"UNIVERSITY HONOR CODE & ACADEMIC HONESTY","title":"Syllabus","text":"University Georgia student, agreed abide UGA academic honesty policy. UGA Student Honor code: academically honest academic work tolerate academic dishonesty others Culture Honesty, University’s policy procedures handling cases suspected dishonesty, can found https://honesty.uga.edu/ responsible informing university’s standards performing academic work. Lack knowledge academic honesty policy reasonable explanation violation. Please ask questions related course assignments academic honesty policy. form possible academic dishonesty reported UGA Office Vice President Instruction.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"accommodations-for-disabilities","dir":"Articles","previous_headings":"","what":"ACCOMMODATIONS FOR DISABILITIES","title":"Syllabus","text":"require disability-required accommodation, essential register Disability Resource Center (Clark Howell Hall; https://drc.uga.edu; 706-542-8719 [voice]; 706-542-8778 [TTY]) notify eligibility reasonable accommodations. can plan best coordinate accommodations. Please note accommodations provided retroactively.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"wellness-statement","dir":"Articles","previous_headings":"","what":"WELLNESS STATEMENT","title":"Syllabus","text":"Mental Health Wellness Resources: someone know needs assistance, encouraged contact Student Care Outreach Division Student Affairs 706-542-7774 visit https://sco.uga.edu/. help navigate difficult circumstances may facing connecting appropriate resources services. UGA several resources student seeking mental health services (https://www.uhs.uga.edu/bewelluga/bewelluga) crisis support (https://www.uhs.uga.edu/info/emergencies). need help managing stress anxiety, relationships, etc., please visit BeWellUGA (https://www.uhs.uga.edu/bewelluga/bewelluga) list FREE workshops, classes, mentoring, health coaching led licensed clinicians health educators University Health Center. Additional resources can accessed UGA App.","code":""},{"path":"http://rushinglab.github.io/WILD8370/articles/syllabus.html","id":"ferpa-notice","dir":"Articles","previous_headings":"","what":"FERPA NOTICE","title":"Syllabus","text":"Federal Family Educational Rights Privacy Act (FERPA) grants students certain information privacy rights. comply FERPA, communication refers individual students must secure medium (UGAMail eLC) person. Instructors allowed respond messages refer individual students student progress course non-UGA accounts, phone calls, types electronic media. details, please visit https://apps.reg.uga.edu/FERPA.","code":""},{"path":"http://rushinglab.github.io/WILD8370/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Clark Rushing. Author, maintainer. Heather Gaya. Author.","code":""},{"path":"http://rushinglab.github.io/WILD8370/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Rushing C, Gaya H (2025). WILD8370: Course Materials WILD 8370. R package version 2025.1.0, http://rushinglab.github.io/WILD8370.","code":"@Manual{,   title = {WILD8370: Course Materials for WILD 8370},   author = {Clark Rushing and Heather Gaya},   year = {2025},   note = {R package version 2025.1.0},   url = {http://rushinglab.github.io/WILD8370}, }"},{"path":"http://rushinglab.github.io/WILD8370/index.html","id":"welcome-to-wild8370-bayesian-models-for-conservation-science","dir":"","previous_headings":"","what":"Course Materials for WILD 8370","title":"Course Materials for WILD 8370","text":"unofficial course website Spring 2025 offering WILD8370: Bayesian Models Conservation Science University Georgia. “official” course website (students enrolled course) eLC. goal website create central repository students access course materials - lecture slides, lab activities, code, data, etc. secondary goal make materials freely available students instructors may find useful. encounter issues suggestions, feel free contact clark.rushing [] uga [dot] edu. General course information can found clicking Syllabus Schedule links . Lecture slides lab activities can accessed using drop menus.","code":""},{"path":"http://rushinglab.github.io/WILD8370/index.html","id":"course-r-package","dir":"","previous_headings":"","what":"Course R package","title":"Course Materials for WILD 8370","text":"addition website, materials course distributed R package called WILD8370. main purpose package distribute code data used labs, though eventually additional materials may included, including lectures reference documents. can install current version WILD8370 :","code":"install.packages(\"devtools\") devtools::install_github(\"RushingLab/WILD8370\")"},{"path":"http://rushinglab.github.io/WILD8370/index.html","id":"use-of-material","dir":"","previous_headings":"","what":"Use of material","title":"Course Materials for WILD 8370","text":"Materials included course purposefully made available anyone finds useful. Users free use, adapt, distribute, display, communicate materials freely. find materials useful, please let know, especially using adapting materials teaching. Tracking use materials outside official course great validation effort, helps demonstrate “positive professional reputation teaching.”","code":""}]
